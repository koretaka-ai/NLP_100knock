{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100本ノック第9章(86~87)：RNN,CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14OVcDEtGYz6Yp0YRt6z3n0DHLu8pDyMN",
      "authorship_tag": "ABX9TyOvSMVvkyEn+QjZlSQqrlwn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyDWX1eNxVfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5394d680-132f-49f8-e74e-0dfdac713c02"
      },
      "source": [
        "# ====================\n",
        "# ライブラリのインストール\n",
        "# ====================\n",
        "! pip install --quiet torch==1.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 748.8 MB 17 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ozo4mmoc-dNu"
      },
      "outputs": [],
      "source": [
        "# ここでランタイムを再起動\n",
        "\n",
        "# ライブラリの読み込み\n",
        "import os\n",
        "import time\n",
        "import string\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "from gensim.models import KeyedVectors\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# データセットのダウンロード\n",
        "if os.path.isfile(\"/content/NewsAggregatorDataset.zip\") == False:\n",
        "    ! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "    ! unzip NewsAggregatorDataset.zip\n",
        "    # 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
        "    ! sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv\n",
        "df = pd.read_csv('/content/newsCorpora.csv', sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "df1 = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割 stratifyを設定することで訓練データとテストデータの指定した中身の割合を同じにすることができる\n",
        "train, temp = train_test_split(df1, test_size=0.2, shuffle=True, random_state=0, stratify=df1['CATEGORY'])\n",
        "test, valid = train_test_split(temp, test_size=0.5, shuffle=True, random_state=0, stratify=temp['CATEGORY'])\n",
        "\n",
        "# データの保存\n",
        "! mkdir -p /content/data/\n",
        "train.to_csv('/content/data/train.txt', sep=\"\\t\", index=False)\n",
        "test.to_csv('/content/data/test.txt', sep=\"\\t\", index=False)\n",
        "valid.to_csv('/content/data/valid.txt', sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 辞書作成の関数\n",
        "def get_word2id(fname):\n",
        "    my_dict = defaultdict(int)\n",
        "    train = pd.read_table(fname)\n",
        "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
        "    for text in train['TITLE']:\n",
        "        for word in text.translate(table).split():\n",
        "            my_dict[word] += 1\n",
        "    my_dict = sorted(my_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "    # 単語ID辞書の作成\n",
        "    word2id = {word: i + 1 for i, (word, cnt) in enumerate(my_dict) if cnt > 1}  # 出現頻度が2回以上の単語を登録\n",
        "    return word2id\n",
        "\n",
        "word2id = get_word2id(\"/content/data/train.txt\")\n",
        "\n",
        "# ラベル列を返す関数\n",
        "def tokenizer(text, word2id=word2id, unknown=0):\n",
        "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    return [word2id.get(word, unknown) for word in text.translate(table).split()] "
      ],
      "metadata": {
        "id": "Bj2hO4H9OZ5k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, X, y, tokenizer):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):  # len(Dataset)で返す値を指定\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "        text = self.X[index]\n",
        "        inputs = self.tokenizer(text)\n",
        "\n",
        "        return {\n",
        "            'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "            'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "        }\n",
        "    \n",
        "def make_dataset(input):\n",
        "    label2id = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "    df = pd.read_table(input)\n",
        "    y = df['CATEGORY'].map(lambda x: label2id[x]).values\n",
        "    dataset = CreateDataset(df['TITLE'], y, tokenizer)\n",
        "    return dataset\n",
        "\n",
        "dataset_train = make_dataset(\"/content/data/train.txt\")\n",
        "dataset_valid = make_dataset(\"/content/data/valid.txt\")\n",
        "dataset_test  = make_dataset(\"/content/data/test.txt\")"
      ],
      "metadata": {
        "id": "vPCskzD1PLsj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習済み単語ベクトルの取得\n",
        "def get_emb_weights(word2id):\n",
        "    wv = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/nlp100/chapter8/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "    vocab_size = len(word2id) + 1\n",
        "    emb_size = wv.vector_size\n",
        "    emb_weights = torch.zeros(vocab_size, emb_size)\n",
        "    for i, word in enumerate(word2id.keys()):\n",
        "        try:\n",
        "            emb_weights[i] = torch.tensor(wv[word])\n",
        "        except KeyError:\n",
        "            emb_weights[i] = torch.rand(emb_size)\n",
        "\n",
        "    return emb_size, emb_weights\n",
        "EMB_SIZE, emb_weights = get_emb_weights(word2id)"
      ],
      "metadata": {
        "id": "T3P7R0W3PpUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae13fffc-0fbc-4cd9-8e89-b5f4945946cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============\n",
        "# 86. 畳み込みニューラルネットワーク(CNN)\n",
        "# ============\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n",
        "        super().__init__()\n",
        "        if emb_weights is None:\n",
        "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n",
        "        self.fc = nn.Linear(out_channels, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x).unsqueeze(1)\n",
        "        conv = self.conv(emb)\n",
        "        act = F.relu(conv.squeeze(3))\n",
        "        max_pool = F.max_pool1d(act, act.size()[2])\n",
        "        out = self.fc(max_pool.squeeze(2))\n",
        "        return out\n",
        "\n",
        "VOCAB_SIZE = len(word2id) + 1\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=emb_weights)\n",
        "\n",
        "# 10件の予測値のみ表示\n",
        "for data, i in zip(dataloader_train, range(10)):\n",
        "    print(torch.softmax(model(data['inputs']), dim=-1))"
      ],
      "metadata": {
        "id": "0YM_Rw2-pnvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a4ee88-b763-4d0c-d80e-b0b80b2db620"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3195, 0.1902, 0.2556, 0.2347]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3396, 0.1846, 0.2541, 0.2218]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3147, 0.2170, 0.2556, 0.2127]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.2983, 0.2198, 0.2581, 0.2239]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3343, 0.1831, 0.2460, 0.2367]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3506, 0.1940, 0.2263, 0.2291]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.2790, 0.2269, 0.2628, 0.2313]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3508, 0.1980, 0.2285, 0.2227]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3481, 0.1863, 0.2418, 0.2238]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3599, 0.1907, 0.2278, 0.2216]], grad_fn=<SoftmaxBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_and_accuracy(model, dataset, device=None, criterion=None):\n",
        "    \"\"\"損失・正解率を計算\"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "    loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader: \n",
        "            # デバイスの指定\n",
        "            inputs = data['inputs'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            # 順伝播\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # 損失計算\n",
        "            if criterion != None:\n",
        "                loss += criterion(outputs, labels).item()\n",
        "\n",
        "            # 正解率計算\n",
        "            pred = torch.argmax(outputs, dim=-1)\n",
        "            total += len(inputs)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "        return loss / len(dataset), correct / total"
      ],
      "metadata": {
        "id": "x1kjvvIrSbcl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Padsequence():\n",
        "    \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
        "    def __init__(self, padding_idx):\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
        "        sequences = [x['inputs'] for x in sorted_batch]\n",
        "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n",
        "        labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
        "\n",
        "        return {'inputs': sequences_padded, 'labels': labels}"
      ],
      "metadata": {
        "id": "s5wSl25vTHZa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, collate_fn=None, device=None):\n",
        "    \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
        "    # デバイスの指定\n",
        "    model.to(device)\n",
        "\n",
        "    # dataloaderの作成\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
        "\n",
        "    # スケジューラの設定\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n",
        "\n",
        "    # 学習\n",
        "    log_train = []\n",
        "    log_valid = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # 開始時刻の記録\n",
        "        s_time = time.time()\n",
        "\n",
        "        # 訓練モードに設定\n",
        "        model.train()\n",
        "        for data in dataloader_train:\n",
        "            # 勾配をゼロで初期化\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "            inputs = data['inputs'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 評価モードに設定\n",
        "        model.eval()\n",
        "\n",
        "        # 損失と正解率の算出\n",
        "        loss_train, acc_train = calculate_loss_and_accuracy(model, dataset_train, device, criterion=criterion)\n",
        "        loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "        # チェックポイントの保存\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "        # 終了時刻の記録\n",
        "        e_time = time.time()\n",
        "\n",
        "        # ログを出力\n",
        "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "        # 検証データの損失が3エポック連続で低下しなかった場合は学習終了\n",
        "        if epoch > 2 and log_valid[epoch - 3][0] <= log_valid[epoch - 2][0] <= log_valid[epoch - 1][0] <= log_valid[epoch][0]:\n",
        "            break\n",
        "\n",
        "        # スケジューラを1ステップ進める\n",
        "        scheduler.step()\n",
        "\n",
        "    return {'train': log_train, 'valid': log_valid}"
      ],
      "metadata": {
        "id": "z2Hg2hTgSmac"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============\n",
        "# 87. 確率的勾配降下法によるCNNの学習\n",
        "# ============\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# モデルの定義\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=emb_weights)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "metadata": {
        "id": "D79myNfrpnnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92faa27a-99d1-4f32-b575-79feabab5b9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss_train: 1.1043, accuracy_train: 0.5216, loss_valid: 1.1064, accuracy_valid: 0.5247, 7.4991sec\n",
            "epoch: 2, loss_train: 1.0188, accuracy_train: 0.6302, loss_valid: 1.0435, accuracy_valid: 0.5922, 7.0529sec\n",
            "epoch: 3, loss_train: 0.9261, accuracy_train: 0.6758, loss_valid: 0.9574, accuracy_valid: 0.6447, 7.0179sec\n",
            "epoch: 4, loss_train: 0.8564, accuracy_train: 0.7056, loss_valid: 0.9033, accuracy_valid: 0.6769, 9.1427sec\n",
            "epoch: 5, loss_train: 0.8139, accuracy_train: 0.7075, loss_valid: 0.8805, accuracy_valid: 0.6709, 7.4416sec\n",
            "epoch: 6, loss_train: 0.7657, accuracy_train: 0.7355, loss_valid: 0.8383, accuracy_valid: 0.6942, 7.1848sec\n",
            "epoch: 7, loss_train: 0.7411, accuracy_train: 0.7456, loss_valid: 0.8210, accuracy_valid: 0.6987, 7.0857sec\n",
            "epoch: 8, loss_train: 0.7264, accuracy_train: 0.7532, loss_valid: 0.8115, accuracy_valid: 0.7039, 7.0549sec\n",
            "epoch: 9, loss_train: 0.7201, accuracy_train: 0.7564, loss_valid: 0.8083, accuracy_valid: 0.7039, 6.9880sec\n",
            "epoch: 10, loss_train: 0.7180, accuracy_train: 0.7571, loss_valid: 0.8070, accuracy_valid: 0.7061, 7.3248sec\n",
            "正解率（学習データ）：0.757\n",
            "正解率（評価データ）：0.726\n"
          ]
        }
      ]
    }
  ]
}