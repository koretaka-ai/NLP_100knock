{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100本ノック第10章：機械翻訳.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1eVaZ5gcbvjSz-UPCMThLdgm-fAjoykOc",
      "authorship_tag": "ABX9TyMznoVXZnz3dmZ+pWEj0oKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koretaka-ai/NLP_100knock/blob/master/100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF%E7%AC%AC10%E7%AB%A0%EF%BC%9A%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前準備"
      ],
      "metadata": {
        "id": "lMDh69-JG4B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============\n",
        "# データのダウンロード\n",
        "# ============\n",
        "! wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
        "! tar xvzf kftt-data-1.0.tar.gz\n",
        "! rm /content/kftt-data-1.0.tar.gz\n",
        "# ginzaのモデルをダウンロード\n",
        "! pip install -U --quiet ja_ginza\n",
        "# spacyのモデルをダウンロード\n",
        "! python -m spacy download en_core_web_sm\n",
        "# tensorboardのinstall\n",
        "! pip install tensorboardX\n",
        "# fairseqのインストール\n",
        "! git clone https://github.com/pytorch/fairseq\n",
        "% cd /content/fairseq/\n",
        "! python -m pip install --editable .\n",
        "% cd ..\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":./fairseq/\"\n",
        "# apexのインストール\n",
        "! rm -r /content/apex\n",
        "! git clone https://github.com/NVIDIA/apex\n",
        "% cd apex\n",
        "! pip install --quiet -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "% cd ..\n",
        "# Mecabのインストール\n",
        "! sudo apt install mecab libmecab-dev mecab-ipadic-utf8 swig -y\n",
        "! pip install mecab-python3 unidic-lite\n",
        "! pip install fugashi\n",
        "! pip install ipadic \n",
        "import MeCab\n",
        "# SentencePieceの構築\n",
        "! sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
        "! git clone https://github.com/google/sentencepiece.git \n",
        "% cd sentencepiece\n",
        "! mkdir build\n",
        "% cd build\n",
        "! cmake ..\n",
        "! make -j $(nproc)\n",
        "! sudo make install\n",
        "! sudo ldconfig -v\n",
        "% cd ../..\n",
        "! git clone https://github.com/Microsoft/vcpkg.git\n",
        "% cd vcpkg\n",
        "! ./bootstrap-vcpkg.sh\n",
        "! ./vcpkg integrate install\n",
        "! ./vcpkg install sentencepiece\n",
        "% cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXcASzxRPasu",
        "outputId": "0e7f1603-02ae-42e9-b496-afb8c0455251"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-01 12:46:13--  http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
            "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
            "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99246893 (95M) [application/gzip]\n",
            "Saving to: ‘kftt-data-1.0.tar.gz’\n",
            "\n",
            "kftt-data-1.0.tar.g 100%[===================>]  94.65M  32.8MB/s    in 2.9s    \n",
            "\n",
            "2022-03-01 12:46:17 (32.8 MB/s) - ‘kftt-data-1.0.tar.gz’ saved [99246893/99246893]\n",
            "\n",
            "kftt-data-1.0/\n",
            "kftt-data-1.0/data/\n",
            "kftt-data-1.0/data/orig/\n",
            "kftt-data-1.0/data/orig/kyoto-tune.en\n",
            "kftt-data-1.0/data/orig/kyoto-dev.ja\n",
            "kftt-data-1.0/data/orig/kyoto-dev.en\n",
            "kftt-data-1.0/data/orig/kyoto-train.en\n",
            "kftt-data-1.0/data/orig/kyoto-tune.ja\n",
            "kftt-data-1.0/data/orig/kyoto-train.ja\n",
            "kftt-data-1.0/data/orig/kyoto-test.ja\n",
            "kftt-data-1.0/data/orig/kyoto-test.en\n",
            "kftt-data-1.0/data/tok/\n",
            "kftt-data-1.0/data/tok/kyoto-tune.en\n",
            "kftt-data-1.0/data/tok/kyoto-dev.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.cln.en\n",
            "kftt-data-1.0/data/tok/kyoto-dev.en\n",
            "kftt-data-1.0/data/tok/kyoto-train.en\n",
            "kftt-data-1.0/data/tok/kyoto-tune.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.cln.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.ja\n",
            "kftt-data-1.0/data/tok/kyoto-test.ja\n",
            "kftt-data-1.0/data/tok/kyoto-test.en\n",
            "kftt-data-1.0/README.txt\n",
            "\u001b[K     |████████████████████████████████| 59.0 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 58.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 88.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 58.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 77.9 MB/s \n",
            "\u001b[?25h  Building wheel for ja-ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sudachidict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 30984, done.\u001b[K\n",
            "remote: Counting objects: 100% (615/615), done.\u001b[K\n",
            "remote: Compressing objects: 100% (365/365), done.\u001b[K\n",
            "remote: Total 30984 (delta 285), reused 486 (delta 240), pack-reused 30369\u001b[K\n",
            "Receiving objects: 100% (30984/30984), 21.27 MiB | 32.31 MiB/s, done.\n",
            "Resolving deltas: 100% (22984/22984), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (0.29.28)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (4.62.3)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (0.10.0+cu111)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (1.10.0+cu111)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 12.0 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (1.21.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e55e094) (1.15.0)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.3.7.tar.gz (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+e55e094) (5.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+e55e094) (3.10.0.2)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+e55e094) (0.8.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+e55e094) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+e55e094) (3.7.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, bitarray\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=8c7a05df9563e2f6f8b19d9d78f85838353272b23f8818734e0a1ddbe9e1dd78\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for bitarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitarray: filename=bitarray-2.3.7-cp37-cp37m-linux_x86_64.whl size=173586 sha256=8fc1df2331ecd081e6eeec5cd3e2e361bfd51f01bf183b2be023c4d1beac3700\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/65/b0/59479ecb406b1769a3126c2a633aad7365b956373d79724ef4\n",
            "Successfully built antlr4-python3-runtime bitarray\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.3.7 colorama-0.4.4 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.0.0\n",
            "/content\n",
            "rm: cannot remove '/content/apex': No such file or directory\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 9079, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (126/126), done.\u001b[K\n",
            "remote: Total 9079 (delta 83), reused 38 (delta 23), pack-reused 8929\u001b[K\n",
            "Receiving objects: 100% (9079/9079), 14.59 MiB | 26.16 MiB/s, done.\n",
            "Resolving deltas: 100% (6194/6194), done.\n",
            "/content/apex\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Processing /content/apex\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed apex-0.1\n",
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-jumandic\n",
            "  mecab-jumandic-utf8 mecab-utils swig swig3.0\n",
            "0 upgraded, 10 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 30.1 MB of archives.\n",
            "After this operation, 282 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 30.1 MB in 3s (10.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../3-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../4-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../5-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../6-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../7-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../8-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../9-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 127 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=2d273a429cb5c42e780f37a0bbd891f77f0322b8cf7a52e6a30bd65c41ce6cb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite, mecab-python3\n",
            "Successfully installed mecab-python3-1.0.5 unidic-lite-1.0.8\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (568 kB)\n",
            "\u001b[K     |████████████████████████████████| 568 kB 8.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.1.2\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 8.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=abf71b22395e9bfbd5ebc8f295919ecfc955949e958bc3031b8d5c6db5e7acd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libunwind-dev\n",
            "The following NEW packages will be installed:\n",
            "  libgoogle-perftools-dev libunwind-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 627 kB of archives.\n",
            "After this operation, 6,761 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libunwind-dev amd64 1.2.1-8 [423 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgoogle-perftools-dev amd64 2.5-2.2ubuntu3 [204 kB]\n",
            "Fetched 627 kB in 1s (508 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libunwind-dev:amd64.\n",
            "(Reading database ... 156289 files and directories currently installed.)\n",
            "Preparing to unpack .../libunwind-dev_1.2.1-8_amd64.deb ...\n",
            "Unpacking libunwind-dev:amd64 (1.2.1-8) ...\n",
            "Selecting previously unselected package libgoogle-perftools-dev.\n",
            "Preparing to unpack .../libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb ...\n",
            "Unpacking libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\n",
            "Setting up libunwind-dev:amd64 (1.2.1-8) ...\n",
            "Setting up libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Cloning into 'sentencepiece'...\n",
            "remote: Enumerating objects: 3855, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 3855 (delta 73), reused 85 (delta 42), pack-reused 3691\u001b[K\n",
            "Receiving objects: 100% (3855/3855), 32.18 MiB | 21.96 MiB/s, done.\n",
            "Resolving deltas: 100% (2665/2665), done.\n",
            "/content/sentencepiece\n",
            "/content/sentencepiece/build\n",
            "-- VERSION: 0.1.96\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found TCMalloc: /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/sentencepiece/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target sentencepiece_train-static\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target sentencepiece-static\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target sentencepiece\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/builder.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/unicode_script.cc.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/trainer_factory.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/trainer_interface.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:494:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:86:35\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:34:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’: specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
            "   return __builtin___memcpy_chk (__dest, __src, __len, __bos0 (__dest)\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:494:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:86:35\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:34:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’: specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Wstringop-overflow=\u001b[m\u001b[K]\n",
            "   return __builtin___memcpy_chk (__dest, __src, __len, __bos0 (__dest)\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX static library libsentencepiece_train.a\u001b[0m\n",
            "[ 36%] Built target sentencepiece_train-static\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/char_model.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/error.cc.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/filesystem.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/model_factory.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/model_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/normalizer.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/char_model.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/error.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/filesystem.cc.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/model_factory.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/model_interface.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/normalizer.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/util.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/word_model.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/absl/strings/string_view.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/util.cc.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/word_model.cc.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/absl/strings/string_view.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX static library libsentencepiece.a\u001b[0m\n",
            "[ 80%] Built target sentencepiece-static\n",
            "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece.so\u001b[0m\n",
            "[ 82%] Built target sentencepiece\n",
            "\u001b[35m\u001b[1mScanning dependencies of target spm_encode\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target spm_decode\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target spm_export_vocab\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target sentencepiece_train\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_decode.dir/spm_decode_main.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_export_vocab.dir/spm_export_vocab_main.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_encode.dir/spm_encode_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/builder.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable spm_export_vocab\u001b[0m\n",
            "[ 86%] Built target spm_export_vocab\n",
            "[ 87%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/unicode_script.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable spm_decode\u001b[0m\n",
            "[ 88%] Built target spm_decode\n",
            "[ 89%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_factory.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable spm_encode\u001b[0m\n",
            "[ 90%] Built target spm_encode\n",
            "[ 90%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_interface.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece_train.so\u001b[0m\n",
            "[ 96%] Built target sentencepiece_train\n",
            "\u001b[35m\u001b[1mScanning dependencies of target spm_train\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target spm_normalize\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_train.dir/spm_train_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_normalize.dir/spm_normalize_main.cc.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable spm_train\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable spm_normalize\u001b[0m\n",
            "[100%] Built target spm_train\n",
            "[100%] Built target spm_normalize\n",
            "[ 36%] Built target sentencepiece\n",
            "[ 46%] Built target sentencepiece_train-static\n",
            "[ 47%] Built target spm_encode\n",
            "[ 56%] Built target sentencepiece_train\n",
            "[ 58%] Built target spm_train\n",
            "[ 60%] Built target spm_decode\n",
            "[ 62%] Built target spm_normalize\n",
            "[ 98%] Built target sentencepiece-static\n",
            "[100%] Built target spm_export_vocab\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Installing: /usr/local/lib/pkgconfig/sentencepiece.pc\n",
            "-- Installing: /usr/local/lib/libsentencepiece.so.0.0.0\n",
            "-- Installing: /usr/local/lib/libsentencepiece.so.0\n",
            "-- Installing: /usr/local/lib/libsentencepiece.so\n",
            "-- Installing: /usr/local/lib/libsentencepiece_train.so.0.0.0\n",
            "-- Installing: /usr/local/lib/libsentencepiece_train.so.0\n",
            "-- Set runtime path of \"/usr/local/lib/libsentencepiece_train.so.0.0.0\" to \"\"\n",
            "-- Installing: /usr/local/lib/libsentencepiece_train.so\n",
            "-- Installing: /usr/local/lib/libsentencepiece.a\n",
            "-- Installing: /usr/local/lib/libsentencepiece_train.a\n",
            "-- Installing: /usr/local/bin/spm_encode\n",
            "-- Set runtime path of \"/usr/local/bin/spm_encode\" to \"\"\n",
            "-- Installing: /usr/local/bin/spm_decode\n",
            "-- Set runtime path of \"/usr/local/bin/spm_decode\" to \"\"\n",
            "-- Installing: /usr/local/bin/spm_normalize\n",
            "-- Set runtime path of \"/usr/local/bin/spm_normalize\" to \"\"\n",
            "-- Installing: /usr/local/bin/spm_train\n",
            "-- Set runtime path of \"/usr/local/bin/spm_train\" to \"\"\n",
            "-- Installing: /usr/local/bin/spm_export_vocab\n",
            "-- Set runtime path of \"/usr/local/bin/spm_export_vocab\" to \"\"\n",
            "-- Installing: /usr/local/include/sentencepiece_trainer.h\n",
            "-- Installing: /usr/local/include/sentencepiece_processor.h\n",
            "/sbin/ldconfig.real: Can't stat /usr/local/cuda-11.1/extras/CUPTI/lib64: No such file or directory\n",
            "/sbin/ldconfig.real: Path `/usr/local/lib' given more than once\n",
            "/sbin/ldconfig.real: Can't stat /usr/local/nvidia/lib: No such file or directory\n",
            "/sbin/ldconfig.real: Can't stat /usr/local/nvidia/lib64: No such file or directory\n",
            "/sbin/ldconfig.real: Can't stat /usr/local/lib/x86_64-linux-gnu: No such file or directory\n",
            "/sbin/ldconfig.real: Path `/lib/x86_64-linux-gnu' given more than once\n",
            "/sbin/ldconfig.real: Path `/usr/lib/x86_64-linux-gnu' given more than once\n",
            "/usr/local/cuda-11.1/targets/x86_64-linux/lib:\n",
            "\tlibnppidei.so.11 -> libnppidei.so.11.1.2.301\n",
            "\tlibnppisu.so.11 -> libnppisu.so.11.1.2.301\n",
            "\tlibnppicc.so.11 -> libnppicc.so.11.1.2.301\n",
            "\tlibnvblas.so.11 -> libnvblas.so.11.3.0.106\n",
            "\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\n",
            "\tlibcurand.so.10 -> libcurand.so.10.2.2.105\n",
            "\tlibnvrtc-builtins.so.11.1 -> libnvrtc-builtins.so.11.1.105\n",
            "\tlibnppim.so.11 -> libnppim.so.11.1.2.301\n",
            "\tlibnppitc.so.11 -> libnppitc.so.11.1.2.301\n",
            "\tlibnppist.so.11 -> libnppist.so.11.1.2.301\n",
            "\tlibcusolver.so.11 -> libcusolver.so.11.0.1.105\n",
            "\tlibcusparse.so.11 -> libcusparse.so.11.3.0.10\n",
            "\tlibnpps.so.11 -> libnpps.so.11.1.2.301\n",
            "\tlibnppc.so.11 -> libnppc.so.11.1.2.301\n",
            "\tlibnppig.so.11 -> libnppig.so.11.1.2.301\n",
            "\tlibcufft.so.10 -> libcufft.so.10.3.0.105\n",
            "\tlibnvrtc.so.11.1 -> libnvrtc.so.11.1.105\n",
            "\tlibcublasLt.so.11 -> libcublasLt.so.11.3.0.106\n",
            "\tlibcufftw.so.10 -> libcufftw.so.10.3.0.105\n",
            "\tlibcusolverMg.so.11 -> libcusolverMg.so.11.0.1.105\n",
            "\tlibnppif.so.11 -> libnppif.so.11.1.2.301\n",
            "\tlibnppial.so.11 -> libnppial.so.11.1.2.301\n",
            "\tlibnvjpeg.so.11 -> libnvjpeg.so.11.3.0.105\n",
            "\tlibcublas.so.11 -> libcublas.so.11.3.0.106\n",
            "\tlibaccinj64.so.11.1 -> libaccinj64.so.11.1.105\n",
            "\tlibcuinj64.so.11.1 -> libcuinj64.so.11.1.105\n",
            "\tlibnvperf_host.so -> libnvperf_host.so\n",
            "\tlibcupti.so.11.1 -> libcupti.so.2020.2.1\n",
            "\tlibnvperf_target.so -> libnvperf_target.so\n",
            "\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\n",
            "\tlibcudart.so.11.0 -> libcudart.so.11.1.74\n",
            "/usr/local/cuda-10.1/extras/CUPTI/lib64:\n",
            "\tlibcupti.so.10.1 -> libcupti.so.10.1.208\n",
            "\tlibnvperf_host.so -> libnvperf_host.so\n",
            "\tlibnvperf_target.so -> libnvperf_target.so\n",
            "/usr/local/lib:\n",
            "\tlibmkl_tbb_thread.so -> libmkl_tbb_thread.so\n",
            "\tlibmkl_gf_ilp64.so -> libmkl_gf_ilp64.so\n",
            "\tlibmkl_scalapack_ilp64.so -> libmkl_scalapack_ilp64.so\n",
            "\tlibmkl_vml_def.so -> libmkl_vml_def.so\n",
            "\tlibiomp5.so -> libiomp5.so\n",
            "\tlibmkl_avx2.so -> libmkl_avx2.so\n",
            "\tlibmkl_vml_avx512_mic.so -> libmkl_vml_avx512_mic.so\n",
            "\tlibmkl_def.so -> libmkl_def.so\n",
            "\tlibomptarget.rtl.x86_64.so -> libomptarget.rtl.x86_64.so\n",
            "\tlibmkl_vml_avx2.so -> libmkl_vml_avx2.so\n",
            "\tlibomptarget.so -> libomptarget.so\n",
            "\tlibmkl_vml_avx.so -> libmkl_vml_avx.so\n",
            "\tlibmkl_pgi_thread.so -> libmkl_pgi_thread.so\n",
            "\tlibmkl_rt.so -> libmkl_rt.so\n",
            "\tlibmkl_sequential.so -> libmkl_sequential.so\n",
            "\tlibiomp5_db.so -> libiomp5_db.so\n",
            "\tlibmkl_blacs_intelmpi_ilp64.so -> libmkl_blacs_intelmpi_ilp64.so\n",
            "\tlibmkl_core.so -> libmkl_core.so\n",
            "\tlibmkl_gf_lp64.so -> libmkl_gf_lp64.so\n",
            "\tlibmkl_intel_ilp64.so -> libmkl_intel_ilp64.so\n",
            "\tlibmkl_blacs_sgimpt_lp64.so -> libmkl_blacs_sgimpt_lp64.so\n",
            "\tlibmkl_vml_avx512.so -> libmkl_vml_avx512.so\n",
            "\tlibmkl_avx512.so -> libmkl_avx512.so\n",
            "\tlibmkl_blacs_openmpi_lp64.so -> libmkl_blacs_openmpi_lp64.so\n",
            "\tlibmkl_intel_lp64.so -> libmkl_intel_lp64.so\n",
            "\tlibomptarget.rtl.opencl.so -> libomptarget.rtl.opencl.so\n",
            "\tlibmkl_blacs_sgimpt_ilp64.so -> libmkl_blacs_sgimpt_ilp64.so\n",
            "\tlibiompstubs5.so -> libiompstubs5.so\n",
            "\tlibmkl_mc.so -> libmkl_mc.so\n",
            "\tlibmkl_mc3.so -> libmkl_mc3.so\n",
            "\tlibmkl_avx.so -> libmkl_avx.so\n",
            "\tlibomptarget.rtl.level0.so -> libomptarget.rtl.level0.so\n",
            "\tlibmkl_vml_cmpt.so -> libmkl_vml_cmpt.so\n",
            "\tlibmkl_blacs_openmpi_ilp64.so -> libmkl_blacs_openmpi_ilp64.so\n",
            "\tlibmkl_blacs_intelmpi_lp64.so -> libmkl_blacs_intelmpi_lp64.so\n",
            "\tlibmkl_cdft_core.so -> libmkl_cdft_core.so\n",
            "\tlibmkl_gnu_thread.so -> libmkl_gnu_thread.so\n",
            "\tlibmkl_intel_thread.so -> libmkl_intel_thread.so\n",
            "\tlibmkl_vml_mc.so -> libmkl_vml_mc.so\n",
            "\tlibmkl_avx512_mic.so -> libmkl_avx512_mic.so\n",
            "\tlibmkl_vml_mc3.so -> libmkl_vml_mc3.so\n",
            "\tlibmkl_vml_mc2.so -> libmkl_vml_mc2.so\n",
            "\tlibmkl_scalapack_lp64.so -> libmkl_scalapack_lp64.so\n",
            "\tlibsentencepiece.so.0 -> libsentencepiece.so.0.0.0\n",
            "\tlibsentencepiece_train.so.0 -> libsentencepiece_train.so.0.0.0\n",
            "/usr/local/lib/python3.7/dist-packages/ideep4py/lib:\n",
            "\tlibiomp5.so -> libiomp5.so\n",
            "\tlibmklml_intel.so -> libmklml_intel.so\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "\tlibmkldnn.so.0 -> libmkldnn.so.0.14.0\n",
            "/usr/local/cuda-10.0/targets/x86_64-linux/lib:\n",
            "\tlibnppif.so.10.0 -> libnppif.so.10.0.130\n",
            "\tlibnppicc.so.10.0 -> libnppicc.so.10.0.130\n",
            "\tlibnvblas.so.10.0 -> libnvblas.so.10.0.130\n",
            "\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\n",
            "\tlibnppitc.so.10.0 -> libnppitc.so.10.0.130\n",
            "\tlibcudart.so.10.0 -> libcudart.so.10.0.130\n",
            "\tlibnppisu.so.10.0 -> libnppisu.so.10.0.130\n",
            "\tlibnppc.so.10.0 -> libnppc.so.10.0.130\n",
            "\tlibcurand.so.10.0 -> libcurand.so.10.0.130\n",
            "\tlibnvrtc.so.10.0 -> libnvrtc.so.10.0.130\n",
            "\tlibnppicom.so.10.0 -> libnppicom.so.10.0.130\n",
            "\tlibcusparse.so.10.0 -> libcusparse.so.10.0.130\n",
            "\tlibnppig.so.10.0 -> libnppig.so.10.0.130\n",
            "\tlibnppial.so.10.0 -> libnppial.so.10.0.130\n",
            "\tlibnvgraph.so.10.0 -> libnvgraph.so.10.0.130\n",
            "\tlibnppim.so.10.0 -> libnppim.so.10.0.130\n",
            "\tlibcuinj64.so.10.0 -> libcuinj64.so.10.0.130\n",
            "\tlibcufftw.so.10.0 -> libcufftw.so.10.0.145\n",
            "\tlibcusolver.so.10.0 -> libcusolver.so.10.0.130\n",
            "\tlibnpps.so.10.0 -> libnpps.so.10.0.130\n",
            "\tlibnppidei.so.10.0 -> libnppidei.so.10.0.130\n",
            "\tlibcublas.so.10.0 -> libcublas.so.10.0.130\n",
            "\tlibnvjpeg.so.10.0 -> libnvjpeg.so.10.0.318\n",
            "\tlibcufft.so.10.0 -> libcufft.so.10.0.145\n",
            "\tlibnppist.so.10.0 -> libnppist.so.10.0.130\n",
            "\tlibaccinj64.so.10.0 -> libaccinj64.so.10.0.130\n",
            "\tlibnvrtc-builtins.so.10.0 -> libnvrtc-builtins.so.10.0.130\n",
            "\tlibOpenCL.so.1 -> libOpenCL.so.1.1\n",
            "/usr/local/cuda-10.1/targets/x86_64-linux/lib:\n",
            "\tlibnppitc.so.10 -> libnppitc.so.10.2.0.243\n",
            "\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\n",
            "\tlibnppidei.so.10 -> libnppidei.so.10.2.0.243\n",
            "\tlibcusparse.so.10 -> libcusparse.so.10.3.0.243\n",
            "\tlibnpps.so.10 -> libnpps.so.10.2.0.243\n",
            "\tlibaccinj64.so.10.1 -> libaccinj64.so.10.1.243\n",
            "\tlibcusolverMg.so.10 -> libcusolverMg.so.10.2.0.243\n",
            "\tlibnppc.so.10 -> libnppc.so.10.2.0.243\n",
            "\tlibnppist.so.10 -> libnppist.so.10.2.0.243\n",
            "\tlibcufftw.so.10 -> libcufftw.so.10.1.1.243\n",
            "\tlibnvjpeg.so.10 -> libnvjpeg.so.10.3.0.243\n",
            "\tlibcurand.so.10 -> libcurand.so.10.1.1.243\n",
            "\tlibnppig.so.10 -> libnppig.so.10.2.0.243\n",
            "\tlibnvgraph.so.10 -> libnvgraph.so.10.1.243\n",
            "\tlibcufft.so.10 -> libcufft.so.10.1.1.243\n",
            "\tlibcusolver.so.10 -> libcusolver.so.10.2.0.243\n",
            "\tlibnppicc.so.10 -> libnppicc.so.10.2.0.243\n",
            "\tlibnppim.so.10 -> libnppim.so.10.2.0.243\n",
            "\tlibnvrtc.so.10.1 -> libnvrtc.so.10.1.243\n",
            "\tlibcudart.so.10.1 -> libcudart.so.10.1.243\n",
            "\tlibnvrtc-builtins.so.10.1 -> libnvrtc-builtins.so.10.1.243\n",
            "\tlibnppicom.so.10 -> libnppicom.so.10.2.0.243\n",
            "\tlibnppif.so.10 -> libnppif.so.10.2.0.243\n",
            "\tlibnppisu.so.10 -> libnppisu.so.10.2.0.243\n",
            "\tlibcuinj64.so.10.1 -> libcuinj64.so.10.1.243\n",
            "\tlibOpenCL.so.1 -> libOpenCL.so.1.1\n",
            "\tlibnppial.so.10 -> libnppial.so.10.2.0.243\n",
            "/usr/local/cuda-11.0/targets/x86_64-linux/lib:\n",
            "\tlibnppidei.so.11 -> libnppidei.so.11.1.0.245\n",
            "\tlibnppisu.so.11 -> libnppisu.so.11.1.0.245\n",
            "\tlibnppicc.so.11 -> libnppicc.so.11.1.0.245\n",
            "\tlibnppc.so.11 -> libnppc.so.11.1.0.245\n",
            "\tlibnvrtc-builtins.so.11.0 -> libnvrtc-builtins.so.11.0.221\n",
            "\tlibcublasLt.so.11 -> libcublasLt.so.11.2.0.252\n",
            "\tlibnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\n",
            "\tlibcusolverMg.so.10 -> libcusolverMg.so.10.6.0.245\n",
            "\tlibnppist.so.11 -> libnppist.so.11.1.0.245\n",
            "\tlibnppial.so.11 -> libnppial.so.11.1.0.245\n",
            "\tlibcusparse.so.11 -> libcusparse.so.11.1.1.245\n",
            "\tlibcufftw.so.10 -> libcufftw.so.10.2.1.245\n",
            "\tlibnpps.so.11 -> libnpps.so.11.1.0.245\n",
            "\tlibcudart.so.11.0 -> libcudart.so.11.0.221\n",
            "\tlibcurand.so.10 -> libcurand.so.10.2.1.245\n",
            "\tlibcupti.so.11.0 -> libcupti.so.2020.1.1\n",
            "\tlibnppig.so.11 -> libnppig.so.11.1.0.245\n",
            "\tlibcufft.so.10 -> libcufft.so.10.2.1.245\n",
            "\tlibaccinj64.so.11.0 -> libaccinj64.so.11.0.221\n",
            "\tlibnppitc.so.11 -> libnppitc.so.11.1.0.245\n",
            "\tlibnppif.so.11 -> libnppif.so.11.1.0.245\n",
            "\tlibnvperf_host.so -> libnvperf_host.so\n",
            "\tlibcusolver.so.10 -> libcusolver.so.10.6.0.245\n",
            "\tlibnvblas.so.11 -> libnvblas.so.11.2.0.252\n",
            "\tlibcuinj64.so.11.0 -> libcuinj64.so.11.0.221\n",
            "\tlibnvjpeg.so.11 -> libnvjpeg.so.11.1.1.245\n",
            "\tlibnvrtc.so.11.0 -> libnvrtc.so.11.0.221\n",
            "\tlibnppim.so.11 -> libnppim.so.11.1.0.245\n",
            "\tlibnvperf_target.so -> libnvperf_target.so\n",
            "\tlibcublas.so.11 -> libcublas.so.11.2.0.252\n",
            "\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\n",
            "/lib/x86_64-linux-gnu:\n",
            "\tlibreadline.so.7 -> libreadline.so.7.0\n",
            "\tlibhistory.so.7 -> libhistory.so.7.0\n",
            "\tlibdevmapper.so.1.02.1 -> libdevmapper.so.1.02.1\n",
            "\tlibwrap.so.0 -> libwrap.so.0.7.6\n",
            "\tlibcryptsetup.so.12 -> libcryptsetup.so.12.2.0\n",
            "\tlibusb-1.0.so.0 -> libusb-1.0.so.0.1.0\n",
            "\tlibulockmgr.so.1 -> libulockmgr.so.1.0.1\n",
            "\tlibidn.so.11 -> libidn.so.11.6.16\n",
            "\tlibexpat.so.1 -> libexpat.so.1.6.7\n",
            "\tlibslang.so.2 -> libslang.so.2.3.1\n",
            "\tlibdbus-1.so.3 -> libdbus-1.so.3.19.4\n",
            "\tlibfuse.so.2 -> libfuse.so.2.9.7\n",
            "\tlibaio.so.1 -> libaio.so.1.0.1\n",
            "\tlibnl-3.so.200 -> libnl-3.so.200.24.0\n",
            "\tlibjson-c.so.3 -> libjson-c.so.3.0.1\n",
            "\tlibkmod.so.2 -> libkmod.so.2.3.2\n",
            "\tlibcap.so.2 -> libcap.so.2.25\n",
            "\tlibkeyutils.so.1 -> libkeyutils.so.1.5\n",
            "\tlibbsd.so.0 -> libbsd.so.0.8.7\n",
            "\tlibapparmor.so.1 -> libapparmor.so.1.4.2\n",
            "\tliblzo2.so.2 -> liblzo2.so.2.0.0\n",
            "\tlibpcprofile.so -> libpcprofile.so\n",
            "\tlibpamc.so.0 -> libpamc.so.0.82.1\n",
            "\tlibsepol.so.1 -> libsepol.so.1\n",
            "\tlibncurses.so.5 -> libncurses.so.5.9\n",
            "\tlibgcc_s.so.1 -> libgcc_s.so.1\n",
            "\tlibpam.so.0 -> libpam.so.0.83.1\n",
            "\tlibmemusage.so -> libmemusage.so\n",
            "\tlibcap-ng.so.0 -> libcap-ng.so.0.0.0\n",
            "\tlibpam_misc.so.0 -> libpam_misc.so.0.82.0\n",
            "\tlibncursesw.so.5 -> libncursesw.so.5.9\n",
            "\tlibprocps.so.6 -> libprocps.so.6.0.0\n",
            "\tlibudev.so.1 -> libudev.so.1.6.9\n",
            "\tlibcrypt.so.1 -> libcrypt-2.27.so\n",
            "\tlibnsl.so.1 -> libnsl-2.27.so\n",
            "\tlibbz2.so.1.0 -> libbz2.so.1.0.4\n",
            "\tlibutil.so.1 -> libutil-2.27.so\n",
            "\tlibpcre.so.3 -> libpcre.so.3.13.3\n",
            "\tlibm.so.6 -> libm-2.27.so\n",
            "\tlibe2p.so.2 -> libe2p.so.2.3\n",
            "\tlibnss_dns.so.2 -> libnss_dns-2.27.so\n",
            "\tlibblkid.so.1 -> libblkid.so.1.1.0\n",
            "\tlibdl.so.2 -> libdl-2.27.so\n",
            "\tlibtinfo.so.5 -> libtinfo.so.5.9\n",
            "\tlibnss_compat.so.2 -> libnss_compat-2.27.so\n",
            "\tlibc.so.6 -> libc-2.27.so\n",
            "\tlibresolv.so.2 -> libresolv-2.27.so\n",
            "/sbin/ldconfig.real: /lib/x86_64-linux-gnu/ld-2.27.so is the dynamic linker, ignoring\n",
            "\n",
            "\tld-linux-x86-64.so.2 -> ld-2.27.so\n",
            "\tlibBrokenLocale.so.1 -> libBrokenLocale-2.27.so\n",
            "\tlibmount.so.1 -> libmount.so.1.1.0\n",
            "\tlibaudit.so.1 -> libaudit.so.1.0.0\n",
            "\tlibpthread.so.0 -> libpthread-2.27.so\n",
            "\tlibmvec.so.1 -> libmvec-2.27.so\n",
            "\tlibselinux.so.1 -> libselinux.so.1\n",
            "\tlibthread_db.so.1 -> libthread_db-1.0.so\n",
            "\tlibnss_hesiod.so.2 -> libnss_hesiod-2.27.so\n",
            "\tlibgcrypt.so.20 -> libgcrypt.so.20.2.1\n",
            "\tlibuuid.so.1 -> libuuid.so.1.3.0\n",
            "\tlibgpg-error.so.0 -> libgpg-error.so.0.22.0\n",
            "\tlibseccomp.so.2 -> libseccomp.so.2.4.3\n",
            "\tlibattr.so.1 -> libattr.so.1.1.0\n",
            "\tlibacl.so.1 -> libacl.so.1.1.0\n",
            "\tlibsmartcols.so.1 -> libsmartcols.so.1.1.0\n",
            "\tlibnss_files.so.2 -> libnss_files-2.27.so\n",
            "\tlibcidn.so.1 -> libcidn-2.27.so\n",
            "\tlibsystemd.so.0 -> libsystemd.so.0.21.0\n",
            "\tlibrt.so.1 -> librt-2.27.so\n",
            "\tlibnss_nis.so.2 -> libnss_nis-2.27.so\n",
            "\tlibcom_err.so.2 -> libcom_err.so.2.1\n",
            "\tlibnss_nisplus.so.2 -> libnss_nisplus-2.27.so\n",
            "\tlibanl.so.1 -> libanl-2.27.so\n",
            "\tlibSegFault.so -> libSegFault.so\n",
            "\tlibfdisk.so.1 -> libfdisk.so.1.1.0\n",
            "\tlibz.so.1 -> libz.so.1.2.11\n",
            "\tliblzma.so.5 -> liblzma.so.5.2.2\n",
            "\tlibext2fs.so.2 -> libext2fs.so.2.4\n",
            "\tlibss.so.2 -> libss.so.2.0\n",
            "/usr/lib/x86_64-linux-gnu:\n",
            "\tlibroken.so.18 -> libroken.so.18.1.0\n",
            "\tlibsasl2.so.2 -> libsasl2.so.2.0.25\n",
            "\tlibhx509.so.5 -> libhx509.so.5.0.0\n",
            "\tlibheimntlm.so.0 -> libheimntlm.so.0.1.0\n",
            "\tlibhcrypto.so.4 -> libhcrypto.so.4.1.0\n",
            "\tliblber-2.4.so.2 -> liblber-2.4.so.2.10.8\n",
            "\tlibwind.so.0 -> libwind.so.0.0.0\n",
            "\tlibasn1.so.8 -> libasn1.so.8.0.0\n",
            "\tlibkrb5.so.26 -> libkrb5.so.26.0.0\n",
            "\tlibheimbase.so.1 -> libheimbase.so.1.0.0\n",
            "\tlibsqlite3.so.0 -> libsqlite3.so.0.8.6\n",
            "\tlibgssapi.so.3 -> libgssapi.so.3.0.0\n",
            "\tlibassuan.so.0 -> libassuan.so.0.8.1\n",
            "\tlibksba.so.8 -> libksba.so.8.11.6\n",
            "\tlibnpth.so.0 -> libnpth.so.0.1.1\n",
            "\tlibldap_r-2.4.so.2 -> libldap_r-2.4.so.2.10.8\n",
            "\tlibnccl.so.2 -> libnccl.so.2.7.8\n",
            "\tlibitm.so.1 -> libitm.so.1.0.0\n",
            "\tlibmpfr.so.6 -> libmpfr.so.6.0.1\n",
            "\tlibcc1.so.0 -> libcc1.so.0.0.0\n",
            "\tlibgdbm_compat.so.4 -> libgdbm_compat.so.4.0.0\n",
            "\tliblsan.so.0 -> liblsan.so.0.0.0\n",
            "\tlibmpc.so.3 -> libmpc.so.3.1.0\n",
            "\tlibgdbm.so.5 -> libgdbm.so.5.0.0\n",
            "\tlibtsan.so.0 -> libtsan.so.0.0.0\n",
            "\tlibopcodes-2.30-system.so -> libopcodes-2.30-system.so\n",
            "\tlibquadmath.so.0 -> libquadmath.so.0.0.0\n",
            "\tlibubsan.so.0 -> libubsan.so.0.0.0\n",
            "\tlibasan.so.4 -> libasan.so.4.0.0\n",
            "\tlibatomic.so.1 -> libatomic.so.1.2.0\n",
            "\tlibcilkrts.so.5 -> libcilkrts.so.5.0.0\n",
            "\tlibperl.so.5.26 -> libperl.so.5.26.1\n",
            "\tlibmpxwrappers.so.2 -> libmpxwrappers.so.2.0.1\n",
            "\tlibbfd-2.30-system.so -> libbfd-2.30-system.so\n",
            "\tlibgomp.so.1 -> libgomp.so.1.0.0\n",
            "\tlibmpx.so.2 -> libmpx.so.2.0.1\n",
            "\tlibisl.so.19 -> libisl.so.19.0.0\n",
            "\tlibcudnn_cnn_infer.so.8 -> libcudnn_cnn_infer.so.8.0.5\n",
            "\tlibcudnn_adv_infer.so.8 -> libcudnn_adv_infer.so.8.0.5\n",
            "\tlibcudnn.so.8 -> libcudnn.so.8.0.5\n",
            "\tlibcudnn_adv_train.so.8 -> libcudnn_adv_train.so.8.0.5\n",
            "\tlibcudnn_ops_train.so.8 -> libcudnn_ops_train.so.8.0.5\n",
            "\tlibcudnn_cnn_train.so.8 -> libcudnn_cnn_train.so.8.0.5\n",
            "\tlibcudnn_ops_infer.so.8 -> libcudnn_ops_infer.so.8.0.5\n",
            "\tlibboost_locale.so.1.65.1 -> libboost_locale.so.1.65.1\n",
            "\tlibboost_wserialization.so.1.65.1 -> libboost_wserialization.so.1.65.1\n",
            "\tlibflite_usenglish.so.1 -> libflite_usenglish.so.2.1\n",
            "\tlibvtkImagingHybridTCL-6.3.so.6.3 -> libvtkImagingHybridTCL-6.3.so.6.3.0\n",
            "\tlibboost_system.so.1.65.1 -> libboost_system.so.1.65.1\n",
            "\tlibvtkIOVPICTCL-6.3.so.6.3 -> libvtkIOVPICTCL-6.3.so.6.3.0\n",
            "\tlibopencv_xphoto.so.3.2 -> libopencv_xphoto.so.3.2.0\n",
            "\tlibvtkIOGeoJSONPython27D-6.3.so.6.3 -> libvtkIOGeoJSONPython27D-6.3.so.6.3.0\n",
            "\tlibboost_math_tr1.so.1.65.1 -> libboost_math_tr1.so.1.65.1\n",
            "\tlibvtkIOFFMPEGPython27D-6.3.so.6.3 -> libvtkIOFFMPEGPython27D-6.3.so.6.3.0\n",
            "\tlibicui18n.so.60 -> libicui18n.so.60.2\n",
            "\tlibboost_regex.so.1.65.1 -> libboost_regex.so.1.65.1\n",
            "\tlibboost_random.so.1.65.1 -> libboost_random.so.1.65.1\n",
            "\tlibcrypto.so.1.1 -> libcrypto.so.1.1\n",
            "\tlibopenjp2.so.7 -> libopenjp2.so.2.3.0\n",
            "\tlibboost_graph_parallel.so.1.65.1 -> libboost_graph_parallel.so.1.65.1\n",
            "\tlibboost_timer.so.1.65.1 -> libboost_timer.so.1.65.1\n",
            "\tlibvtkIOSQL-6.3.so.6.3 -> libvtkIOSQL-6.3.so.6.3.0\n",
            "\tlibvtkCommonColorPython27D-6.3.so.6.3 -> libvtkCommonColorPython27D-6.3.so.6.3.0\n",
            "\tlibgfortran.so.4 -> libgfortran.so.4.0.0\n",
            "\tlibopenal.so.1 -> libopenal.so.1.18.2\n",
            "\tlibopencv_highgui.so.3.2 -> libopencv_highgui.so.3.2.0\n",
            "\tlibvtkIOImageTCL-6.3.so.6.3 -> libvtkIOImageTCL-6.3.so.6.3.0\n",
            "\tlibsndfile.so.1 -> libsndfile.so.1.0.28\n",
            "\tlibmca_common_verbs.so.20 -> libmca_common_verbs.so.20.10.0\n",
            "\tlibvtkCommonComputationalGeometryPython27D-6.3.so.6.3 -> libvtkCommonComputationalGeometryPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOXMLPython27D-6.3.so.6.3 -> libvtkIOXMLPython27D-6.3.so.6.3.0\n",
            "\tlibX11.so.6 -> libX11.so.6.3.0\n",
            "\tlibqhull.so.7 -> libqhull.so.7.2.0\n",
            "\tlibxcb-present.so.0 -> libxcb-present.so.0.0.0\n",
            "\tlibx265.so.146 -> libx265.so.146\n",
            "\tlibnss3.so -> libnss3.so\n",
            "\tlibvtkIOParallelExodus-6.3.so.6.3 -> libvtkIOParallelExodus-6.3.so.6.3.0\n",
            "\tlibGLX.so.0 -> libGLX.so.0.0.0\n",
            "\tlibvtkIONetCDF-6.3.so.6.3 -> libvtkIONetCDF-6.3.so.6.3.0\n",
            "\tlibvtkParallelCorePython27D-6.3.so.6.3 -> libvtkParallelCorePython27D-6.3.so.6.3.0\n",
            "\tlibhdf5_serial_hl.so.100 -> libhdf5_serial_hl.so.100.0.0\n",
            "\tlibvtkIOParallelLSDynaTCL-6.3.so.6.3 -> libvtkIOParallelLSDynaTCL-6.3.so.6.3.0\n",
            "\tlibvtkViewsCore-6.3.so.6.3 -> libvtkViewsCore-6.3.so.6.3.0\n",
            "\tlibopencv_calib3d.so.3.2 -> libopencv_calib3d.so.3.2.0\n",
            "\tlibvtkFiltersHybridTCL-6.3.so.6.3 -> libvtkFiltersHybridTCL-6.3.so.6.3.0\n",
            "\tlibvtkCommonSystemPython27D-6.3.so.6.3 -> libvtkCommonSystemPython27D-6.3.so.6.3.0\n",
            "\tlibnvidia-glcore.so.510.47.03 -> libnvidia-glcore.so.510.47.03\n",
            "\tlibgobject-2.0.so.0 -> libgobject-2.0.so.0.5600.4\n",
            "\tlibnvidia-ml.so.1 -> libnvidia-ml.so.510.47.03\n",
            "\tlibxcb-xinerama.so.0 -> libxcb-xinerama.so.0.0.0\n",
            "\tlibass.so.9 -> libass.so.9.0.2\n",
            "\tlibvtkRenderingParallelTCL-6.3.so.6.3 -> libvtkRenderingParallelTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOXdmf2Python27D-6.3.so.6.3 -> libvtkIOXdmf2Python27D-6.3.so.6.3.0\n",
            "\tlibopencv_imgcodecs.so.3.2 -> libopencv_imgcodecs.so.3.2.0\n",
            "\tlibhdf5_openmpi.so.100 -> libhdf5_openmpi.so.100.0.1\n",
            "\tlibvtkParallelMPI4Py-6.3.so.6.3 -> libvtkParallelMPI4Py-6.3.so.6.3.0\n",
            "\tlibgc.so.1 -> libgc.so.1.0.3\n",
            "\tlibboost_prg_exec_monitor.so.1.65.1 -> libboost_prg_exec_monitor.so.1.65.1\n",
            "\tlibtcmalloc.so.4 -> libtcmalloc.so.4.3.0\n",
            "\tlibEGL.so.1 -> libEGL.so.1.0.0\n",
            "\tlibfreetype.so.6 -> libfreetype.so.6.15.0\n",
            "\tlibvtkFiltersParallelGeometry-6.3.so.6.3 -> libvtkFiltersParallelGeometry-6.3.so.6.3.0\n",
            "\tlibvtkIOPLY-6.3.so.6.3 -> libvtkIOPLY-6.3.so.6.3.0\n",
            "\tlibvtkChartsCorePython27D-6.3.so.6.3 -> libvtkChartsCorePython27D-6.3.so.6.3.0\n",
            "\tlibwebpmux.so.3 -> libwebpmux.so.3.0.1\n",
            "\tlibtcmalloc_debug.so.4 -> libtcmalloc_debug.so.4.3.0\n",
            "\tlibvtkImagingHybrid-6.3.so.6.3 -> libvtkImagingHybrid-6.3.so.6.3.0\n",
            "\tlibvtkFiltersVerdictTCL-6.3.so.6.3 -> libvtkFiltersVerdictTCL-6.3.so.6.3.0\n",
            "\tlibfyba.so.0 -> libfyba.so.0.0.0\n",
            "\tlibxcb-image.so.0 -> libxcb-image.so.0.0.0\n",
            "\tlibvtkViewsCoreTCL-6.3.so.6.3 -> libvtkViewsCoreTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOMovie-6.3.so.6.3 -> libvtkIOMovie-6.3.so.6.3.0\n",
            "\tlibGLdispatch.so.0 -> libGLdispatch.so.0.0.0\n",
            "\tlibopencv_photo.so.3.2 -> libopencv_photo.so.3.2.0\n",
            "\tlibnvidia-glsi.so.510.47.03 -> libnvidia-glsi.so.510.47.03\n",
            "\tlibvtkFiltersSourcesTCL-6.3.so.6.3 -> libvtkFiltersSourcesTCL-6.3.so.6.3.0\n",
            "\tlibGLU.so.1 -> libGLU.so.1.3.1\n",
            "\tlibsocket++.so.1 -> libsocket++.so.1.0.2\n",
            "\tlibboost_stacktrace_basic.so.1.65.1 -> libboost_stacktrace_basic.so.1.65.1\n",
            "\tliblcms2.so.2 -> liblcms2.so.2.0.8\n",
            "\tlibvtkIOMINCTCL-6.3.so.6.3 -> libvtkIOMINCTCL-6.3.so.6.3.0\n",
            "\tlibboost_wave.so.1.65.1 -> libboost_wave.so.1.65.1\n",
            "\tlibvtkFiltersSMPPython27D-6.3.so.6.3 -> libvtkFiltersSMPPython27D-6.3.so.6.3.0\n",
            "\tlibltdl.so.7 -> libltdl.so.7.3.1\n",
            "\tlibrom1394.so.0 -> librom1394.so.0.3.0\n",
            "\tlibvtkCommonTransforms-6.3.so.6.3 -> libvtkCommonTransforms-6.3.so.6.3.0\n",
            "\tlibgdk_pixbuf-2.0.so.0 -> libgdk_pixbuf-2.0.so.0.3611.0\n",
            "\tlibQt5EglFSDeviceIntegration.so.5 -> libQt5EglFSDeviceIntegration.so.5.9.5\n",
            "\tlibmtdev.so.1 -> libmtdev.so.1.0.0\n",
            "\tlibboost_math_c99f.so.1.65.1 -> libboost_math_c99f.so.1.65.1\n",
            "\tlibf77blas.so.3 -> libf77blas.so.3.10.3\n",
            "\tlibvtkRenderingContextIIDTCL-6.3.so.6.3 -> libvtkRenderingContextIIDTCL-6.3.so.6.3.0\n",
            "\tlibopencv_videoio.so.3.2 -> libopencv_videoio.so.3.2.0\n",
            "\tlibdc1394.so.22 -> libdc1394.so.22.2.1\n",
            "\tlibvtkRenderingLabelTCL-6.3.so.6.3 -> libvtkRenderingLabelTCL-6.3.so.6.3.0\n",
            "\tlibvtkCommonComputationalGeometryTCL-6.3.so.6.3 -> libvtkCommonComputationalGeometryTCL-6.3.so.6.3.0\n",
            "\tlibflite_cmu_us_kal16.so.1 -> libflite_cmu_us_kal16.so.2.1\n",
            "\tlibgdcmIOD.so.2.8 -> libgdcmIOD.so.2.8.4\n",
            "\tlibXmuu.so.1 -> libXmuu.so.1.0.0\n",
            "\tlibvtkRenderingImageTCL-6.3.so.6.3 -> libvtkRenderingImageTCL-6.3.so.6.3.0\n",
            "\tlibnetcdf.so.13 -> libnetcdf.so.13\n",
            "\tlibLLVM-6.0.so.1 -> libLLVM-6.0.so.1\n",
            "\tlibtiff.so.5 -> libtiff.so.5.3.0\n",
            "\tlibrdmacm.so.1 -> librdmacm.so.1.1.17.1\n",
            "\tlibsoup-gnome-2.4.so.1 -> libsoup-gnome-2.4.so.1.8.0\n",
            "\tlibboost_date_time.so.1.65.1 -> libboost_date_time.so.1.65.1\n",
            "\tlibpulse.so.0 -> libpulse.so.0.20.2\n",
            "\tlibboost_program_options.so.1.65.1 -> libboost_program_options.so.1.65.1\n",
            "\tlibcroco-0.6.so.3 -> libcroco-0.6.so.3.0.1\n",
            "\tlibgccpp.so.1 -> libgccpp.so.1.0.3\n",
            "\tlibXmu.so.6 -> libXmu.so.6.2.0\n",
            "\tlibpciaccess.so.0 -> libpciaccess.so.0.11.1\n",
            "\tlibvtkTestingIOSQL-6.3.so.6.3 -> libvtkTestingIOSQL-6.3.so.6.3.0\n",
            "\tlibpolkit-gobject-1.so.0 -> libpolkit-gobject-1.so.0.0.0\n",
            "\tlibnvidia-rtcore.so.510.47.03 -> libnvidia-rtcore.so.510.47.03\n",
            "\tlibtcmalloc_minimal_debug.so.4 -> libtcmalloc_minimal_debug.so.4.3.0\n",
            "\tlibsnappy.so.1 -> libsnappy.so.1.1.7\n",
            "\tlibxcb-keysyms.so.1 -> libxcb-keysyms.so.1.0.0\n",
            "\tlibssl3.so -> libssl3.so\n",
            "\tlibvtkRenderingMatplotlibPython27D-6.3.so.6.3 -> libvtkRenderingMatplotlibPython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingContextOpenGLTCL-6.3.so.6.3 -> libvtkRenderingContextOpenGLTCL-6.3.so.6.3.0\n",
            "\tlibQt5Concurrent.so.5 -> libQt5Concurrent.so.5.9.5\n",
            "\tlibvtkIOEnSight-6.3.so.6.3 -> libvtkIOEnSight-6.3.so.6.3.0\n",
            "\tlibmbedtls.so.12 -> libmbedtls.so.2.16.0\n",
            "\tlibvtkIOImportTCL-6.3.so.6.3 -> libvtkIOImportTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOMySQLPython27D-6.3.so.6.3 -> libvtkIOMySQLPython27D-6.3.so.6.3.0\n",
            "\tlibsmime3.so -> libsmime3.so\n",
            "\tlibjpeg.so.8 -> libjpeg.so.8.1.2\n",
            "\tlibvtkIOVPICPython27D-6.3.so.6.3 -> libvtkIOVPICPython27D-6.3.so.6.3.0\n",
            "\tlibboost_stacktrace_addr2line.so.1.65.1 -> libboost_stacktrace_addr2line.so.1.65.1\n",
            "\tlibQt5EglFsKmsSupport.so.5 -> libQt5EglFsKmsSupport.so.5.9.5\n",
            "\tlibplds4.so -> libplds4.so\n",
            "\tlibvtkIOParallelTCL-6.3.so.6.3 -> libvtkIOParallelTCL-6.3.so.6.3.0\n",
            "\tlibboost_math_c99l.so.1.65.1 -> libboost_math_c99l.so.1.65.1\n",
            "\tlibvtkIOExodus-6.3.so.6.3 -> libvtkIOExodus-6.3.so.6.3.0\n",
            "\tlibvtkFiltersSourcesPython27D-6.3.so.6.3 -> libvtkFiltersSourcesPython27D-6.3.so.6.3.0\n",
            "\tlibpango-1.0.so.0 -> libpango-1.0.so.0.4000.14\n",
            "\tlibnuma.so.1 -> libnuma.so.1.0.0\n",
            "\tlibdouble-conversion.so.1 -> libdouble-conversion.so.1.0\n",
            "\tlibvtkIOLSDynaTCL-6.3.so.6.3 -> libvtkIOLSDynaTCL-6.3.so.6.3.0\n",
            "\tlibopencv_core.so.3.2 -> libopencv_core.so.3.2.0\n",
            "\tlibXt.so.6 -> libXt.so.6.0.0\n",
            "\tlibvtkFiltersAMR-6.3.so.6.3 -> libvtkFiltersAMR-6.3.so.6.3.0\n",
            "\tlibatk-1.0.so.0 -> libatk-1.0.so.0.22810.1\n",
            "\tlibgphoto2.so.6 -> libgphoto2.so.6.0.0\n",
            "\tlibvtkRenderingFreeTypePython27D-6.3.so.6.3 -> libvtkRenderingFreeTypePython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOInfovis-6.3.so.6.3 -> libvtkIOInfovis-6.3.so.6.3.0\n",
            "\tlibvtkImagingSourcesPython27D-6.3.so.6.3 -> libvtkImagingSourcesPython27D-6.3.so.6.3.0\n",
            "\tlibssh-gcrypt.so.4 -> libssh-gcrypt.so.4.5.0\n",
            "\tlibwebpdemux.so.2 -> libwebpdemux.so.2.0.3\n",
            "\tlibmpi_usempi_ignore_tkr.so.20 -> libmpi_usempi_ignore_tkr.so.20.10.0\n",
            "\tlibvtkRenderingExternal-6.3.so.6.3 -> libvtkRenderingExternal-6.3.so.6.3.0\n",
            "\tlibnvidia-fbc.so.1 -> libnvidia-fbc.so.510.47.03\n",
            "\tlibvtksys-6.3.so.6.3 -> libvtksys-6.3.so.6.3.0\n",
            "\tlibvtkRenderingLabelPython27D-6.3.so.6.3 -> libvtkRenderingLabelPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersFlowPathsPython27D-6.3.so.6.3 -> libvtkFiltersFlowPathsPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOMoviePython27D-6.3.so.6.3 -> libvtkIOMoviePython27D-6.3.so.6.3.0\n",
            "\tlibvtkViewsGeovisTCL-6.3.so.6.3 -> libvtkViewsGeovisTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOParallelLSDynaPython27D-6.3.so.6.3 -> libvtkIOParallelLSDynaPython27D-6.3.so.6.3.0\n",
            "\tlibvtkInteractionWidgetsTCL-6.3.so.6.3 -> libvtkInteractionWidgetsTCL-6.3.so.6.3.0\n",
            "\tlibx264.so.152 -> libx264.so.152\n",
            "\tlibxcb-sync.so.1 -> libxcb-sync.so.1.0.0\n",
            "\tlibwayland-cursor.so.0 -> libwayland-cursor.so.0.0.0\n",
            "\tlibavahi-client.so.3 -> libavahi-client.so.3.2.9\n",
            "\tlibgthread-2.0.so.0 -> libgthread-2.0.so.0.5600.4\n",
            "\tlibicutu.so.60 -> libicutu.so.60.2\n",
            "\tlibXNVCtrl.so.0 -> libXNVCtrl.so.0.0.0\n",
            "\tlibvtkRenderingMatplotlibTCL-6.3.so.6.3 -> libvtkRenderingMatplotlibTCL-6.3.so.6.3.0\n",
            "\tlibevent-2.1.so.6 -> libevent-2.1.so.6.0.2\n",
            "\tlibvtkIOInfovisPython27D-6.3.so.6.3 -> libvtkIOInfovisPython27D-6.3.so.6.3.0\n",
            "\tlibminizip.so.1 -> libminizip.so.1.0.0\n",
            "\tlibepsilon.so.1 -> libepsilon.so.1.0.0\n",
            "\tlibvtkIOODBC-6.3.so.6.3 -> libvtkIOODBC-6.3.so.6.3.0\n",
            "\tlibhdf5_serialhl_fortran.so.100 -> libhdf5_serialhl_fortran.so.100.0.0\n",
            "\tlibvtkIOPLYTCL-6.3.so.6.3 -> libvtkIOPLYTCL-6.3.so.6.3.0\n",
            "\tlibvtkImagingHybridPython27D-6.3.so.6.3 -> libvtkImagingHybridPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersHyperTreeTCL-6.3.so.6.3 -> libvtkFiltersHyperTreeTCL-6.3.so.6.3.0\n",
            "\tlibEGL_nvidia.so.0 -> libEGL_nvidia.so.510.47.03\n",
            "\tlibvtkIOParallelXML-6.3.so.6.3 -> libvtkIOParallelXML-6.3.so.6.3.0\n",
            "\tlibX11-xcb.so.1 -> libX11-xcb.so.1.0.0\n",
            "\tlibvtkIOImage-6.3.so.6.3 -> libvtkIOImage-6.3.so.6.3.0\n",
            "\tlibvtkRenderingParallelLIC-6.3.so.6.3 -> libvtkRenderingParallelLIC-6.3.so.6.3.0\n",
            "\tlibvtkFiltersGeneralPython27D-6.3.so.6.3 -> libvtkFiltersGeneralPython27D-6.3.so.6.3.0\n",
            "\tlibvtkTestingGenericBridge-6.3.so.6.3 -> libvtkTestingGenericBridge-6.3.so.6.3.0\n",
            "\tlibvtkFiltersSMPTCL-6.3.so.6.3 -> libvtkFiltersSMPTCL-6.3.so.6.3.0\n",
            "\tlibkrb5support.so.0 -> libkrb5support.so.0.1\n",
            "\tlibvtkRenderingGLtoPSTCL-6.3.so.6.3 -> libvtkRenderingGLtoPSTCL-6.3.so.6.3.0\n",
            "\tlibsuperlu.so.5 -> libsuperlu.so.5.2.1\n",
            "\tlibxvidcore.so.4 -> libxvidcore.so.4.3\n",
            "\tliburiparser.so.1 -> liburiparser.so.1.0.20\n",
            "\tlibvtkIOFFMPEG-6.3.so.6.3 -> libvtkIOFFMPEG-6.3.so.6.3.0\n",
            "\tlibwayland-client.so.0 -> libwayland-client.so.0.3.0\n",
            "\tlibvtkParallelCore-6.3.so.6.3 -> libvtkParallelCore-6.3.so.6.3.0\n",
            "\tlibcdt.so.5 -> libcdt.so.5.0.0\n",
            "\tlibopencv_dpm.so.3.2 -> libopencv_dpm.so.3.2.0\n",
            "\tlibvtkRenderingParallelLICTCL-6.3.so.6.3 -> libvtkRenderingParallelLICTCL-6.3.so.6.3.0\n",
            "\tlibicudata.so.60 -> libicudata.so.60.2\n",
            "\tlibvtkImagingStencil-6.3.so.6.3 -> libvtkImagingStencil-6.3.so.6.3.0\n",
            "\tlibvtkFiltersPython-6.3.so.6.3 -> libvtkFiltersPython-6.3.so.6.3.0\n",
            "\tlibavformat.so.57 -> libavformat.so.57.83.100\n",
            "\tlibvtkRenderingGL2PSPython27D-6.3.so.6.3 -> libvtkRenderingGL2PSPython27D-6.3.so.6.3.0\n",
            "\tlibfftw3_threads.so.3 -> libfftw3_threads.so.3.5.7\n",
            "\tliblapack_atlas.so.3 -> liblapack_atlas.so.3.10.3\n",
            "\tlibwebp.so.6 -> libwebp.so.6.0.2\n",
            "\tlibvtkFiltersGenericPython27D-6.3.so.6.3 -> libvtkFiltersGenericPython27D-6.3.so.6.3.0\n",
            "\tlibgvc.so.6 -> libgvc.so.6.0.0\n",
            "\tlibvtkFiltersExtractionTCL-6.3.so.6.3 -> libvtkFiltersExtractionTCL-6.3.so.6.3.0\n",
            "\tlibdatrie.so.1 -> libdatrie.so.1.3.3\n",
            "\tlibnvblas.so.10 -> libnvblas.so.10.2.1.243\n",
            "\tlibpostproc.so.54 -> libpostproc.so.54.7.100\n",
            "\tlibvtkImagingGeneralPython27D-6.3.so.6.3 -> libvtkImagingGeneralPython27D-6.3.so.6.3.0\n",
            "\tlibvtkChartsCoreTCL-6.3.so.6.3 -> libvtkChartsCoreTCL-6.3.so.6.3.0\n",
            "\tlibhdf5_openmpi_fortran.so.100 -> libhdf5_openmpi_fortran.so.100.0.1\n",
            "\tlibvtkFiltersAMRPython27D-6.3.so.6.3 -> libvtkFiltersAMRPython27D-6.3.so.6.3.0\n",
            "\tlibvtkverdict-6.3.so.6.3 -> libvtkverdict-6.3.so.6.3.0\n",
            "\tlibgdk-3.so.0 -> libgdk-3.so.0.2200.30\n",
            "\tlibvtkIOXMLTCL-6.3.so.6.3 -> libvtkIOXMLTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersGeometryTCL-6.3.so.6.3 -> libvtkFiltersGeometryTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOGDALPython27D-6.3.so.6.3 -> libvtkIOGDALPython27D-6.3.so.6.3.0\n",
            "\tlibopencv_saliency.so.3.2 -> libopencv_saliency.so.3.2.0\n",
            "\tlibboost_stacktrace_noop.so.1.65.1 -> libboost_stacktrace_noop.so.1.65.1\n",
            "\tlibjbig.so.0 -> libjbig.so.0\n",
            "\tlibcairo.so.2 -> libcairo.so.2.11510.0\n",
            "\tlibnvidia-nvvm.so.4 -> libnvidia-nvvm.so.4.0.0\n",
            "\tlibharfbuzz.so.0 -> libharfbuzz.so.0.10702.0\n",
            "\tlibvtkDICOMParser-6.3.so.6.3 -> libvtkDICOMParser-6.3.so.6.3.0\n",
            "\tlibboost_iostreams.so.1.65.1 -> libboost_iostreams.so.1.65.1\n",
            "\tlibvtkIOImport-6.3.so.6.3 -> libvtkIOImport-6.3.so.6.3.0\n",
            "\tlibpolkit-agent-1.so.0 -> libpolkit-agent-1.so.0.0.0\n",
            "\tlibtbbmalloc_proxy.so.2 -> libtbbmalloc_proxy.so.2\n",
            "\tlibvtkImagingStatisticsTCL-6.3.so.6.3 -> libvtkImagingStatisticsTCL-6.3.so.6.3.0\n",
            "\tlibXft.so.2 -> libXft.so.2.3.2\n",
            "\tlibvtkIOMINC-6.3.so.6.3 -> libvtkIOMINC-6.3.so.6.3.0\n",
            "\tlibopencv_features2d.so.3.2 -> libopencv_features2d.so.3.2.0\n",
            "\tlibarpack.so.2 -> libarpack.so.2.0.0\n",
            "\tlibvtkIOGeometryPython27D-6.3.so.6.3 -> libvtkIOGeometryPython27D-6.3.so.6.3.0\n",
            "\tlibsoxr.so.0 -> libsoxr.so.0.1.1\n",
            "\tlibvtkIOCoreTCL-6.3.so.6.3 -> libvtkIOCoreTCL-6.3.so.6.3.0\n",
            "\tlibnvidia-opencl.so.1 -> libnvidia-opencl.so.510.47.03\n",
            "\tlibmpg123.so.0 -> libmpg123.so.0.44.8\n",
            "\tlibvtkIOMySQLTCL-6.3.so.6.3 -> libvtkIOMySQLTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOCorePython27D-6.3.so.6.3 -> libvtkIOCorePython27D-6.3.so.6.3.0\n",
            "\tlibvtkLocalExampleTCL-6.3.so.6.3 -> libvtkLocalExampleTCL-6.3.so.6.3.0\n",
            "\tlibtheoraenc.so.1 -> libtheoraenc.so.1.1.2\n",
            "\tlibopen-rte.so.20 -> libopen-rte.so.20.10.1\n",
            "\tlibdapserver.so.7 -> libdapserver.so.7.6.7\n",
            "\tlibmlx5.so.1 -> libmlx5.so.1.4.17.1\n",
            "\tlibvtkImagingColor-6.3.so.6.3 -> libvtkImagingColor-6.3.so.6.3.0\n",
            "\tlibxcb.so.1 -> libxcb.so.1.1.0\n",
            "\tlibrados.so.2 -> librados.so.2.0.0\n",
            "\tlibvtkIOVideo-6.3.so.6.3 -> libvtkIOVideo-6.3.so.6.3.0\n",
            "\tlibrtmp.so.1 -> librtmp.so.1\n",
            "\tlibflite_cmu_indic_lex.so.1 -> libflite_cmu_indic_lex.so.2.1\n",
            "\tlibgdcmjpeg12.so.2.8 -> libgdcmjpeg12.so.2.8.4\n",
            "\tlibvtkFiltersTextureTCL-6.3.so.6.3 -> libvtkFiltersTextureTCL-6.3.so.6.3.0\n",
            "\tlibXtst.so.6 -> libXtst.so.6.1.0\n",
            "\tlibXxf86vm.so.1 -> libXxf86vm.so.1.0.0\n",
            "\tlibturbojpeg.so.0 -> libturbojpeg.so.0.1.0\n",
            "\tlibxkbcommon-x11.so.0 -> libxkbcommon-x11.so.0.0.0\n",
            "\tlibvtkFiltersGeneralTCL-6.3.so.6.3 -> libvtkFiltersGeneralTCL-6.3.so.6.3.0\n",
            "\tlibxcb-dri3.so.0 -> libxcb-dri3.so.0.0.0\n",
            "\tlibXrandr.so.2 -> libXrandr.so.2.2.0\n",
            "\tlibicuuc.so.60 -> libicuuc.so.60.2\n",
            "\tlibgeotiff.so.2 -> libgeotiff.so.2.1.2\n",
            "\tlibfreexl.so.1 -> libfreexl.so.1.1.0\n",
            "\tlibvtkFiltersParallelFlowPathsPython27D-6.3.so.6.3 -> libvtkFiltersParallelFlowPathsPython27D-6.3.so.6.3.0\n",
            "\tlibzvbi-chains.so.0 -> libzvbi-chains.so.0.0.0\n",
            "\tlibvtkRenderingExternalTCL-6.3.so.6.3 -> libvtkRenderingExternalTCL-6.3.so.6.3.0\n",
            "\tlibvtkCommonExecutionModelTCL-6.3.so.6.3 -> libvtkCommonExecutionModelTCL-6.3.so.6.3.0\n",
            "\tlibmpdec.so.2 -> libmpdec.so.2.4.2\n",
            "\tlibvtkViewsInfovis-6.3.so.6.3 -> libvtkViewsInfovis-6.3.so.6.3.0\n",
            "\tlibvtkViewsInfovisTCL-6.3.so.6.3 -> libvtkViewsInfovisTCL-6.3.so.6.3.0\n",
            "\tlibboost_stacktrace_backtrace.so.1.65.1 -> libboost_stacktrace_backtrace.so.1.65.1\n",
            "\tlibopencv_bioinspired.so.3.2 -> libopencv_bioinspired.so.3.2.0\n",
            "\tlibboost_thread.so.1.65.1 -> libboost_thread.so.1.65.1\n",
            "\tlibvtkChartsCore-6.3.so.6.3 -> libvtkChartsCore-6.3.so.6.3.0\n",
            "\tlibXfont2.so.2 -> libXfont2.so.2.0.0\n",
            "\tlibopencv_ml.so.3.2 -> libopencv_ml.so.3.2.0\n",
            "\tlibvtkIOMPIParallelPython27D-6.3.so.6.3 -> libvtkIOMPIParallelPython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingOpenGLPython27D-6.3.so.6.3 -> libvtkRenderingOpenGLPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersParallelImagingTCL-6.3.so.6.3 -> libvtkFiltersParallelImagingTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersProgrammablePython27D-6.3.so.6.3 -> libvtkFiltersProgrammablePython27D-6.3.so.6.3.0\n",
            "\tlibvtkIONetCDFTCL-6.3.so.6.3 -> libvtkIONetCDFTCL-6.3.so.6.3.0\n",
            "\tlibfftw3.so.3 -> libfftw3.so.3.5.7\n",
            "\tlibmpi_java.so.20 -> libmpi_java.so.20.10.0\n",
            "\tlibvtkFiltersSelectionPython27D-6.3.so.6.3 -> libvtkFiltersSelectionPython27D-6.3.so.6.3.0\n",
            "\tlibdapclient.so.6 -> libdapclient.so.6.1.7\n",
            "\tlibvtkImagingMathPython27D-6.3.so.6.3 -> libvtkImagingMathPython27D-6.3.so.6.3.0\n",
            "\tlibpathplan.so.4 -> libpathplan.so.4.0.0\n",
            "\tlibvtkRenderingContext2D-6.3.so.6.3 -> libvtkRenderingContext2D-6.3.so.6.3.0\n",
            "\tlibgio-2.0.so.0 -> libgio-2.0.so.0.5600.4\n",
            "\tlibgvpr.so.2 -> libgvpr.so.2.0.0\n",
            "\tlibvtkFiltersModeling-6.3.so.6.3 -> libvtkFiltersModeling-6.3.so.6.3.0\n",
            "\tlibvtkImagingMorphological-6.3.so.6.3 -> libvtkImagingMorphological-6.3.so.6.3.0\n",
            "\tlibXdamage.so.1 -> libXdamage.so.1.1.0\n",
            "\tlibrubberband.so.2 -> librubberband.so.2.1.0\n",
            "\tlibvtkFiltersGeneral-6.3.so.6.3 -> libvtkFiltersGeneral-6.3.so.6.3.0\n",
            "\tlibXrender.so.1 -> libXrender.so.1.3.0\n",
            "\tlibvtkIOPostgreSQLPython27D-6.3.so.6.3 -> libvtkIOPostgreSQLPython27D-6.3.so.6.3.0\n",
            "\tlibnvidia-ngx.so.1 -> libnvidia-ngx.so.510.47.03\n",
            "\tlibImath-2_2.so.12 -> libImath.so\n",
            "\tlibflite_cmu_us_slt.so.1 -> libflite_cmu_us_slt.so.2.1\n",
            "\tlibICE.so.6 -> libICE.so.6.3.0\n",
            "\tlibvtkInfovisCore-6.3.so.6.3 -> libvtkInfovisCore-6.3.so.6.3.0\n",
            "\tlibvtkFiltersFlowPathsTCL-6.3.so.6.3 -> libvtkFiltersFlowPathsTCL-6.3.so.6.3.0\n",
            "\tliblept.so.5 -> liblept.so.5.0.2\n",
            "\tlibGL.so.1 -> libGL.so.1.0.0\n",
            "\tlibopencv_ccalib.so.3.2 -> libopencv_ccalib.so.3.2.0\n",
            "\tlibvtkDomainsChemistryPython27D-6.3.so.6.3 -> libvtkDomainsChemistryPython27D-6.3.so.6.3.0\n",
            "\tlibopencv_surface_matching.so.3.2 -> libopencv_surface_matching.so.3.2.0\n",
            "\tlibvtkInfovisCorePython27D-6.3.so.6.3 -> libvtkInfovisCorePython27D-6.3.so.6.3.0\n",
            "\tlibclang-6.0.so.1 -> libclang-6.0.so.1\n",
            "\tlibboost_mpi.so.1.65.1 -> libboost_mpi.so.1.65.1\n",
            "\tlibgeos_c.so.1 -> libgeos_c.so.1.10.2\n",
            "\tlibvtkxdmf2-6.3.so.6.3 -> libvtkxdmf2-6.3.so.6.3.0\n",
            "\tlibvtkImagingStatistics-6.3.so.6.3 -> libvtkImagingStatistics-6.3.so.6.3.0\n",
            "\tlibvtkFiltersHybridPython27D-6.3.so.6.3 -> libvtkFiltersHybridPython27D-6.3.so.6.3.0\n",
            "\tlibvtkVPIC-6.3.so.6.3 -> libvtkVPIC-6.3.so.6.3.0\n",
            "\tlibvtkPythonInterpreter-6.3.so.6.3 -> libvtkPythonInterpreter-6.3.so.6.3.0\n",
            "\tlibpcre32.so.3 -> libpcre32.so.3.13.3\n",
            "\tlibnspr4.so -> libnspr4.so\n",
            "\tlibnl-route-3.so.200 -> libnl-route-3.so.200.24.0\n",
            "\tlibvtkPythonInterpreterTCL-6.3.so.6.3 -> libvtkPythonInterpreterTCL-6.3.so.6.3.0\n",
            "\tlibvtkWrappingPython27Core-6.3.so.6.3 -> libvtkWrappingPython27Core-6.3.so.6.3.0\n",
            "\tlibvtkRenderingVolumeAMR-6.3.so.6.3 -> libvtkRenderingVolumeAMR-6.3.so.6.3.0\n",
            "\tlibgirepository-1.0.so.1 -> libgirepository-1.0.so.1.0.0\n",
            "\tlibvtkFiltersTexture-6.3.so.6.3 -> libvtkFiltersTexture-6.3.so.6.3.0\n",
            "\tlibxcb-glx.so.0 -> libxcb-glx.so.0.0.0\n",
            "\tlibSDL2-2.0.so.0 -> libSDL2-2.0.so.0.8.0\n",
            "\tlibmbedx509.so.0 -> libmbedx509.so.2.16.0\n",
            "\tlibXi.so.6 -> libXi.so.6.1.0\n",
            "\tlibpq.so.5 -> libpq.so.5.10\n",
            "\tliblab_gamut.so.1 -> liblab_gamut.so.1.0.0\n",
            "\tlibpcrecpp.so.0 -> libpcrecpp.so.0.0.1\n",
            "\tlibxcb-shape.so.0 -> libxcb-shape.so.0.0.0\n",
            "\tlibboost_graph.so.1.65.1 -> libboost_graph.so.1.65.1\n",
            "\tlibjson-glib-1.0.so.0 -> libjson-glib-1.0.so.0.400.2\n",
            "\tlibQt5Sql.so.5 -> libQt5Sql.so.5.9.5\n",
            "\tlibmpi_usempif08.so.20 -> libmpi_usempif08.so.20.10.0\n",
            "\tlibvtkFiltersModelingTCL-6.3.so.6.3 -> libvtkFiltersModelingTCL-6.3.so.6.3.0\n",
            "\tlibproxy.so.1 -> libproxy.so.1.0.0\n",
            "\tlibXpm.so.4 -> libXpm.so.4.11.0\n",
            "\tlibvtkInfovisCoreTCL-6.3.so.6.3 -> libvtkInfovisCoreTCL-6.3.so.6.3.0\n",
            "\tlibthai.so.0 -> libthai.so.0.3.0\n",
            "\tlibprotoc.so.10 -> libprotoc.so.10.0.0\n",
            "\tlibtcmalloc_minimal.so.4 -> libtcmalloc_minimal.so.4.3.0\n",
            "\tlibvtkIOMPIImage-6.3.so.6.3 -> libvtkIOMPIImage-6.3.so.6.3.0\n",
            "\tlibvtkInteractionStylePython27D-6.3.so.6.3 -> libvtkInteractionStylePython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOCore-6.3.so.6.3 -> libvtkIOCore-6.3.so.6.3.0\n",
            "\tlibgdcmCommon.so.2.8 -> libgdcmCommon.so.2.8.4\n",
            "\tlibIex-2_2.so.12 -> libIex.so\n",
            "\tlibcups.so.2 -> libcups.so.2\n",
            "\tlibjansson.so.4 -> libjansson.so.4.11.0\n",
            "\tlibgdcmDSED.so.2.8 -> libgdcmDSED.so.2.8.4\n",
            "\tlibvtkCommonMathTCL-6.3.so.6.3 -> libvtkCommonMathTCL-6.3.so.6.3.0\n",
            "\tlibxcb-xfixes.so.0 -> libxcb-xfixes.so.0.0.0\n",
            "\tlibvtkInteractionStyle-6.3.so.6.3 -> libvtkInteractionStyle-6.3.so.6.3.0\n",
            "\tlibcudnn.so.7 -> libcudnn.so.7.6.5\n",
            "\tlibcaca.so.0 -> libcaca.so.0.99.19\n",
            "\tlibgdcmMSFF.so.2.8 -> libgdcmMSFF.so.2.8.4\n",
            "\tlibopencv_line_descriptor.so.3.2 -> libopencv_line_descriptor.so.3.2.0\n",
            "\tlibIlmThread-2_2.so.12 -> libIlmThread.so\n",
            "\tlibvtkIOXML-6.3.so.6.3 -> libvtkIOXML-6.3.so.6.3.0\n",
            "\tlibQt5Gui.so.5 -> libQt5Gui.so.5.9.5\n",
            "\tlibutempter.so.0 -> libutempter.so.1.1.6\n",
            "\tlibvtkIOParallelNetCDFPython27D-6.3.so.6.3 -> libvtkIOParallelNetCDFPython27D-6.3.so.6.3.0\n",
            "\tlibaec.so.0 -> libaec.so.0.0.3\n",
            "\tlibvtkParallelMPIPython27D-6.3.so.6.3 -> libvtkParallelMPIPython27D-6.3.so.6.3.0\n",
            "\tlibnvidia-allocator.so.1 -> libnvidia-allocator.so.510.47.03\n",
            "\tlibvtkFiltersParallelMPITCL-6.3.so.6.3 -> libvtkFiltersParallelMPITCL-6.3.so.6.3.0\n",
            "\tlibvtkImagingMathTCL-6.3.so.6.3 -> libvtkImagingMathTCL-6.3.so.6.3.0\n",
            "\tlibjack.so.0 -> libjack.so.0.0.28\n",
            "\tlibvtkIOMINCPython27D-6.3.so.6.3 -> libvtkIOMINCPython27D-6.3.so.6.3.0\n",
            "\tlibicu-le-hb.so.0 -> libicu-le-hb.so.0.0.0\n",
            "\tlibvtkParallelMPI4PyPython27D-6.3.so.6.3 -> libvtkParallelMPI4PyPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersImagingTCL-6.3.so.6.3 -> libvtkFiltersImagingTCL-6.3.so.6.3.0\n",
            "\tlibxml2.so.2 -> libxml2.so.2.9.4\n",
            "\tlibXext.so.6 -> libXext.so.6.4.0\n",
            "\tlibnvidia-cfg.so.1 -> libnvidia-cfg.so.510.47.03\n",
            "\tlibvtkRenderingOpenGLTCL-6.3.so.6.3 -> libvtkRenderingOpenGLTCL-6.3.so.6.3.0\n",
            "\tlibelf.so.1 -> libelf-0.170.so\n",
            "\tlibvtkFiltersVerdict-6.3.so.6.3 -> libvtkFiltersVerdict-6.3.so.6.3.0\n",
            "\tlibboost_context.so.1.65.1 -> libboost_context.so.1.65.1\n",
            "\tlibvtkRenderingGL2PS-6.3.so.6.3 -> libvtkRenderingGL2PS-6.3.so.6.3.0\n",
            "\tlibvtkFiltersGeometryPython27D-6.3.so.6.3 -> libvtkFiltersGeometryPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersStatisticsTCL-6.3.so.6.3 -> libvtkFiltersStatisticsTCL-6.3.so.6.3.0\n",
            "\tlibvtkInfovisLayout-6.3.so.6.3 -> libvtkInfovisLayout-6.3.so.6.3.0\n",
            "\tlibvtkIOMPIParallel-6.3.so.6.3 -> libvtkIOMPIParallel-6.3.so.6.3.0\n",
            "\tlibopencv_stereo.so.3.2 -> libopencv_stereo.so.3.2.0\n",
            "\tlibvtkRenderingLODTCL-6.3.so.6.3 -> libvtkRenderingLODTCL-6.3.so.6.3.0\n",
            "\tlibvtkInteractionStyleTCL-6.3.so.6.3 -> libvtkInteractionStyleTCL-6.3.so.6.3.0\n",
            "\tlibspeex.so.1 -> libspeex.so.1.5.0\n",
            "\tlibGLESv1_CM_nvidia.so.1 -> libGLESv1_CM_nvidia.so.510.47.03\n",
            "\tlibvtkCommonSystemTCL-6.3.so.6.3 -> libvtkCommonSystemTCL-6.3.so.6.3.0\n",
            "\tlibmca_common_sm.so.20 -> libmca_common_sm.so.20.10.1\n",
            "\tlibvtkFiltersFlowPaths-6.3.so.6.3 -> libvtkFiltersFlowPaths-6.3.so.6.3.0\n",
            "\tlibvtkImagingGeneral-6.3.so.6.3 -> libvtkImagingGeneral-6.3.so.6.3.0\n",
            "\tlibhdf5_serial.so.100 -> libhdf5_serial.so.100.0.1\n",
            "\tlibvtkRenderingAnnotationTCL-6.3.so.6.3 -> libvtkRenderingAnnotationTCL-6.3.so.6.3.0\n",
            "\tlibGLX_mesa.so.0 -> libGLX_mesa.so.0.0.0\n",
            "\tlibvtkInfovisLayoutTCL-6.3.so.6.3 -> libvtkInfovisLayoutTCL-6.3.so.6.3.0\n",
            "\tlibvpx.so.5 -> libvpx.so.5.0.0\n",
            "\tlibgts-0.7.so.5 -> libgts-0.7.so.5.0.1\n",
            "\tlibboost_type_erasure.so.1.65.1 -> libboost_type_erasure.so.1.65.1\n",
            "\tlibkmlengine.so.1 -> libkmlengine.so.1.3.0\n",
            "\tlibvtkIOParallelLSDyna-6.3.so.6.3 -> libvtkIOParallelLSDyna-6.3.so.6.3.0\n",
            "\tlibvtkCommonMath-6.3.so.6.3 -> libvtkCommonMath-6.3.so.6.3.0\n",
            "\tlibmysofa.so.0 -> libmysofa.so.0.5.1\n",
            "\tlibxcb-randr.so.0 -> libxcb-randr.so.0.1.0\n",
            "\tlibopencv_superres.so.3.2 -> libopencv_superres.so.3.2.0\n",
            "\tlibgsm.so.1 -> libgsm.so.1.0.12\n",
            "\tlibvtkRenderingVolumeOpenGL-6.3.so.6.3 -> libvtkRenderingVolumeOpenGL-6.3.so.6.3.0\n",
            "\tlibboost_serialization.so.1.65.1 -> libboost_serialization.so.1.65.1\n",
            "\tlibshine.so.3 -> libshine.so.3.0.1\n",
            "\tlibbluray.so.2 -> libbluray.so.2.0.2\n",
            "\tlibopencv_optflow.so.3.2 -> libopencv_optflow.so.3.2.0\n",
            "\tlibvtkFiltersParallelMPI-6.3.so.6.3 -> libvtkFiltersParallelMPI-6.3.so.6.3.0\n",
            "\tlibcrypto.so.1.0.0 -> libcrypto.so.1.0.0\n",
            "\tlibvtkIOMPIImageTCL-6.3.so.6.3 -> libvtkIOMPIImageTCL-6.3.so.6.3.0\n",
            "\tlibCharLS.so.1 -> libCharLS.so.1.0\n",
            "\tlibvtkRenderingLabel-6.3.so.6.3 -> libvtkRenderingLabel-6.3.so.6.3.0\n",
            "\tlibpcre2-16.so.0 -> libpcre2-16.so.0.7.0\n",
            "\tlibnvidia-tls.so.510.47.03 -> libnvidia-tls.so.510.47.03\n",
            "\tlibboost_signals.so.1.65.1 -> libboost_signals.so.1.65.1\n",
            "\tlibvtkIOPLYPython27D-6.3.so.6.3 -> libvtkIOPLYPython27D-6.3.so.6.3.0\n",
            "\tlibflite_cmu_grapheme_lang.so.1 -> libflite_cmu_grapheme_lang.so.2.1\n",
            "\tlibvtkImagingStencilPython27D-6.3.so.6.3 -> libvtkImagingStencilPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOLegacy-6.3.so.6.3 -> libvtkIOLegacy-6.3.so.6.3.0\n",
            "\tlibboost_fiber.so.1.65.1 -> libboost_fiber.so.1.65.1\n",
            "\tlibgl2ps.so.1.4 -> libgl2ps.so.1.4.0\n",
            "\tlibvtkIOSQLPython27D-6.3.so.6.3 -> libvtkIOSQLPython27D-6.3.so.6.3.0\n",
            "\tlibvtkParallelMPITCL-6.3.so.6.3 -> libvtkParallelMPITCL-6.3.so.6.3.0\n",
            "\tlibxcb-shm.so.0 -> libxcb-shm.so.0.0.0\n",
            "\tlibvtkRenderingAnnotation-6.3.so.6.3 -> libvtkRenderingAnnotation-6.3.so.6.3.0\n",
            "\tlibvtkIOVideoTCL-6.3.so.6.3 -> libvtkIOVideoTCL-6.3.so.6.3.0\n",
            "\tlibopencv_freetype.so.3.2 -> libopencv_freetype.so.3.2.0\n",
            "\tlibkmldom.so.1 -> libkmldom.so.1.3.0\n",
            "\tlibvtkTestingRenderingPython27D-6.3.so.6.3 -> libvtkTestingRenderingPython27D-6.3.so.6.3.0\n",
            "\tlibgdcmjpeg16.so.2.8 -> libgdcmjpeg16.so.2.8.4\n",
            "\tlibk5crypto.so.3 -> libk5crypto.so.3.1\n",
            "\tlibedit.so.2 -> libedit.so.2.0.56\n",
            "\tlibavc1394.so.0 -> libavc1394.so.0.3.0\n",
            "\tlibvtkIOPostgreSQLTCL-6.3.so.6.3 -> libvtkIOPostgreSQLTCL-6.3.so.6.3.0\n",
            "\tlibHalf.so.12 -> libHalf.so.12.0.0\n",
            "\tlibpixman-1.so.0 -> libpixman-1.so.0.34.0\n",
            "\tlibvtkIOODBCPython27D-6.3.so.6.3 -> libvtkIOODBCPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOPostgreSQL-6.3.so.6.3 -> libvtkIOPostgreSQL-6.3.so.6.3.0\n",
            "\tlibXdmcp.so.6 -> libXdmcp.so.6.0.0\n",
            "\tlibvtkCommonExecutionModel-6.3.so.6.3 -> libvtkCommonExecutionModel-6.3.so.6.3.0\n",
            "\tlibopenblas.so.0 -> libopenblasp-r0.2.20.so\n",
            "\tlibboost_log_setup.so.1.65.1 -> libboost_log_setup.so.1.65.1\n",
            "\tlibgme.so.0 -> libgme.so.0.6.2\n",
            "\tlibXv.so.1 -> libXv.so.1.0.0\n",
            "\tlibvtkFiltersParallelStatistics-6.3.so.6.3 -> libvtkFiltersParallelStatistics-6.3.so.6.3.0\n",
            "\tlibmbedcrypto.so.3 -> libmbedcrypto.so.2.16.0\n",
            "\tlibvtkViewsInfovisPython27D-6.3.so.6.3 -> libvtkViewsInfovisPython27D-6.3.so.6.3.0\n",
            "\tlibxkbcommon.so.0 -> libxkbcommon.so.0.0.0\n",
            "\tlibpng16.so.16 -> libpng16.so.16.34.0\n",
            "\tlibvtkIOAMR-6.3.so.6.3 -> libvtkIOAMR-6.3.so.6.3.0\n",
            "\tlibexslt.so.0 -> libexslt.so.0.8.17\n",
            "\tlibvtkImagingCoreTCL-6.3.so.6.3 -> libvtkImagingCoreTCL-6.3.so.6.3.0\n",
            "\tlibdrm.so.2 -> libdrm.so.2.4.0\n",
            "\tlibsensors.so.4 -> libsensors.so.4.4.0\n",
            "\tlibvtkIOLegacyTCL-6.3.so.6.3 -> libvtkIOLegacyTCL-6.3.so.6.3.0\n",
            "\tlibhttp_parser.so.2.7.1 -> libhttp_parser.so.2.7.1\n",
            "\tlibobjc.so.4 -> libobjc.so.4.0.0\n",
            "\tlibhdf5_openmpihl_fortran.so.100 -> libhdf5_openmpihl_fortran.so.100.0.0\n",
            "\tlibboost_coroutine.so.1.65.1 -> libboost_coroutine.so.1.65.1\n",
            "\tlibtesseract.so.4 -> libtesseract.so.4.0.0\n",
            "\tlibavresample.so.3 -> libavresample.so.3.7.0\n",
            "\tlibpolkit-backend-1.so.0 -> libpolkit-backend-1.so.0.0.0\n",
            "\tlibnetcdf_c++.so.4 -> libnetcdf_c++.so.4.2.0\n",
            "\tlibvtkFiltersReebGraph-6.3.so.6.3 -> libvtkFiltersReebGraph-6.3.so.6.3.0\n",
            "\tlibcurl-gnutls.so.4 -> libcurl-gnutls.so.4.5.0\n",
            "\tlibvtkRenderingFreeType-6.3.so.6.3 -> libvtkRenderingFreeType-6.3.so.6.3.0\n",
            "\tlibhdf5_serial_fortran.so.100 -> libhdf5_serial_fortran.so.100.0.1\n",
            "\tlibvtkCommonColor-6.3.so.6.3 -> libvtkCommonColor-6.3.so.6.3.0\n",
            "\tlibcuda.so.1 -> libcuda.so.510.47.03\n",
            "\tlibrest-0.7.so.0 -> librest-0.7.so.0.0.0\n",
            "\tlibvtkmetaio-6.3.so.6.3 -> libvtkmetaio-6.3.so.6.3.0\n",
            "\tlibdrm_amdgpu.so.1 -> libdrm_amdgpu.so.1.0.0\n",
            "\tlibgudev-1.0.so.0 -> libgudev-1.0.so.0.2.0\n",
            "\tlibvtkRenderingCoreTCL-6.3.so.6.3 -> libvtkRenderingCoreTCL-6.3.so.6.3.0\n",
            "\tlibvorbisfile.so.3 -> libvorbisfile.so.3.3.7\n",
            "\tlibvtkRenderingVolumeOpenGLTCL-6.3.so.6.3 -> libvtkRenderingVolumeOpenGLTCL-6.3.so.6.3.0\n",
            "\tlibplc4.so -> libplc4.so\n",
            "\tlibvtkIOGDALTCL-6.3.so.6.3 -> libvtkIOGDALTCL-6.3.so.6.3.0\n",
            "\tlibnorm.so.1 -> libnorm.so.1.0.0\n",
            "\tlibvtkFiltersExtraction-6.3.so.6.3 -> libvtkFiltersExtraction-6.3.so.6.3.0\n",
            "\tlibfygm.so.0 -> libfygm.so.0.0.0\n",
            "\tlibvtkCommonDataModelPython27D-6.3.so.6.3 -> libvtkCommonDataModelPython27D-6.3.so.6.3.0\n",
            "\tlibwayland-server.so.0 -> libwayland-server.so.0.1.0\n",
            "\tlibhdf5_cpp.so.100 -> libhdf5_cpp.so.100.0.0\n",
            "\tlibvtkFiltersGeometry-6.3.so.6.3 -> libvtkFiltersGeometry-6.3.so.6.3.0\n",
            "\tlibboost_mpi_python3-py36.so.1.65.1 -> libboost_mpi_python3.so\n",
            "\tlibvtkGeovisCorePython27D-6.3.so.6.3 -> libvtkGeovisCorePython27D-6.3.so.6.3.0\n",
            "\tlibvtkImagingMorphologicalPython27D-6.3.so.6.3 -> libvtkImagingMorphologicalPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOParallelNetCDFTCL-6.3.so.6.3 -> libvtkIOParallelNetCDFTCL-6.3.so.6.3.0\n",
            "\tlibboost_unit_test_framework.so.1.65.1 -> libboost_unit_test_framework.so.1.65.1\n",
            "\tlibwacom.so.2 -> libwacom.so.2.6.1\n",
            "\tlibvtkFiltersAMRTCL-6.3.so.6.3 -> libvtkFiltersAMRTCL-6.3.so.6.3.0\n",
            "\tlibssl.so.1.1 -> libssl.so.1.1\n",
            "\tlibvtkFiltersCorePython27D-6.3.so.6.3 -> libvtkFiltersCorePython27D-6.3.so.6.3.0\n",
            "\tlibbs2b.so.0 -> libbs2b.so.0.0.0\n",
            "\tlibpython3.7m.so.1.0 -> libpython3.7m.so.1.0\n",
            "\tlibvtkLocalExample-6.3.so.6.3 -> libvtkLocalExample-6.3.so.6.3.0\n",
            "\tlibboost_filesystem.so.1.65.1 -> libboost_filesystem.so.1.65.1\n",
            "\tlibvtkInteractionWidgets-6.3.so.6.3 -> libvtkInteractionWidgets-6.3.so.6.3.0\n",
            "\tlibopencv_viz.so.3.2 -> libopencv_viz.so.3.2.0\n",
            "\tlibmlx4.so.1 -> libmlx4.so.1.0.17.1\n",
            "\tlibvtkRenderingAnnotationPython27D-6.3.so.6.3 -> libvtkRenderingAnnotationPython27D-6.3.so.6.3.0\n",
            "\tlibvtkImagingColorPython27D-6.3.so.6.3 -> libvtkImagingColorPython27D-6.3.so.6.3.0\n",
            "\tlibnvidia-glvkspirv.so.510.47.03 -> libnvidia-glvkspirv.so.510.47.03\n",
            "\tlibvtkIOGeoJSONTCL-6.3.so.6.3 -> libvtkIOGeoJSONTCL-6.3.so.6.3.0\n",
            "\tlibboost_mpi_python-py27.so.1.65.1 -> libboost_mpi_python.so\n",
            "\tlibopen-pal.so.20 -> libopen-pal.so.20.10.1\n",
            "\tlibwayland-egl.so.1 -> libwayland-egl.so.1.0.0\n",
            "\tlibchromaprint.so.1 -> libchromaprint.so.1.4.3\n",
            "\tlibopencv_fuzzy.so.3.2 -> libopencv_fuzzy.so.3.2.0\n",
            "\tlibdrm_nouveau.so.2 -> libdrm_nouveau.so.2.0.0\n",
            "\tlibdrm_radeon.so.1 -> libdrm_radeon.so.1.0.1\n",
            "\tlibvtkImagingCorePython27D-6.3.so.6.3 -> libvtkImagingCorePython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersVerdictPython27D-6.3.so.6.3 -> libvtkFiltersVerdictPython27D-6.3.so.6.3.0\n",
            "\tlibvtkImagingColorTCL-6.3.so.6.3 -> libvtkImagingColorTCL-6.3.so.6.3.0\n",
            "\tlibatlas.so.3 -> libatlas.so.3.10.3\n",
            "\tlibkmlregionator.so.1 -> libkmlregionator.so.1.3.0\n",
            "\tlibcdio.so.17 -> libcdio.so.17.0.0\n",
            "\tlibkmlconvenience.so.1 -> libkmlconvenience.so.1.3.0\n",
            "\tlibvtkRenderingParallelLICPython27D-6.3.so.6.3 -> libvtkRenderingParallelLICPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersCoreTCL-6.3.so.6.3 -> libvtkFiltersCoreTCL-6.3.so.6.3.0\n",
            "\tlibnvoptix.so.1 -> libnvoptix.so.510.47.03\n",
            "\tlibvtkFiltersParallelStatisticsTCL-6.3.so.6.3 -> libvtkFiltersParallelStatisticsTCL-6.3.so.6.3.0\n",
            "\tlibrsvg-2.so.2 -> librsvg-2.so.2.40.20\n",
            "\tlibopencv_objdetect.so.3.2 -> libopencv_objdetect.so.3.2.0\n",
            "\tlibvtkIOXMLParserTCL-6.3.so.6.3 -> libvtkIOXMLParserTCL-6.3.so.6.3.0\n",
            "\tlibvtkCommonExecutionModelPython27D-6.3.so.6.3 -> libvtkCommonExecutionModelPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersSelection-6.3.so.6.3 -> libvtkFiltersSelection-6.3.so.6.3.0\n",
            "\tlibpcre2-32.so.0 -> libpcre2-32.so.0.7.0\n",
            "\tlibvtkIOInfovisTCL-6.3.so.6.3 -> libvtkIOInfovisTCL-6.3.so.6.3.0\n",
            "\tlibgmodule-2.0.so.0 -> libgmodule-2.0.so.0.5600.4\n",
            "\tlibvtkCommonCore-6.3.so.6.3 -> libvtkCommonCore-6.3.so.6.3.0\n",
            "\tlibvtkInteractionImageTCL-6.3.so.6.3 -> libvtkInteractionImageTCL-6.3.so.6.3.0\n",
            "\tlibcaca++.so.0 -> libcaca++.so.0.99.19\n",
            "\tlibpcre2-8.so.0 -> libpcre2-8.so.0.7.0\n",
            "\tlibvtkCommonTransformsTCL-6.3.so.6.3 -> libvtkCommonTransformsTCL-6.3.so.6.3.0\n",
            "\tlibflite_cmu_us_rms.so.1 -> libflite_cmu_us_rms.so.2.1\n",
            "\tlibasyncns.so.0 -> libasyncns.so.0.3.1\n",
            "\tlibboost_atomic.so.1.65.1 -> libboost_atomic.so.1.65.1\n",
            "\tlibxshmfence.so.1 -> libxshmfence.so.1.0.0\n",
            "\tlibvtkRenderingImagePython27D-6.3.so.6.3 -> libvtkRenderingImagePython27D-6.3.so.6.3.0\n",
            "\tlibGLESv1_CM.so.1 -> libGLESv1_CM.so.1.0.0\n",
            "\tlibgdk_pixbuf_xlib-2.0.so.0 -> libgdk_pixbuf_xlib-2.0.so.0.3611.0\n",
            "\tlibvtkImagingFourierPython27D-6.3.so.6.3 -> libvtkImagingFourierPython27D-6.3.so.6.3.0\n",
            "\tlibpython2.7.so.1.0 -> libpython2.7.so.1.0\n",
            "\tlibvtkTestingRenderingTCL-6.3.so.6.3 -> libvtkTestingRenderingTCL-6.3.so.6.3.0\n",
            "\tlibvtkWrappingJava-6.3.so.6.3 -> libvtkWrappingJava-6.3.so.6.3.0\n",
            "\tlibvtkImagingMorphologicalTCL-6.3.so.6.3 -> libvtkImagingMorphologicalTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersGeneric-6.3.so.6.3 -> libvtkFiltersGeneric-6.3.so.6.3.0\n",
            "\tlibvtkFiltersParallelImaging-6.3.so.6.3 -> libvtkFiltersParallelImaging-6.3.so.6.3.0\n",
            "\tlibfabric.so.1 -> libfabric.so.1.9.3\n",
            "\tlibvtkIOEnSightPython27D-6.3.so.6.3 -> libvtkIOEnSightPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersImagingPython27D-6.3.so.6.3 -> libvtkFiltersImagingPython27D-6.3.so.6.3.0\n",
            "\tlibvtkViewsGeovis-6.3.so.6.3 -> libvtkViewsGeovis-6.3.so.6.3.0\n",
            "\tlibglut.so.3 -> libglut.so.3.9.0\n",
            "\tlibvtkGeovisCore-6.3.so.6.3 -> libvtkGeovisCore-6.3.so.6.3.0\n",
            "\tlibboost_log.so.1.65.1 -> libboost_log.so.1.65.1\n",
            "\tlibarchive.so.13 -> libarchive.so.13.2.2\n",
            "\tlibvtkIOExport-6.3.so.6.3 -> libvtkIOExport-6.3.so.6.3.0\n",
            "\tlibvtkIOParallelExodusTCL-6.3.so.6.3 -> libvtkIOParallelExodusTCL-6.3.so.6.3.0\n",
            "\tlibQt5Core.so.5 -> libQt5Core.so.5.9.5\n",
            "\tlibvtkInteractionWidgetsPython27D-6.3.so.6.3 -> libvtkInteractionWidgetsPython27D-6.3.so.6.3.0\n",
            "\tlibboost_numpy-py27.so.1.65.1 -> libboost_numpy.so\n",
            "\tlibvtkFiltersParallelFlowPaths-6.3.so.6.3 -> libvtkFiltersParallelFlowPaths-6.3.so.6.3.0\n",
            "\tlibvtkFiltersParallel-6.3.so.6.3 -> libvtkFiltersParallel-6.3.so.6.3.0\n",
            "\tlibpsl.so.5 -> libpsl.so.5.2.0\n",
            "\tlibvtkRenderingVolumeAMRPython27D-6.3.so.6.3 -> libvtkRenderingVolumeAMRPython27D-6.3.so.6.3.0\n",
            "\tlibvtkParallelCoreTCL-6.3.so.6.3 -> libvtkParallelCoreTCL-6.3.so.6.3.0\n",
            "\tlibXau.so.6 -> libXau.so.6.0.0\n",
            "\tlibpangoft2-1.0.so.0 -> libpangoft2-1.0.so.0.4000.14\n",
            "\tlibvtkIOXdmfIITCL-6.3.so.6.3 -> libvtkIOXdmfIITCL-6.3.so.6.3.0\n",
            "\tlibvtkPythonInterpreterPython27D-6.3.so.6.3 -> libvtkPythonInterpreterPython27D-6.3.so.6.3.0\n",
            "\tlibQt5Widgets.so.5 -> libQt5Widgets.so.5.9.5\n",
            "\tlibmpi_cxx.so.20 -> libmpi_cxx.so.20.10.0\n",
            "\tlibflite_cmu_grapheme_lex.so.1 -> libflite_cmu_grapheme_lex.so.2.1\n",
            "\tlibvtkIOGeometry-6.3.so.6.3 -> libvtkIOGeometry-6.3.so.6.3.0\n",
            "\tlibfyut.so.0 -> libfyut.so.0.0.0\n",
            "\tlibvtkRenderingCorePython27D-6.3.so.6.3 -> libvtkRenderingCorePython27D-6.3.so.6.3.0\n",
            "\tlibcblas.so.3 -> libcblas.so.3.10.3\n",
            "\tlibgbm.so.1 -> libgbm.so.1.0.0\n",
            "\tlibvtkCommonMiscTCL-6.3.so.6.3 -> libvtkCommonMiscTCL-6.3.so.6.3.0\n",
            "\tlibvtkCommonComputationalGeometry-6.3.so.6.3 -> libvtkCommonComputationalGeometry-6.3.so.6.3.0\n",
            "\tlibvtkIOParallelExodusPython27D-6.3.so.6.3 -> libvtkIOParallelExodusPython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingOpenGL-6.3.so.6.3 -> libvtkRenderingOpenGL-6.3.so.6.3.0\n",
            "\tlibvtkRenderingVolumeOpenGLPython27D-6.3.so.6.3 -> libvtkRenderingVolumeOpenGLPython27D-6.3.so.6.3.0\n",
            "\tlibvtkCommonMathPython27D-6.3.so.6.3 -> libvtkCommonMathPython27D-6.3.so.6.3.0\n",
            "\tlibatspi.so.0 -> libatspi.so.0.0.1\n",
            "\tlibtk8.6.so -> libtk8.6.so.0\n",
            "\tlibflite_cmu_us_kal.so.1 -> libflite_cmu_us_kal.so.2.1\n",
            "\tlibvtkIOSQLTCL-6.3.so.6.3 -> libvtkIOSQLTCL-6.3.so.6.3.0\n",
            "\tlibssh-gcrypt_threads.so.4 -> libssh-gcrypt_threads.so.4.5.0\n",
            "\tlibvtkFiltersParallelMPIPython27D-6.3.so.6.3 -> libvtkFiltersParallelMPIPython27D-6.3.so.6.3.0\n",
            "\tlibglib-2.0.so.0 -> libglib-2.0.so.0.5600.4\n",
            "\tlibvtkIOParallelNetCDF-6.3.so.6.3 -> libvtkIOParallelNetCDF-6.3.so.6.3.0\n",
            "\tlibvtkDomainsChemistry-6.3.so.6.3 -> libvtkDomainsChemistry-6.3.so.6.3.0\n",
            "\tlibvtkalglib-6.3.so.6.3 -> libvtkalglib-6.3.so.6.3.0\n",
            "\tlibnghttp2.so.14 -> libnghttp2.so.14.15.2\n",
            "\tlibnvcuvid.so.1 -> libnvcuvid.so.510.47.03\n",
            "\tlibvtkImagingFourier-6.3.so.6.3 -> libvtkImagingFourier-6.3.so.6.3.0\n",
            "\tlibva-drm.so.2 -> libva-drm.so.2.100.0\n",
            "\tlibmpi.so.20 -> libmpi.so.20.10.1\n",
            "\tlibvtkIOExportPython27D-6.3.so.6.3 -> libvtkIOExportPython27D-6.3.so.6.3.0\n",
            "\tlibvtkCommonMiscPython27D-6.3.so.6.3 -> libvtkCommonMiscPython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingParallel-6.3.so.6.3 -> libvtkRenderingParallel-6.3.so.6.3.0\n",
            "\tlibvtkDomainsChemistryTCL-6.3.so.6.3 -> libvtkDomainsChemistryTCL-6.3.so.6.3.0\n",
            "\tlibpgm-5.2.so.0 -> libpgm-5.2.so.0.0.122\n",
            "\tlibvtkIOFFMPEGTCL-6.3.so.6.3 -> libvtkIOFFMPEGTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersPythonPython27D-6.3.so.6.3 -> libvtkFiltersPythonPython27D-6.3.so.6.3.0\n",
            "\tlibip4tc.so.0 -> libip4tc.so.0.1.0\n",
            "\tlibevdev.so.2 -> libevdev.so.2.1.20\n",
            "\tlibzmq.so.5 -> libzmq.so.5.1.5\n",
            "\tlibEGL_mesa.so.0 -> libEGL_mesa.so.0.0.0\n",
            "\tlibpangocairo-1.0.so.0 -> libpangocairo-1.0.so.0.4000.14\n",
            "\tlibvtkIONetCDFPython27D-6.3.so.6.3 -> libvtkIONetCDFPython27D-6.3.so.6.3.0\n",
            "\tlibgit2.so.27 -> libgit2.so.0.27.7\n",
            "\tlibnvidia-compiler.so.510.47.03 -> libnvidia-compiler.so.510.47.03\n",
            "\tlibXcomposite.so.1 -> libXcomposite.so.1.0.0\n",
            "\tlibboost_container.so.1.65.1 -> libboost_container.so.1.65.1\n",
            "\tlibmp3lame.so.0 -> libmp3lame.so.0.0.0\n",
            "\tlibvtkIOExodusTCL-6.3.so.6.3 -> libvtkIOExodusTCL-6.3.so.6.3.0\n",
            "\tlibpipeline.so.1 -> libpipeline.so.1.5.0\n",
            "\tlibhdf5_hl_cpp.so.100 -> libhdf5_hl_cpp.so.100.0.0\n",
            "\tlibFLAC.so.8 -> libFLAC.so.8.3.0\n",
            "\tlibvtkFiltersParallelImagingPython27D-6.3.so.6.3 -> libvtkFiltersParallelImagingPython27D-6.3.so.6.3.0\n",
            "\tlibvorbisenc.so.2 -> libvorbisenc.so.2.0.11\n",
            "\tlibprotobuf.so.10 -> libprotobuf.so.10.0.0\n",
            "\tlibopencv_rgbd.so.3.2 -> libopencv_rgbd.so.3.2.0\n",
            "\tlibxcb-xkb.so.1 -> libxcb-xkb.so.1.0.0\n",
            "\tlibopencv_flann.so.3.2 -> libopencv_flann.so.3.2.0\n",
            "\tlibnssutil3.so -> libnssutil3.so\n",
            "\tlibsoup-2.4.so.1 -> libsoup-2.4.so.1.8.0\n",
            "\tlibcolord.so.2 -> libcolord.so.2.0.5\n",
            "\tlibvtkFiltersModelingPython27D-6.3.so.6.3 -> libvtkFiltersModelingPython27D-6.3.so.6.3.0\n",
            "\tlibvtkImagingStencilTCL-6.3.so.6.3 -> libvtkImagingStencilTCL-6.3.so.6.3.0\n",
            "\tlibopenmpt.so.0 -> libopenmpt.so.0.1.1\n",
            "\tlibvtkFiltersParallelGeometryTCL-6.3.so.6.3 -> libvtkFiltersParallelGeometryTCL-6.3.so.6.3.0\n",
            "\tlibgif.so.7 -> libgif.so.7.0.0\n",
            "\tlibvtkIOAMRTCL-6.3.so.6.3 -> libvtkIOAMRTCL-6.3.so.6.3.0\n",
            "\tlibXaw.so.7 -> libXaw7.so.7.0.0\n",
            "\tlibvtkFiltersSelectionTCL-6.3.so.6.3 -> libvtkFiltersSelectionTCL-6.3.so.6.3.0\n",
            "\tlibXss.so.1 -> libXss.so.1.0.0\n",
            "\tlibvtkIOImagePython27D-6.3.so.6.3 -> libvtkIOImagePython27D-6.3.so.6.3.0\n",
            "\tlibogg.so.0 -> libogg.so.0.8.2\n",
            "\tlibcrystalhd.so.3 -> libcrystalhd.so.3.6\n",
            "\tlibopencv_hdf.so.3.2 -> libopencv_hdf.so.3.2.0\n",
            "\tlibpopt.so.0 -> libpopt.so.0.0.0\n",
            "\tlibvtkGeovisCoreTCL-6.3.so.6.3 -> libvtkGeovisCoreTCL-6.3.so.6.3.0\n",
            "\tlibopencv_stitching.so.3.2 -> libopencv_stitching.so.3.2.0\n",
            "\tlibXfixes.so.3 -> libXfixes.so.3.1.0\n",
            "\tlibavahi-common.so.3 -> libavahi-common.so.3.5.3\n",
            "\tlibavutil.so.55 -> libavutil.so.55.78.100\n",
            "\tlibboost_python-py27.so.1.65.1 -> libboost_python.so\n",
            "\tlibvtkFiltersGenericTCL-6.3.so.6.3 -> libvtkFiltersGenericTCL-6.3.so.6.3.0\n",
            "\tlibopencv_face.so.3.2 -> libopencv_face.so.3.2.0\n",
            "\tlibfribidi.so.0 -> libfribidi.so.0.3.6\n",
            "\tlibtcl8.6.so -> libtcl8.6.so.0\n",
            "\tlibkrb5.so.3 -> libkrb5.so.3.3\n",
            "\tlibvtkIOXMLParser-6.3.so.6.3 -> libvtkIOXMLParser-6.3.so.6.3.0\n",
            "\tlibQt5DBus.so.5 -> libQt5DBus.so.5.9.5\n",
            "\tlibopencv_videostab.so.3.2 -> libopencv_videostab.so.3.2.0\n",
            "\tlibproj.so.12 -> libproj.so.12.0.0\n",
            "\tlibvtkRenderingVolume-6.3.so.6.3 -> libvtkRenderingVolume-6.3.so.6.3.0\n",
            "\tlibvtkFiltersHyperTree-6.3.so.6.3 -> libvtkFiltersHyperTree-6.3.so.6.3.0\n",
            "\tlibapt-inst.so.2.0 -> libapt-inst.so.2.0.0\n",
            "\tlibvtkParallelMPI-6.3.so.6.3 -> libvtkParallelMPI-6.3.so.6.3.0\n",
            "\tlibgphoto2_port.so.12 -> libgphoto2_port.so.12.0.0\n",
            "\tlibboost_python3-py36.so.1.65.1 -> libboost_python3.so\n",
            "\tlibqhull_r.so.7 -> libqhull_r.so.7.2.0\n",
            "\tlibopencv_shape.so.3.2 -> libopencv_shape.so.3.2.0\n",
            "\tlibboost_math_c99.so.1.65.1 -> libboost_math_c99.so.1.65.1\n",
            "\tlibgtk-3.so.0 -> libgtk-3.so.0.2200.30\n",
            "\tlibdconf.so.1 -> libdconf.so.1.0.0\n",
            "\tlibgeos-3.6.2.so -> libgeos-3.6.2.so\n",
            "\tlibpython3.6m.so.1.0 -> libpython3.6m.so.1.0\n",
            "\tlibGLX_nvidia.so.0 -> libGLX_nvidia.so.510.47.03\n",
            "\tlibvtkRenderingVolumePython27D-6.3.so.6.3 -> libvtkRenderingVolumePython27D-6.3.so.6.3.0\n",
            "\tlibvtkImagingFourierTCL-6.3.so.6.3 -> libvtkImagingFourierTCL-6.3.so.6.3.0\n",
            "\tlibva.so.2 -> libva.so.2.100.0\n",
            "\tlibvtkFiltersReebGraphTCL-6.3.so.6.3 -> libvtkFiltersReebGraphTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersHyperTreePython27D-6.3.so.6.3 -> libvtkFiltersHyperTreePython27D-6.3.so.6.3.0\n",
            "\tlibopencv_imgproc.so.3.2 -> libopencv_imgproc.so.3.2.0\n",
            "\tlibvtkTestingRendering-6.3.so.6.3 -> libvtkTestingRendering-6.3.so.6.3.0\n",
            "\tlibtbb.so.2 -> libtbb.so.2\n",
            "\tlibOpenGL.so.0 -> libOpenGL.so.0.0.0\n",
            "\tlibvtkCommonSystem-6.3.so.6.3 -> libvtkCommonSystem-6.3.so.6.3.0\n",
            "\tlibvtkCommonTransformsPython27D-6.3.so.6.3 -> libvtkCommonTransformsPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersParallelStatisticsPython27D-6.3.so.6.3 -> libvtkFiltersParallelStatisticsPython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingLOD-6.3.so.6.3 -> libvtkRenderingLOD-6.3.so.6.3.0\n",
            "\tlibflite_cmu_time_awb.so.1 -> libflite_cmu_time_awb.so.2.1\n",
            "\tlibopencv_structured_light.so.3.2 -> libopencv_structured_light.so.3.2.0\n",
            "\tlibkmlxsd.so.1 -> libkmlxsd.so.1.3.0\n",
            "\tlibvtkFiltersImaging-6.3.so.6.3 -> libvtkFiltersImaging-6.3.so.6.3.0\n",
            "\tlibargon2.so.0 -> libargon2.so.0\n",
            "\tlibhwloc.so.5 -> libhwloc.so.5.7.6\n",
            "\tlibvtkFiltersParallelFlowPathsTCL-6.3.so.6.3 -> libvtkFiltersParallelFlowPathsTCL-6.3.so.6.3.0\n",
            "\tlibvtkViewsGeovisPython27D-6.3.so.6.3 -> libvtkViewsGeovisPython27D-6.3.so.6.3.0\n",
            "\tlibpcsclite.so.1 -> libpcsclite.so.1.0.0\n",
            "\tlibvtkRenderingTkTCL-6.3.so.6.3 -> libvtkRenderingTkTCL-6.3.so.6.3.0\n",
            "\tlibjackserver.so.0 -> libjackserver.so.0.0.28\n",
            "\tlibsodium.so.23 -> libsodium.so.23.1.0\n",
            "\tlibxerces-c-3.2.so -> libxerces-c.so\n",
            "\tlibicuio.so.60 -> libicuio.so.60.2\n",
            "\tlibopus.so.0 -> libopus.so.0.5.2\n",
            "\tlibboost_math_tr1l.so.1.65.1 -> libboost_math_tr1l.so.1.65.1\n",
            "\tlibwavpack.so.1 -> libwavpack.so.1.2.0\n",
            "\tlibexpatw.so.1 -> libexpatw.so.1.6.7\n",
            "\tlibvtkViewsContextIIDTCL-6.3.so.6.3 -> libvtkViewsContextIIDTCL-6.3.so.6.3.0\n",
            "\tlibvtkImagingCore-6.3.so.6.3 -> libvtkImagingCore-6.3.so.6.3.0\n",
            "\tlibflite_cmu_indic_lang.so.1 -> libflite_cmu_indic_lang.so.2.1\n",
            "\tlibasound.so.2 -> libasound.so.2.0.0\n",
            "\tlibvtkIOXdmf2-6.3.so.6.3 -> libvtkIOXdmf2-6.3.so.6.3.0\n",
            "\tlibvtkIOAMRPython27D-6.3.so.6.3 -> libvtkIOAMRPython27D-6.3.so.6.3.0\n",
            "\tlibfontconfig.so.1 -> libfontconfig.so.1.10.1\n",
            "\tlibboost_math_tr1f.so.1.65.1 -> libboost_math_tr1f.so.1.65.1\n",
            "\tlibSM.so.6 -> libSM.so.6.0.1\n",
            "\tlibpoppler.so.73 -> libpoppler.so.73.0.0\n",
            "\tlibvtkFiltersParallelGeometryPython27D-6.3.so.6.3 -> libvtkFiltersParallelGeometryPython27D-6.3.so.6.3.0\n",
            "\tlibvtkInfovisLayoutPython27D-6.3.so.6.3 -> libvtkInfovisLayoutPython27D-6.3.so.6.3.0\n",
            "\tlibspatialite.so.7 -> libspatialite.so.7.1.0\n",
            "\tlibvtkexoIIc-6.3.so.6.3 -> libvtkexoIIc-6.3.so.6.3.0\n",
            "\tlibvtkCommonDataModel-6.3.so.6.3 -> libvtkCommonDataModel-6.3.so.6.3.0\n",
            "\tlibompitrace.so.20 -> libompitrace.so.20.10.0\n",
            "\tlibobjc_gc.so.4 -> libobjc_gc.so.4.0.0\n",
            "\tlibvtkInfovisBoostGraphAlgorithmsTCL-6.3.so.6.3 -> libvtkInfovisBoostGraphAlgorithmsTCL-6.3.so.6.3.0\n",
            "\tlibdrm_intel.so.1 -> libdrm_intel.so.1.0.0\n",
            "\tlibunwind-coredump.so.0 -> libunwind-coredump.so.0.0.0\n",
            "\tlibvtkIOVPIC-6.3.so.6.3 -> libvtkIOVPIC-6.3.so.6.3.0\n",
            "\tlibharfbuzz-gobject.so.0 -> libharfbuzz-gobject.so.0.10702.0\n",
            "\tlibiec61883.so.0 -> libiec61883.so.0.1.1\n",
            "\tlibnvidia-ptxjitcompiler.so.1 -> libnvidia-ptxjitcompiler.so.510.47.03\n",
            "\tlibvtkFiltersStatistics-6.3.so.6.3 -> libvtkFiltersStatistics-6.3.so.6.3.0\n",
            "\tlibvtkIOLSDynaPython27D-6.3.so.6.3 -> libvtkIOLSDynaPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOXMLParserPython27D-6.3.so.6.3 -> libvtkIOXMLParserPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOMovieTCL-6.3.so.6.3 -> libvtkIOMovieTCL-6.3.so.6.3.0\n",
            "\tlibgdcmDICT.so.2.8 -> libgdcmDICT.so.2.8.4\n",
            "\tlibexif.so.12 -> libexif.so.12.3.3\n",
            "\tlibvtkRenderingLICPython27D-6.3.so.6.3 -> libvtkRenderingLICPython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingLICTCL-6.3.so.6.3 -> libvtkRenderingLICTCL-6.3.so.6.3.0\n",
            "\tlibuv.so.1 -> libuv.so.1.0.0\n",
            "\tlibvtkftgl-6.3.so.6.3 -> libvtkftgl-6.3.so.6.3.0\n",
            "\tlibcdio_cdda.so.2 -> libcdio_cdda.so.2.0.0\n",
            "\tlibmysqlclient.so.20 -> libmysqlclient.so.20.3.24\n",
            "\tlibinfinipath.so.4 -> libinfinipath.so.4.0\n",
            "\tlibcurl.so.4 -> libcurl.so.4.5.0\n",
            "\tlibboost_chrono.so.1.65.1 -> libboost_chrono.so.1.65.1\n",
            "\tlibnvidia-encode.so.1 -> libnvidia-encode.so.510.47.03\n",
            "\tlibvtkFiltersCore-6.3.so.6.3 -> libvtkFiltersCore-6.3.so.6.3.0\n",
            "\tlibvtkIOVideoPython27D-6.3.so.6.3 -> libvtkIOVideoPython27D-6.3.so.6.3.0\n",
            "\tlibsz.so.2 -> libsz.so.2.0.1\n",
            "\tlibvtkRenderingContextOpenGL-6.3.so.6.3 -> libvtkRenderingContextOpenGL-6.3.so.6.3.0\n",
            "\tlibvtkRenderingLODPython27D-6.3.so.6.3 -> libvtkRenderingLODPython27D-6.3.so.6.3.0\n",
            "\tlibboost_numpy3-py36.so.1.65.1 -> libboost_numpy3.so\n",
            "\tlibunwind-ptrace.so.0 -> libunwind-ptrace.so.0.0.0\n",
            "\tlibgd.so.3 -> libgd.so.3.0.5\n",
            "\tlibopencv_phase_unwrapping.so.3.2 -> libopencv_phase_unwrapping.so.3.2.0\n",
            "\tlibcolordprivate.so.2 -> libcolordprivate.so.2.0.5\n",
            "\tlibvorbis.so.0 -> libvorbis.so.0.4.8\n",
            "\tlibvtkCommonMisc-6.3.so.6.3 -> libvtkCommonMisc-6.3.so.6.3.0\n",
            "\tlibsndio.so.6.1 -> libsndio.so.6.1\n",
            "\tlibvtkRenderingVolumeTCL-6.3.so.6.3 -> libvtkRenderingVolumeTCL-6.3.so.6.3.0\n",
            "\tlibXcursor.so.1 -> libXcursor.so.1.0.2\n",
            "\tlibIlmImfUtil-2_2.so.22 -> libIlmImfUtil.so\n",
            "\tlibvtkIOImportPython27D-6.3.so.6.3 -> libvtkIOImportPython27D-6.3.so.6.3.0\n",
            "\tlibflite.so.1 -> libflite.so.2.1\n",
            "\tlibfftw3_omp.so.3 -> libfftw3_omp.so.3.5.7\n",
            "\tlibssh2.so.1 -> libssh2.so.1.0.1\n",
            "\tlibvtkRenderingMatplotlib-6.3.so.6.3 -> libvtkRenderingMatplotlib-6.3.so.6.3.0\n",
            "\tlibvtkImagingSourcesTCL-6.3.so.6.3 -> libvtkImagingSourcesTCL-6.3.so.6.3.0\n",
            "\tlibgdcmMEXD.so.2.8 -> libgdcmMEXD.so.2.8.4\n",
            "\tlibQt5Xml.so.5 -> libQt5Xml.so.5.9.5\n",
            "\tlibdap.so.25 -> libdap.so.25.0.1\n",
            "\tlibvtkIOEnSightTCL-6.3.so.6.3 -> libvtkIOEnSightTCL-6.3.so.6.3.0\n",
            "\tlibQt5PrintSupport.so.5 -> libQt5PrintSupport.so.5.9.5\n",
            "\tlibopencv_text.so.3.2 -> libopencv_text.so.3.2.0\n",
            "\tlibharfbuzz-icu.so.0 -> libharfbuzz-icu.so.0.10702.0\n",
            "\tlibvtkImagingSources-6.3.so.6.3 -> libvtkImagingSources-6.3.so.6.3.0\n",
            "\tlibxslt.so.1 -> libxslt.so.1.1.29\n",
            "\tlibavdevice.so.57 -> libavdevice.so.57.10.100\n",
            "\tlibpcre2-posix.so.2 -> libpcre2-posix.so.2.0.0\n",
            "\tlibraw1394.so.11 -> libraw1394.so.11.1.0\n",
            "\tlibvtkIOParallelXMLPython27D-6.3.so.6.3 -> libvtkIOParallelXMLPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOGeometryTCL-6.3.so.6.3 -> libvtkIOGeometryTCL-6.3.so.6.3.0\n",
            "\tlibopencv_xobjdetect.so.3.2 -> libopencv_xobjdetect.so.3.2.0\n",
            "\tlibpcre16.so.3 -> libpcre16.so.3.13.3\n",
            "\tlibopencv_reg.so.3.2 -> libopencv_reg.so.3.2.0\n",
            "\tlibswresample.so.2 -> libswresample.so.2.9.100\n",
            "\tlibvtkFiltersSources-6.3.so.6.3 -> libvtkFiltersSources-6.3.so.6.3.0\n",
            "\tlibunwind.so.8 -> libunwind.so.8.0.1\n",
            "\tlibxcb-dri2.so.0 -> libxcb-dri2.so.0.0.0\n",
            "\tlibvtkViewsCorePython27D-6.3.so.6.3 -> libvtkViewsCorePython27D-6.3.so.6.3.0\n",
            "\tlibgdcmjpeg8.so.2.8 -> libgdcmjpeg8.so.2.8.4\n",
            "\tlibatk-bridge-2.0.so.0 -> libatk-bridge-2.0.so.0.0.0\n",
            "\tlibavcodec.so.57 -> libavcodec.so.57.107.100\n",
            "\tlibxcb-icccm.so.4 -> libxcb-icccm.so.4.0.0\n",
            "\tlibpaper.so.1 -> libpaper.so.1.1.2\n",
            "\tlibxkbfile.so.1 -> libxkbfile.so.1.0.2\n",
            "\tlibrhash.so.0 -> librhash.so.0\n",
            "\tlibflite_cmulex.so.1 -> libflite_cmulex.so.2.1\n",
            "\tlibvtkFiltersParallelPython27D-6.3.so.6.3 -> libvtkFiltersParallelPython27D-6.3.so.6.3.0\n",
            "\tlibQt5Network.so.5 -> libQt5Network.so.5.9.5\n",
            "\tlibvtkFiltersHybrid-6.3.so.6.3 -> libvtkFiltersHybrid-6.3.so.6.3.0\n",
            "\tlibodbc.so.2 -> libodbc.so.2.0.0\n",
            "\tlibvtkIOGDAL-6.3.so.6.3 -> libvtkIOGDAL-6.3.so.6.3.0\n",
            "\tlibtbbmalloc.so.2 -> libtbbmalloc.so.2\n",
            "\tlibglapi.so.0 -> libglapi.so.0.0.0\n",
            "\tlibvtkImagingStatisticsPython27D-6.3.so.6.3 -> libvtkImagingStatisticsPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersStatisticsPython27D-6.3.so.6.3 -> libvtkFiltersStatisticsPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersParallelTCL-6.3.so.6.3 -> libvtkFiltersParallelTCL-6.3.so.6.3.0\n",
            "\tlibopencv_aruco.so.3.2 -> libopencv_aruco.so.3.2.0\n",
            "\tlibxcb-render.so.0 -> libxcb-render.so.0.0.0\n",
            "\tlibGLESv2.so.2 -> libGLESv2.so.2.0.0\n",
            "\tlibmca_common_libfabric.so.20 -> libmca_common_libfabric.so.20.10.0\n",
            "\tlibvtkIOMPIImagePython27D-6.3.so.6.3 -> libvtkIOMPIImagePython27D-6.3.so.6.3.0\n",
            "\tlibicutest.so.60 -> libicutest.so.60.2\n",
            "\tlibvdpau.so.1 -> libvdpau.so.1.0.0\n",
            "\tlibepoxy.so.0 -> libepoxy.so.0.0.0\n",
            "\tlibIlmImf-2_2.so.22 -> libIlmImf.so\n",
            "\tlibvtkRenderingLIC-6.3.so.6.3 -> libvtkRenderingLIC-6.3.so.6.3.0\n",
            "\tlibinput.so.10 -> libinput.so.10.13.0\n",
            "\tlibvtkIOExodusPython27D-6.3.so.6.3 -> libvtkIOExodusPython27D-6.3.so.6.3.0\n",
            "\tlibvtkIOParallelXMLTCL-6.3.so.6.3 -> libvtkIOParallelXMLTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersProgrammable-6.3.so.6.3 -> libvtkFiltersProgrammable-6.3.so.6.3.0\n",
            "\tlibvtkIOLSDyna-6.3.so.6.3 -> libvtkIOLSDyna-6.3.so.6.3.0\n",
            "\tlibgssapi_krb5.so.2 -> libgssapi_krb5.so.2.2\n",
            "\tlibva-x11.so.2 -> libva-x11.so.2.100.0\n",
            "\tlibvtkIOParallelPython27D-6.3.so.6.3 -> libvtkIOParallelPython27D-6.3.so.6.3.0\n",
            "\tlibIexMath-2_2.so.12 -> libIexMath.so\n",
            "\tlibvtkIOLegacyPython27D-6.3.so.6.3 -> libvtkIOLegacyPython27D-6.3.so.6.3.0\n",
            "\tlibBlocksRuntime.so.0 -> libBlocksRuntime.so.0.0.0\n",
            "\tlibLLVM-10.so.1 -> libLLVM-10.so.1\n",
            "\tlibvtkIOMPIParallelTCL-6.3.so.6.3 -> libvtkIOMPIParallelTCL-6.3.so.6.3.0\n",
            "\tlibvtkInteractionImagePython27D-6.3.so.6.3 -> libvtkInteractionImagePython27D-6.3.so.6.3.0\n",
            "\tlibssl.so.1.0.0 -> libssl.so.1.0.0\n",
            "\tlibcdio_paranoia.so.2 -> libcdio_paranoia.so.2.0.0\n",
            "\tlibhdf5_openmpi_hl.so.100 -> libhdf5_openmpi_hl.so.100.0.0\n",
            "\tlibgraphite2.so.3 -> libgraphite2.so.3.0.1\n",
            "\tlibprofiler.so.0 -> libprofiler.so.0.4.8\n",
            "\tlibvtkIOParallel-6.3.so.6.3 -> libvtkIOParallel-6.3.so.6.3.0\n",
            "\tlibvtkViewsContext2D-6.3.so.6.3 -> libvtkViewsContext2D-6.3.so.6.3.0\n",
            "\tlibcgraph.so.6 -> libcgraph.so.6.0.0\n",
            "\tlibnvidia-opticalflow.so.1 -> libnvidia-opticalflow.so.510.47.03\n",
            "\tlibswscale.so.4 -> libswscale.so.4.8.100\n",
            "\tlibodbcinst.so.2 -> libodbcinst.so.2.0.0\n",
            "\tlibodbccr.so.2 -> libodbccr.so.2.0.0\n",
            "\tlibvtkRenderingFreeTypeFontConfig-6.3.so.6.3 -> libvtkRenderingFreeTypeFontConfig-6.3.so.6.3.0\n",
            "\tlibvtkCommonCoreTCL-6.3.so.6.3 -> libvtkCommonCoreTCL-6.3.so.6.3.0\n",
            "\tlibvtkIOGeoJSON-6.3.so.6.3 -> libvtkIOGeoJSON-6.3.so.6.3.0\n",
            "\tlibvtkImagingGeneralTCL-6.3.so.6.3 -> libvtkImagingGeneralTCL-6.3.so.6.3.0\n",
            "\tlibsamplerate.so.0 -> libsamplerate.so.0.1.8\n",
            "\tlibvtkIOMySQL-6.3.so.6.3 -> libvtkIOMySQL-6.3.so.6.3.0\n",
            "\tlibvtkRenderingContextOpenGLPython27D-6.3.so.6.3 -> libvtkRenderingContextOpenGLPython27D-6.3.so.6.3.0\n",
            "\tlibflite_cmu_us_awb.so.1 -> libflite_cmu_us_awb.so.2.1\n",
            "\tlibvtkInfovisBoostGraphAlgorithmsPython27D-6.3.so.6.3 -> libvtkInfovisBoostGraphAlgorithmsPython27D-6.3.so.6.3.0\n",
            "\tlibzvbi.so.0 -> libzvbi.so.0.13.2\n",
            "\tlibnvidia-eglcore.so.510.47.03 -> libnvidia-eglcore.so.510.47.03\n",
            "\tlibrbd.so.1 -> librbd.so.1.12.0\n",
            "\tlibvtkFiltersProgrammableTCL-6.3.so.6.3 -> libvtkFiltersProgrammableTCL-6.3.so.6.3.0\n",
            "\tlibvtkRenderingImage-6.3.so.6.3 -> libvtkRenderingImage-6.3.so.6.3.0\n",
            "\tlibtheora.so.0 -> libtheora.so.0.3.10\n",
            "\tlibxcb-render-util.so.0 -> libxcb-render-util.so.0.0.0\n",
            "\tlibtiffxx.so.5 -> libtiffxx.so.5.3.0\n",
            "\tlibvtkInfovisBoostGraphAlgorithms-6.3.so.6.3 -> libvtkInfovisBoostGraphAlgorithms-6.3.so.6.3.0\n",
            "\tlibvtkCommonDataModelTCL-6.3.so.6.3 -> libvtkCommonDataModelTCL-6.3.so.6.3.0\n",
            "\tlibGLESv2_nvidia.so.2 -> libGLESv2_nvidia.so.510.47.03\n",
            "\tlibvtkCommonColorTCL-6.3.so.6.3 -> libvtkCommonColorTCL-6.3.so.6.3.0\n",
            "\tlibXinerama.so.1 -> libXinerama.so.1.0.0\n",
            "\tlibopencv_video.so.3.2 -> libopencv_video.so.3.2.0\n",
            "\tlibvtkViewsContext2DPython27D-6.3.so.6.3 -> libvtkViewsContext2DPython27D-6.3.so.6.3.0\n",
            "\tlibtheoradec.so.1 -> libtheoradec.so.1.1.4\n",
            "\tlibvtkFiltersExtractionPython27D-6.3.so.6.3 -> libvtkFiltersExtractionPython27D-6.3.so.6.3.0\n",
            "\tlibvtkFiltersTexturePython27D-6.3.so.6.3 -> libvtkFiltersTexturePython27D-6.3.so.6.3.0\n",
            "\tlibvtkInteractionImage-6.3.so.6.3 -> libvtkInteractionImage-6.3.so.6.3.0\n",
            "\tlibxcb-util.so.1 -> libxcb-util.so.1.0.0\n",
            "\tlibvtkCommonCorePython27D-6.3.so.6.3 -> libvtkCommonCorePython27D-6.3.so.6.3.0\n",
            "\tlibvtkRenderingFreeTypeTCL-6.3.so.6.3 -> libvtkRenderingFreeTypeTCL-6.3.so.6.3.0\n",
            "\tlibvtkRenderingExternalPython27D-6.3.so.6.3 -> libvtkRenderingExternalPython27D-6.3.so.6.3.0\n",
            "\tlibOpenCL.so.1 -> libOpenCL.so.1.0.0\n",
            "\tlibopencv_ximgproc.so.3.2 -> libopencv_ximgproc.so.3.2.0\n",
            "\tlibibverbs.so.1 -> libibverbs.so.1.1.17.1\n",
            "\tlibavfilter.so.6 -> libavfilter.so.6.107.100\n",
            "\tlibjsoncpp.so.1 -> libjsoncpp.so.1.7.4\n",
            "\tlibkmlbase.so.1 -> libkmlbase.so.1.3.0\n",
            "\tlibvtkIOExportTCL-6.3.so.6.3 -> libvtkIOExportTCL-6.3.so.6.3.0\n",
            "\tlibpulse-simple.so.0 -> libpulse-simple.so.0.1.1\n",
            "\tlibvtkRenderingCore-6.3.so.6.3 -> libvtkRenderingCore-6.3.so.6.3.0\n",
            "\tlibcairo-gobject.so.2 -> libcairo-gobject.so.2.11510.0\n",
            "\tlibtwolame.so.0 -> libtwolame.so.0.0.0\n",
            "\tlibcublasLt.so.10 -> libcublasLt.so.10.2.1.243\n",
            "\tlibvtkFiltersReebGraphPython27D-6.3.so.6.3 -> libvtkFiltersReebGraphPython27D-6.3.so.6.3.0\n",
            "\tlibcublas.so.10 -> libcublas.so.10.2.1.243\n",
            "\tlibopencv_plot.so.3.2 -> libopencv_plot.so.3.2.0\n",
            "\tlibvtkIOODBCTCL-6.3.so.6.3 -> libvtkIOODBCTCL-6.3.so.6.3.0\n",
            "\tlibvtkFiltersSMP-6.3.so.6.3 -> libvtkFiltersSMP-6.3.so.6.3.0\n",
            "\tlibmpi_mpifh.so.20 -> libmpi_mpifh.so.20.11.0\n",
            "\tlibvtkImagingMath-6.3.so.6.3 -> libvtkImagingMath-6.3.so.6.3.0\n",
            "\tlibtcmalloc_and_profiler.so.4 -> libtcmalloc_and_profiler.so.4.3.0\n",
            "\tlibopencv_datasets.so.3.2 -> libopencv_datasets.so.3.2.0\n",
            "\tlibfontenc.so.1 -> libfontenc.so.1.0.0\n",
            "\tlibQt5XcbQpa.so.5 -> libQt5XcbQpa.so.5.9.5\n",
            "\tlibvtkRenderingContext2DPython27D-6.3.so.6.3 -> libvtkRenderingContext2DPython27D-6.3.so.6.3.0\n",
            "\tlibQt5Test.so.5 -> libQt5Test.so.5.9.5\n",
            "\tlibvtkRenderingVolumeAMRTCL-6.3.so.6.3 -> libvtkRenderingVolumeAMRTCL-6.3.so.6.3.0\n",
            "\tlibopencv_bgsegm.so.3.2 -> libopencv_bgsegm.so.3.2.0\n",
            "\tlibiculx.so.60 -> libiculx.so.60.2\n",
            "\tlibvtkRenderingParallelPython27D-6.3.so.6.3 -> libvtkRenderingParallelPython27D-6.3.so.6.3.0\n",
            "\tliblua5.1.so.0 -> liblua5.1.so.0.0.0\n",
            "\tliblua5.1-c++.so.0 -> liblua5.1-c++.so.0.0.0\n",
            "\tlibluajit-5.1.so.2 -> libluajit-5.1.so.2.1.0\n",
            "\tlibyaml-0.so.2 -> libyaml-0.so.2.0.5\n",
            "\tlibunwind-x86_64.so.8 -> libunwind-x86_64.so.8.0.1\n",
            "\tlibmecab.so.2 -> libmecab.so.2.0.0\n",
            "\tlibp11-kit.so.0 -> libp11-kit.so.0.3.0\n",
            "\tlibsemanage.so.1 -> libsemanage.so.1\n",
            "\tlibmenuw.so.5 -> libmenuw.so.5.9\n",
            "\tlibnettle.so.6 -> libnettle.so.6.4\n",
            "\tlibapt-pkg.so.5.0 -> libapt-pkg.so.5.0.2\n",
            "\tlibpanelw.so.5 -> libpanelw.so.5.9\n",
            "\tlibdb-5.3.so -> libdb-5.3.so\n",
            "\tlibpcreposix.so.3 -> libpcreposix.so.3.13.3\n",
            "\tlibidn2.so.0 -> libidn2.so.0.3.3\n",
            "\tlibffi.so.6 -> libffi.so.6.0.4\n",
            "\tlibdebconfclient.so.0 -> libdebconfclient.so.0.0.0\n",
            "\tlibgnutls.so.30 -> libgnutls.so.30.14.10\n",
            "\tlibzstd.so.1 -> libzstd.so.1.3.3\n",
            "\tlibpanel.so.5 -> libpanel.so.5.9\n",
            "\tlibgmp.so.10 -> libgmp.so.10.3.2\n",
            "\tliblz4.so.1 -> liblz4.so.1.7.1\n",
            "\tlibtasn1.so.6 -> libtasn1.so.6.5.5\n",
            "\tlibform.so.5 -> libform.so.5.9\n",
            "\tlibunistring.so.2 -> libunistring.so.2.1.0\n",
            "\tlibmenu.so.5 -> libmenu.so.5.9\n",
            "\tlibformw.so.5 -> libformw.so.5.9\n",
            "\tlibhogweed.so.4 -> libhogweed.so.4.4\n",
            "\tlibapt-private.so.0.0 -> libapt-private.so.0.0.0\n",
            "\tlibtic.so.5 -> libtic.so.5.9\n",
            "\tlibstdc++.so.6 -> libstdc++.so.6.0.25\n",
            "/lib32:\n",
            "\tlibpcprofile.so -> libpcprofile.so\n",
            "\tlibmemusage.so -> libmemusage.so\n",
            "\tlibcrypt.so.1 -> libcrypt-2.27.so\n",
            "\tlibnsl.so.1 -> libnsl-2.27.so\n",
            "\tlibutil.so.1 -> libutil-2.27.so\n",
            "/sbin/ldconfig.real: /lib32/ld-2.27.so is the dynamic linker, ignoring\n",
            "\n",
            "\tld-linux.so.2 -> ld-2.27.so\n",
            "\tlibm.so.6 -> libm-2.27.so\n",
            "\tlibnss_dns.so.2 -> libnss_dns-2.27.so\n",
            "\tlibdl.so.2 -> libdl-2.27.so\n",
            "\tlibnss_compat.so.2 -> libnss_compat-2.27.so\n",
            "\tlibc.so.6 -> libc-2.27.so\n",
            "\tlibresolv.so.2 -> libresolv-2.27.so\n",
            "\tlibBrokenLocale.so.1 -> libBrokenLocale-2.27.so\n",
            "\tlibpthread.so.0 -> libpthread-2.27.so\n",
            "\tlibthread_db.so.1 -> libthread_db-1.0.so\n",
            "\tlibnss_hesiod.so.2 -> libnss_hesiod-2.27.so\n",
            "\tlibnss_files.so.2 -> libnss_files-2.27.so\n",
            "\tlibcidn.so.1 -> libcidn-2.27.so\n",
            "\tlibrt.so.1 -> librt-2.27.so\n",
            "\tlibnss_nis.so.2 -> libnss_nis-2.27.so\n",
            "\tlibnss_nisplus.so.2 -> libnss_nisplus-2.27.so\n",
            "\tlibanl.so.1 -> libanl-2.27.so\n",
            "\tlibSegFault.so -> libSegFault.so\n",
            "/usr/lib32:\n",
            "\tlibgcc_s.so.1 -> libgcc_s.so.1\n",
            "\tlibstdc++.so.6 -> libstdc++.so.6.0.25\n",
            "/lib:\n",
            "/usr/lib:\n",
            "\tlibBLT.2.5.so.8.6 -> libBLT.2.5.so.8.6\n",
            "\tlibogdi.so.3.2 -> libogdi.so.3.2\n",
            "\tlibvpf.so.3.2 -> libvpf.so.3.2\n",
            "\tlibgdal.so.20 -> libgdal.so.20.3.2\n",
            "\tlibmfhdfalt.so.0 -> libmfhdfalt.so.0.0.0\n",
            "\tlibdfalt.so.0 -> libdfalt.so.0.0.0\n",
            "\tlibann.so.0 -> libann.so.0.0.0\n",
            "\tlibnvidia-gtk2.so.510.47.03 -> libnvidia-gtk2.so.510.47.03\n",
            "\tlibarmadillo.so.8 -> libarmadillo.so.8.400.0\n",
            "\tlibBLTlite.2.5.so.8.6 -> libBLTlite.2.5.so.8.6\n",
            "\tlibnvidia-gtk3.so.510.47.03 -> libnvidia-gtk3.so.510.47.03\n",
            "/content\n",
            "Cloning into 'vcpkg'...\n",
            "remote: Enumerating objects: 143670, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 143670 (delta 81), reused 73 (delta 73), pack-reused 143576\u001b[K\n",
            "Receiving objects: 100% (143670/143670), 52.90 MiB | 13.06 MiB/s, done.\n",
            "Resolving deltas: 100% (89747/89747), done.\n",
            "/content/vcpkg\n",
            "Downloading vcpkg-glibc...\n",
            "Telemetry\n",
            "---------\n",
            "vcpkg collects usage data in order to help us improve your experience.\n",
            "The data collected by Microsoft is anonymous.\n",
            "You can opt-out of telemetry by re-running the bootstrap-vcpkg script with -disableMetrics,\n",
            "passing --disable-metrics to vcpkg on the command line,\n",
            "or by setting the VCPKG_DISABLE_METRICS environment variable.\n",
            "\n",
            "Read more about vcpkg telemetry at docs/about/privacy.md\n",
            "\u001b[92mApplied user-wide integration for this vcpkg root.\n",
            "\u001b[0m\n",
            "CMake projects should use: \"-DCMAKE_TOOLCHAIN_FILE=/content/vcpkg/scripts/buildsystems/vcpkg.cmake\"\n",
            "Computing installation plan...\n",
            "A suitable version of cmake was not found (required v3.22.2). Downloading portable cmake v3.22.2...\n",
            "Downloading cmake...\n",
            "  https://github.com/Kitware/CMake/releases/download/v3.22.2/cmake-3.22.2-linux-x86_64.tar.gz -> /content/vcpkg/downloads/cmake-3.22.2linux-x86_64.tar.gz\n",
            "Extracting cmake...\n",
            "The following packages will be built and installed:\n",
            "    sentencepiece[core]:x64-linux -> 0.1.96\n",
            "  * vcpkg-cmake[core]:x64-linux -> 2022-01-19\n",
            "Additional packages (*) will be modified to complete this operation.\n",
            "Detecting compiler hash for triplet x64-linux...\n",
            "A suitable version of ninja was not found (required v1.10.2). Downloading portable ninja v1.10.2...\n",
            "Downloading ninja...\n",
            "  https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-linux.zip -> /content/vcpkg/downloads/ninja-linux-1.10.2.zip\n",
            "Extracting ninja...\n",
            "Restored 0 packages from /root/.cache/vcpkg/archives in 85.15 us. Use --debug to see more details.\n",
            "Starting package 1/2: vcpkg-cmake:x64-linux\n",
            "Building package vcpkg-cmake[core]:x64-linux...\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/vcpkg_cmake_configure.cmake\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/vcpkg_cmake_build.cmake\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/vcpkg_cmake_install.cmake\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/vcpkg_cmake_get_vars.cmake\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/cmake_get_vars\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/cmake_get_vars/CMakeLists.txt\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/vcpkg-port-config.cmake\n",
            "-- Installing: /content/vcpkg/packages/vcpkg-cmake_x64-linux/share/vcpkg-cmake/copyright\n",
            "-- Performing post-build validation\n",
            "-- Performing post-build validation done\n",
            "Stored binary cache: /root/.cache/vcpkg/archives/76/76edc0a9e476c40c2e10a57e43828c063ed8c79660659ba3be4aa1160e2da6ec.zip\n",
            "Installing package vcpkg-cmake[core]:x64-linux...\n",
            "Elapsed time for package vcpkg-cmake:x64-linux: 30.36 ms\n",
            "Starting package 2/2: sentencepiece:x64-linux\n",
            "Building package sentencepiece[core]:x64-linux...\n",
            "-- Downloading https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz -> google-sentencepiece-v0.1.96.tar.gz...\n",
            "-- Extracting source /content/vcpkg/downloads/google-sentencepiece-v0.1.96.tar.gz\n",
            "-- Using source at /content/vcpkg/buildtrees/sentencepiece/src/v0.1.96-bd63ffbdd1.clean\n",
            "-- Configuring x64-linux-dbg\n",
            "-- Configuring x64-linux-rel\n",
            "-- Building x64-linux-dbg\n",
            "-- Building x64-linux-rel\n",
            "-- Fixing pkgconfig file: /content/vcpkg/packages/sentencepiece_x64-linux/lib/pkgconfig/sentencepiece.pc\n",
            "-- Fixing pkgconfig file: /content/vcpkg/packages/sentencepiece_x64-linux/debug/lib/pkgconfig/sentencepiece.pc\n",
            "-- Performing post-build validation\n",
            "-- Performing post-build validation done\n",
            "Stored binary cache: /root/.cache/vcpkg/archives/a0/a090d209fd33970cf20c70ad954692514c65d322667204205741b18534397bd0.zip\n",
            "Installing package sentencepiece[core]:x64-linux...\n",
            "Elapsed time for package sentencepiece:x64-linux: 2.435 min\n",
            "\n",
            "Total elapsed time: 2.507 min\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ginzaで形態素解析\n",
        "! mkdir -p /content/ginza-data\n",
        "! cat kftt-data-1.0/data/orig/kyoto-train.ja | sed 's/\\s+/ /g' | ginzame > /content/ginza-data/train.ginza.ja\n",
        "! cat kftt-data-1.0/data/orig/kyoto-dev.ja | sed 's/\\s+/ /g' | ginzame > /content/ginza-data/dev.ginza.ja\n",
        "! cat kftt-data-1.0/data/orig/kyoto-test.ja | sed 's/\\s+/ /g' | ginzame > /content/ginza-data/test.ginza.ja"
      ],
      "metadata": {
        "id": "bX1jkFPdM9UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head -n 10 /content/ginza-data/test.ginza.ja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP8A1ilqUdDo",
        "outputId": "5eb8e3ee-d96e-45c0-f685-1b2ca908cf83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InfoboxBuddhist\t名詞,普通名詞,一般,*,*,*,infoboxbuddhist,infoboxbuddhist,*\n",
            "EOS\n",
            "\n",
            "道元（どうげん）\t名詞,固有名詞,人名,一般,*,*,道元,ドウゲン,*\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,*\n",
            "、\t補助記号,読点,*,*,*,*,、,、,*\n",
            "鎌倉\t名詞,固有名詞,地名,一般,*,*,鎌倉,カマクラ,*\n",
            "時代\t名詞,普通名詞,一般,*,*,*,時代,ジダイ,*\n",
            "初期\t名詞,普通名詞,一般,*,*,*,初期,ショキ,*\n",
            "の\t助詞,格助詞,*,*,*,*,の,ノ,*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lagsE4b7-o1S"
      },
      "outputs": [],
      "source": [
        "# ===========\n",
        "# 90. データの準備\n",
        "#============\n",
        "# 日本語のトークン化\n",
        "import spacy\n",
        "! mkdir -p /content/data\n",
        "for input, output in [\n",
        "    ('/content/ginza-data/train.ginza.ja', '/content/data/train.ja'),\n",
        "    ('/content/ginza-data/dev.ginza.ja', '/content/data/dev.ja'),\n",
        "    ('/content/ginza-data/test.ginza.ja', '/content/data/test.ja'),\n",
        "]:\n",
        "    with open(input,'r') as f:\n",
        "        contents = []\n",
        "        temp = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == 'EOS':\n",
        "                contents.append(' '.join(temp))\n",
        "                temp = []\n",
        "            elif line != '':\n",
        "                temp.append(line.split('\\t')[0])\n",
        "    with open(output, 'w') as f:\n",
        "        for content in contents:\n",
        "            print(content, file=f)\n",
        "\n",
        "# 英語のトークン化\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "for input, output in [\n",
        "    ('/content/kftt-data-1.0/data/orig/kyoto-train.en', '/content/data/train.en'),\n",
        "    ('/content/kftt-data-1.0/data/orig/kyoto-dev.en', '/content/data/dev.en'),\n",
        "    ('/content/kftt-data-1.0/data/orig/kyoto-test.en', '/content/data/test.en'),\n",
        "]:\n",
        "    with open(input) as f, open(output, 'w') as g:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            line = re.sub(r'\\s+', ' ', line)\n",
        "            line = nlp.make_doc(line)\n",
        "            line = ' '.join([doc.text for doc in line])\n",
        "            print(line, file=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrxNyOdv4GmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2d82a4-7d21-40f9-d004-66f38f78dae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data-bin': No such file or directory\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-27 12:41:00 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=30000, nwordstgt=30000, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='ja', task='translation', tensorboard_logdir=None, testpref='/content/data/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/data/train', use_plasma_view=False, user_dir=None, validpref='/content/data/dev', wandb_project=None, workers=1)\n",
            "2022-02-27 12:42:14 | INFO | fairseq_cli.preprocess | [en] Dictionary: 30000 types\n",
            "2022-02-27 12:43:18 | INFO | fairseq_cli.preprocess | [en] /content/data/train.en: 440288 sents, 12321279 tokens, 5.97% replaced (by <unk>)\n",
            "2022-02-27 12:43:18 | INFO | fairseq_cli.preprocess | [en] Dictionary: 30000 types\n",
            "2022-02-27 12:43:18 | INFO | fairseq_cli.preprocess | [en] /content/data/dev.en: 1166 sents, 26101 tokens, 6.74% replaced (by <unk>)\n",
            "2022-02-27 12:43:18 | INFO | fairseq_cli.preprocess | [en] Dictionary: 30000 types\n",
            "2022-02-27 12:43:18 | INFO | fairseq_cli.preprocess | [en] /content/data/test.en: 1160 sents, 28796 tokens, 6.12% replaced (by <unk>)\n",
            "2022-02-27 12:43:18 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 30000 types\n",
            "2022-02-27 12:44:45 | INFO | fairseq_cli.preprocess | [ja] /content/data/train.ja: 440288 sents, 11550262 tokens, 5.91% replaced (by <unk>)\n",
            "2022-02-27 12:44:45 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 30000 types\n",
            "2022-02-27 12:44:45 | INFO | fairseq_cli.preprocess | [ja] /content/data/dev.ja: 1166 sents, 26468 tokens, 5.31% replaced (by <unk>)\n",
            "2022-02-27 12:44:45 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 30000 types\n",
            "2022-02-27 12:44:46 | INFO | fairseq_cli.preprocess | [ja] /content/data/test.ja: 1160 sents, 28111 tokens, 5.12% replaced (by <unk>)\n",
            "2022-02-27 12:44:46 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
          ]
        }
      ],
      "source": [
        "# ===========\n",
        "# 91. 機械翻訳モデルの訓練\n",
        "#============\n",
        "# 前処理\n",
        "! rm -r data-bin\n",
        "SRC = \"en\"\n",
        "TRG = \"ja\"\n",
        "DATA_DIR = \"/content/data\"\n",
        "! fairseq-preprocess \\\n",
        "    --source-lang $SRC \\\n",
        "    --target-lang $TRG \\\n",
        "    --trainpref $DATA_DIR/train \\\n",
        "    --validpref $DATA_DIR/dev \\\n",
        "    --testpref $DATA_DIR/test \\\n",
        "    --joined-dictionary \\\n",
        "    --nwordstgt 30000 \\\n",
        "    --nwordssrc 30000 \\"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========\n",
        "# 91. 機械翻訳モデルの訓練\n",
        "# ============\n",
        "# 訓練\n",
        "! rm -r checkpoints\n",
        "! fairseq-train data-bin --arch transformer \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --lr 3e-5 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.1 --clip-norm 0.0 \\\n",
        "    --optimizer adam --max-tokens 5000 --max-epoch 20 \\\n",
        "    --patience 5 --no-epoch-checkpoints \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fg9HVK4QllM",
        "outputId": "c7ece6a9-993b-4a1d-cec1-980f3bfdfb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-30 20:01:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-30 20:01:02 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=5000, max_tokens_valid=5000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-01-30 20:01:02 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-01-30 20:01:02 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-01-30 20:01:03 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(30000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(30000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=30000, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-01-30 20:01:03 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-01-30 20:01:03 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-01-30 20:01:03 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-01-30 20:01:03 | INFO | fairseq_cli.train | num. shared model params: 90,218,496 (num. trained: 90,218,496)\n",
            "2022-01-30 20:01:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-01-30 20:01:03 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.en\n",
            "2022-01-30 20:01:03 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.ja\n",
            "2022-01-30 20:01:03 | INFO | fairseq.tasks.translation | data-bin valid en-ja 1166 examples\n",
            "2022-01-30 20:01:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-01-30 20:01:06 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2022-01-30 20:01:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-01-30 20:01:06 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-01-30 20:01:06 | INFO | fairseq_cli.train | max tokens per device = 5000 and max sentences per device = None\n",
            "2022-01-30 20:01:06 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2022-01-30 20:01:06 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2022-01-30 20:01:06 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-01-30 20:01:06 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.en\n",
            "2022-01-30 20:01:06 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.ja\n",
            "2022-01-30 20:01:06 | INFO | fairseq.tasks.translation | data-bin train en-ja 440288 examples\n",
            "2022-01-30 20:01:06 | INFO | fairseq.optim.adam | using FusedAdam\n",
            "2022-01-30 20:01:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 001:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 20:01:06 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-01-30 20:01:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 001: 100% 2853/2854 [14:21<00:00,  3.50it/s, loss=8.221, nll_loss=7.206, ppl=147.69, wps=12983.1, ups=3.31, wpb=3927.5, bsz=172.3, num_updates=2800, lr=2.1e-05, gnorm=2.431, train_wall=30, gb_free=11, wall=846]2022-01-30 20:15:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 6/11 [00:00<00:00, 10.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.70it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 20:15:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.242 | nll_loss 7.204 | ppl 147.41 | wps 26701.7 | wpb 2406.2 | bsz 106 | num_updates 2854\n",
            "2022-01-30 20:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2854 updates\n",
            "2022-01-30 20:15:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:15:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 2854 updates, score 8.242) (writing took 7.85149895900031 seconds)\n",
            "2022-01-30 20:15:37 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-01-30 20:15:37 | INFO | train | epoch 001 | loss 9.898 | nll_loss 9.163 | ppl 573.42 | wps 13271.6 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 2854 | lr 2.1405e-05 | gnorm 3.139 | train_wall 854 | gb_free 10.8 | wall 871\n",
            "2022-01-30 20:15:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 002:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 20:15:37 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-01-30 20:15:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 2853/2854 [14:24<00:00,  3.51it/s, loss=7.198, nll_loss=6.031, ppl=65.41, wps=13579, ups=3.33, wpb=4077.4, bsz=140, num_updates=5700, lr=2.51312e-05, gnorm=1.606, train_wall=30, gb_free=11.1, wall=1733]2022-01-30 20:30:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 6/11 [00:00<00:00, 10.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.67it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 20:30:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.234 | nll_loss 6.052 | ppl 66.35 | wps 26704.4 | wpb 2406.2 | bsz 106 | num_updates 5708 | best_loss 7.234\n",
            "2022-01-30 20:30:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 5708 updates\n",
            "2022-01-30 20:30:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:30:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:30:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 5708 updates, score 7.234) (writing took 8.132754035001199 seconds)\n",
            "2022-01-30 20:30:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-01-30 20:30:11 | INFO | train | epoch 002 | loss 7.609 | nll_loss 6.502 | ppl 90.65 | wps 13221.7 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 5708 | lr 2.51136e-05 | gnorm 2.045 | train_wall 856 | gb_free 10.9 | wall 1744\n",
            "2022-01-30 20:30:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 003:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 20:30:11 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-01-30 20:30:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 2853/2854 [14:24<00:00,  3.14it/s, loss=6.777, nll_loss=5.548, ppl=46.78, wps=13605.6, ups=3.3, wpb=4121, bsz=161, num_updates=8500, lr=2.05798e-05, gnorm=1.559, train_wall=30, gb_free=11.2, wall=2590]2022-01-30 20:44:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 6/11 [00:00<00:00, 10.00it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.66it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 20:44:36 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.77 | nll_loss 5.528 | ppl 46.14 | wps 26686.2 | wpb 2406.2 | bsz 106 | num_updates 8562 | best_loss 6.77\n",
            "2022-01-30 20:44:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 8562 updates\n",
            "2022-01-30 20:44:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:44:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:44:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 8562 updates, score 6.77) (writing took 8.005277491996821 seconds)\n",
            "2022-01-30 20:44:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-01-30 20:44:44 | INFO | train | epoch 003 | loss 6.971 | nll_loss 5.771 | ppl 54.6 | wps 13221.5 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 8562 | lr 2.05052e-05 | gnorm 1.786 | train_wall 856 | gb_free 10.8 | wall 2618\n",
            "2022-01-30 20:44:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 004:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 20:44:44 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-01-30 20:44:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 2853/2854 [14:22<00:00,  3.17it/s, loss=6.455, nll_loss=5.178, ppl=36.2, wps=13654, ups=3.3, wpb=4143.8, bsz=171, num_updates=11400, lr=1.77705e-05, gnorm=1.595, train_wall=30, gb_free=10.8, wall=3476]2022-01-30 20:59:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  9.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.64it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 20:59:08 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.482 | nll_loss 5.188 | ppl 36.46 | wps 26615.3 | wpb 2406.2 | bsz 106 | num_updates 11416 | best_loss 6.482\n",
            "2022-01-30 20:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 11416 updates\n",
            "2022-01-30 20:59:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 20:59:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 11416 updates, score 6.482) (writing took 8.534233367001434 seconds)\n",
            "2022-01-30 20:59:17 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-01-30 20:59:17 | INFO | train | epoch 004 | loss 6.624 | nll_loss 5.371 | ppl 41.39 | wps 13236.2 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 11416 | lr 1.7758e-05 | gnorm 1.693 | train_wall 855 | gb_free 10.9 | wall 3490\n",
            "2022-01-30 20:59:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 005:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 20:59:17 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-01-30 20:59:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 2853/2854 [14:24<00:00,  3.21it/s, loss=6.359, nll_loss=5.062, ppl=33.41, wps=13527.4, ups=3.26, wpb=4155.3, bsz=126.1, num_updates=14200, lr=1.59223e-05, gnorm=1.561, train_wall=30, gb_free=11.2, wall=4334]2022-01-30 21:13:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.76it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 21:13:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.337 | nll_loss 5.011 | ppl 32.25 | wps 27065.8 | wpb 2406.2 | bsz 106 | num_updates 14270 | best_loss 6.337\n",
            "2022-01-30 21:13:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 14270 updates\n",
            "2022-01-30 21:13:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:13:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 14270 updates, score 6.337) (writing took 8.53164180299791 seconds)\n",
            "2022-01-30 21:13:52 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-01-30 21:13:52 | INFO | train | epoch 005 | loss 6.383 | nll_loss 5.093 | ppl 34.13 | wps 13202.1 | ups 3.26 | wpb 4047 | bsz 154.3 | num_updates 14270 | lr 1.58832e-05 | gnorm 1.666 | train_wall 857 | gb_free 10.7 | wall 4365\n",
            "2022-01-30 21:13:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 006:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 21:13:52 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-01-30 21:13:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 2853/2854 [14:21<00:00,  3.49it/s, loss=6.107, nll_loss=4.775, ppl=27.38, wps=13479.5, ups=3.31, wpb=4067.1, bsz=167.7, num_updates=17100, lr=1.45095e-05, gnorm=1.74, train_wall=30, gb_free=11.1, wall=5219]2022-01-30 21:28:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.81it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.83it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.13it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.77it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 21:28:14 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.099 | nll_loss 4.733 | ppl 26.6 | wps 27068.9 | wpb 2406.2 | bsz 106 | num_updates 17124 | best_loss 6.099\n",
            "2022-01-30 21:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 17124 updates\n",
            "2022-01-30 21:28:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:28:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 17124 updates, score 6.099) (writing took 8.520905180997943 seconds)\n",
            "2022-01-30 21:28:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-01-30 21:28:23 | INFO | train | epoch 006 | loss 6.196 | nll_loss 4.877 | ppl 29.38 | wps 13261.4 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 17124 | lr 1.44994e-05 | gnorm 1.667 | train_wall 854 | gb_free 10.9 | wall 5236\n",
            "2022-01-30 21:28:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 007:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 21:28:23 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-01-30 21:28:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 2853/2854 [14:20<00:00,  3.25it/s, loss=6.016, nll_loss=4.666, ppl=25.39, wps=13499.7, ups=3.34, wpb=4047.8, bsz=156.6, num_updates=19900, lr=1.34501e-05, gnorm=1.724, train_wall=30, gb_free=11.5, wall=6073]2022-01-30 21:42:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.48it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.00it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.62it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 21:42:45 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.981 | nll_loss 4.589 | ppl 24.07 | wps 26850.9 | wpb 2406.2 | bsz 106 | num_updates 19978 | best_loss 5.981\n",
            "2022-01-30 21:42:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 19978 updates\n",
            "2022-01-30 21:42:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:42:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 19978 updates, score 5.981) (writing took 8.157217644999037 seconds)\n",
            "2022-01-30 21:42:53 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-01-30 21:42:53 | INFO | train | epoch 007 | loss 6.045 | nll_loss 4.701 | ppl 26.02 | wps 13268.9 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 19978 | lr 1.34238e-05 | gnorm 1.681 | train_wall 854 | gb_free 10.9 | wall 6107\n",
            "2022-01-30 21:42:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 008:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 21:42:53 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-01-30 21:42:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 2853/2854 [14:20<00:00,  3.24it/s, loss=5.957, nll_loss=4.596, ppl=24.19, wps=13278.1, ups=3.3, wpb=4019, bsz=151.6, num_updates=22800, lr=1.25656e-05, gnorm=1.736, train_wall=30, gb_free=10.7, wall=6958]2022-01-30 21:57:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.71it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.80it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.10it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.75it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 21:57:15 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.882 | nll_loss 4.476 | ppl 22.25 | wps 26953.6 | wpb 2406.2 | bsz 106 | num_updates 22832 | best_loss 5.882\n",
            "2022-01-30 21:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 22832 updates\n",
            "2022-01-30 21:57:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:57:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 21:57:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 22832 updates, score 5.882) (writing took 8.427369689998159 seconds)\n",
            "2022-01-30 21:57:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-01-30 21:57:23 | INFO | train | epoch 008 | loss 5.92 | nll_loss 4.556 | ppl 23.52 | wps 13271.5 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 22832 | lr 1.25568e-05 | gnorm 1.686 | train_wall 854 | gb_free 11.3 | wall 6977\n",
            "2022-01-30 21:57:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 009:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 21:57:23 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-01-30 21:57:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 2853/2854 [14:20<00:00,  3.42it/s, loss=5.762, nll_loss=4.374, ppl=20.73, wps=13411, ups=3.32, wpb=4043.1, bsz=149.7, num_updates=25600, lr=1.18585e-05, gnorm=1.652, train_wall=30, gb_free=10.7, wall=7812]2022-01-30 22:11:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.68it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.64it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 6/11 [00:00<00:00, 10.11it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.98it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.78it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 22:11:45 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.801 | nll_loss 4.374 | ppl 20.73 | wps 26845.3 | wpb 2406.2 | bsz 106 | num_updates 25686 | best_loss 5.801\n",
            "2022-01-30 22:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 25686 updates\n",
            "2022-01-30 22:11:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:11:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 25686 updates, score 5.801) (writing took 8.410604778000561 seconds)\n",
            "2022-01-30 22:11:54 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-01-30 22:11:54 | INFO | train | epoch 009 | loss 5.813 | nll_loss 4.432 | ppl 21.58 | wps 13273.7 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 25686 | lr 1.18387e-05 | gnorm 1.688 | train_wall 853 | gb_free 11.5 | wall 7847\n",
            "2022-01-30 22:11:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 010:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 22:11:54 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-01-30 22:11:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 2853/2854 [14:20<00:00,  3.35it/s, loss=5.622, nll_loss=4.211, ppl=18.52, wps=13353.1, ups=3.32, wpb=4019.2, bsz=165, num_updates=28500, lr=1.1239e-05, gnorm=1.672, train_wall=30, gb_free=10.9, wall=8696]2022-01-30 22:26:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.64it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.74it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.06it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.73it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 22:26:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.718 | nll_loss 4.275 | ppl 19.36 | wps 26941 | wpb 2406.2 | bsz 106 | num_updates 28540 | best_loss 5.718\n",
            "2022-01-30 22:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 28540 updates\n",
            "2022-01-30 22:26:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:26:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:26:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 28540 updates, score 5.718) (writing took 8.243921441997372 seconds)\n",
            "2022-01-30 22:26:24 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-01-30 22:26:24 | INFO | train | epoch 010 | loss 5.722 | nll_loss 4.326 | ppl 20.05 | wps 13274.5 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 28540 | lr 1.12312e-05 | gnorm 1.704 | train_wall 853 | gb_free 11.6 | wall 8717\n",
            "2022-01-30 22:26:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 011:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 22:26:24 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2022-01-30 22:26:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011: 100% 2853/2854 [14:20<00:00,  3.55it/s, loss=5.661, nll_loss=4.252, ppl=19.06, wps=13482.6, ups=3.3, wpb=4088.4, bsz=143.5, num_updates=31300, lr=1.07246e-05, gnorm=1.745, train_wall=30, gb_free=11.8, wall=9551]2022-01-30 22:40:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.57it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.73it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.06it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.74it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 22:40:46 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.685 | nll_loss 4.236 | ppl 18.84 | wps 26953.3 | wpb 2406.2 | bsz 106 | num_updates 31394 | best_loss 5.685\n",
            "2022-01-30 22:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 31394 updates\n",
            "2022-01-30 22:40:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:40:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 31394 updates, score 5.685) (writing took 8.270042957999976 seconds)\n",
            "2022-01-30 22:40:54 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-01-30 22:40:54 | INFO | train | epoch 011 | loss 5.641 | nll_loss 4.231 | ppl 18.78 | wps 13273.1 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 31394 | lr 1.07085e-05 | gnorm 1.72 | train_wall 854 | gb_free 11.7 | wall 9588\n",
            "2022-01-30 22:40:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 012:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 22:40:54 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2022-01-30 22:40:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012: 100% 2853/2854 [14:21<00:00,  3.32it/s, loss=5.556, nll_loss=4.131, ppl=17.52, wps=13335.5, ups=3.36, wpb=3972.8, bsz=141.2, num_updates=34200, lr=1.02598e-05, gnorm=1.785, train_wall=30, gb_free=10.8, wall=10434]2022-01-30 22:55:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.72it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.05it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.73it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 22:55:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.614 | nll_loss 4.149 | ppl 17.75 | wps 26974.2 | wpb 2406.2 | bsz 106 | num_updates 34248 | best_loss 5.614\n",
            "2022-01-30 22:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 34248 updates\n",
            "2022-01-30 22:55:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:55:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 22:55:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 34248 updates, score 5.614) (writing took 8.383968016001745 seconds)\n",
            "2022-01-30 22:55:25 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-01-30 22:55:25 | INFO | train | epoch 012 | loss 5.57 | nll_loss 4.149 | ppl 17.73 | wps 13265 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 34248 | lr 1.02526e-05 | gnorm 1.734 | train_wall 854 | gb_free 11.4 | wall 10458\n",
            "2022-01-30 22:55:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 013:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 22:55:25 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2022-01-30 22:55:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013: 100% 2853/2854 [14:20<00:00,  3.23it/s, loss=5.526, nll_loss=4.095, ppl=17.08, wps=13462.9, ups=3.32, wpb=4057.4, bsz=146.6, num_updates=37100, lr=9.85064e-06, gnorm=1.694, train_wall=30, gb_free=11.4, wall=11319]2022-01-30 23:09:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.46it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.54it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  55% 6/11 [00:00<00:00, 10.01it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.89it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.67it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 23:09:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.56 | nll_loss 4.089 | ppl 17.02 | wps 26711.9 | wpb 2406.2 | bsz 106 | num_updates 37102 | best_loss 5.56\n",
            "2022-01-30 23:09:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 37102 updates\n",
            "2022-01-30 23:09:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:09:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 37102 updates, score 5.56) (writing took 8.343216069999471 seconds)\n",
            "2022-01-30 23:09:55 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2022-01-30 23:09:55 | INFO | train | epoch 013 | loss 5.507 | nll_loss 4.075 | ppl 16.85 | wps 13266.7 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 37102 | lr 9.85037e-06 | gnorm 1.742 | train_wall 854 | gb_free 10.8 | wall 11329\n",
            "2022-01-30 23:09:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 014:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 23:09:55 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2022-01-30 23:09:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014: 100% 2853/2854 [14:21<00:00,  3.54it/s, loss=5.408, nll_loss=3.959, ppl=15.55, wps=13299.4, ups=3.29, wpb=4047, bsz=164, num_updates=39900, lr=9.49871e-06, gnorm=1.705, train_wall=30, gb_free=11.5, wall=12174]2022-01-30 23:24:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.41it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.62it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.97it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.82it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.62it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 23:24:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.545 | nll_loss 4.072 | ppl 16.81 | wps 26686.2 | wpb 2406.2 | bsz 106 | num_updates 39956 | best_loss 5.545\n",
            "2022-01-30 23:24:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 39956 updates\n",
            "2022-01-30 23:24:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:24:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 39956 updates, score 5.545) (writing took 8.643878266000684 seconds)\n",
            "2022-01-30 23:24:27 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2022-01-30 23:24:27 | INFO | train | epoch 014 | loss 5.45 | nll_loss 4.008 | ppl 16.09 | wps 13253.3 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 39956 | lr 9.49206e-06 | gnorm 1.764 | train_wall 854 | gb_free 11.3 | wall 12200\n",
            "2022-01-30 23:24:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 015:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 23:24:27 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2022-01-30 23:24:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015: 100% 2853/2854 [14:20<00:00,  3.32it/s, loss=5.308, nll_loss=3.845, ppl=14.37, wps=13769.4, ups=3.32, wpb=4147.3, bsz=166.8, num_updates=42800, lr=9.17127e-06, gnorm=1.791, train_wall=30, gb_free=11.5, wall=13059]2022-01-30 23:38:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.66it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.77it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.09it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.75it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 23:38:49 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.497 | nll_loss 4.004 | ppl 16.04 | wps 26916.8 | wpb 2406.2 | bsz 106 | num_updates 42810 | best_loss 5.497\n",
            "2022-01-30 23:38:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 42810 updates\n",
            "2022-01-30 23:38:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:38:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:39:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 42810 updates, score 5.497) (writing took 11.505529947000468 seconds)\n",
            "2022-01-30 23:39:01 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2022-01-30 23:39:01 | INFO | train | epoch 015 | loss 5.398 | nll_loss 3.948 | ppl 15.43 | wps 13218.2 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 42810 | lr 9.1702e-06 | gnorm 1.768 | train_wall 854 | gb_free 11.4 | wall 13074\n",
            "2022-01-30 23:39:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 016:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 23:39:01 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2022-01-30 23:39:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016: 100% 2853/2854 [14:21<00:00,  3.23it/s, loss=5.315, nll_loss=3.85, ppl=14.42, wps=13331.9, ups=3.33, wpb=3998.4, bsz=155.8, num_updates=45600, lr=8.88523e-06, gnorm=1.837, train_wall=30, gb_free=10.8, wall=13917]2022-01-30 23:53:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.55it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.72it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.02it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.71it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-30 23:53:23 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.456 | nll_loss 3.964 | ppl 15.61 | wps 26901.3 | wpb 2406.2 | bsz 106 | num_updates 45664 | best_loss 5.456\n",
            "2022-01-30 23:53:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 45664 updates\n",
            "2022-01-30 23:53:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:53:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-30 23:53:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 45664 updates, score 5.456) (writing took 8.3474279760012 seconds)\n",
            "2022-01-30 23:53:32 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2022-01-30 23:53:32 | INFO | train | epoch 016 | loss 5.351 | nll_loss 3.893 | ppl 14.85 | wps 13258.7 | ups 3.28 | wpb 4047 | bsz 154.3 | num_updates 45664 | lr 8.879e-06 | gnorm 1.782 | train_wall 854 | gb_free 11.8 | wall 13945\n",
            "2022-01-30 23:53:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 017:   0% 0/2854 [00:00<?, ?it/s]2022-01-30 23:53:32 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2022-01-30 23:53:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017: 100% 2853/2854 [14:21<00:00,  3.18it/s, loss=5.32, nll_loss=3.855, ppl=14.47, wps=13193.8, ups=3.33, wpb=3957.6, bsz=138.9, num_updates=48500, lr=8.6155e-06, gnorm=1.869, train_wall=30, gb_free=10.6, wall=14802]2022-01-31 00:07:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.46it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.58it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.91it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.75it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.56it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-31 00:07:55 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.406 | nll_loss 3.902 | ppl 14.95 | wps 26532.7 | wpb 2406.2 | bsz 106 | num_updates 48518 | best_loss 5.406\n",
            "2022-01-31 00:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 48518 updates\n",
            "2022-01-31 00:07:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:08:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 48518 updates, score 5.406) (writing took 8.517218638000486 seconds)\n",
            "2022-01-31 00:08:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2022-01-31 00:08:04 | INFO | train | epoch 017 | loss 5.308 | nll_loss 3.842 | ppl 14.34 | wps 13247.8 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 48518 | lr 8.6139e-06 | gnorm 1.79 | train_wall 855 | gb_free 10.8 | wall 14817\n",
            "2022-01-31 00:08:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 018:   0% 0/2854 [00:00<?, ?it/s]2022-01-31 00:08:04 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2022-01-31 00:08:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018: 100% 2853/2854 [14:22<00:00,  3.61it/s, loss=5.256, nll_loss=3.781, ppl=13.75, wps=13636.6, ups=3.29, wpb=4149.4, bsz=158.1, num_updates=51300, lr=8.37708e-06, gnorm=1.771, train_wall=30, gb_free=11.1, wall=15659]2022-01-31 00:22:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.48it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.68it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.02it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.68it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-31 00:22:28 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.392 | nll_loss 3.883 | ppl 14.75 | wps 26795.5 | wpb 2406.2 | bsz 106 | num_updates 51372 | best_loss 5.392\n",
            "2022-01-31 00:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 51372 updates\n",
            "2022-01-31 00:22:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:22:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:22:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 51372 updates, score 5.392) (writing took 11.542124276998948 seconds)\n",
            "2022-01-31 00:22:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2022-01-31 00:22:39 | INFO | train | epoch 018 | loss 5.268 | nll_loss 3.795 | ppl 13.88 | wps 13188.6 | ups 3.26 | wpb 4047 | bsz 154.3 | num_updates 51372 | lr 8.37121e-06 | gnorm 1.796 | train_wall 856 | gb_free 10.9 | wall 15693\n",
            "2022-01-31 00:22:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 019:   0% 0/2854 [00:00<?, ?it/s]2022-01-31 00:22:39 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2022-01-31 00:22:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019: 100% 2853/2854 [14:22<00:00,  3.14it/s, loss=5.236, nll_loss=3.757, ppl=13.52, wps=13103.5, ups=3.32, wpb=3947.5, bsz=144.9, num_updates=54200, lr=8.14989e-06, gnorm=1.824, train_wall=30, gb_free=10.6, wall=16549]2022-01-31 00:37:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.47it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  27% 3/11 [00:00<00:00,  8.65it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  9.66it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  64% 7/11 [00:00<00:00, 10.01it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.84it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.65it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-31 00:37:04 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.36 | nll_loss 3.844 | ppl 14.36 | wps 26700 | wpb 2406.2 | bsz 106 | num_updates 54226 | best_loss 5.36\n",
            "2022-01-31 00:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 54226 updates\n",
            "2022-01-31 00:37:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:37:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:37:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 54226 updates, score 5.36) (writing took 8.044386333000148 seconds)\n",
            "2022-01-31 00:37:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2022-01-31 00:37:12 | INFO | train | epoch 019 | loss 5.231 | nll_loss 3.753 | ppl 13.48 | wps 13239.4 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 54226 | lr 8.14793e-06 | gnorm 1.811 | train_wall 856 | gb_free 10.9 | wall 16565\n",
            "2022-01-31 00:37:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2854\n",
            "epoch 020:   0% 0/2854 [00:00<?, ?it/s]2022-01-31 00:37:12 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2022-01-31 00:37:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020: 100% 2853/2854 [14:23<00:00,  3.17it/s, loss=5.26, nll_loss=3.783, ppl=13.76, wps=12988.7, ups=3.3, wpb=3930.2, bsz=135.8, num_updates=57000, lr=7.94719e-06, gnorm=1.835, train_wall=30, gb_free=11.2, wall=17404]2022-01-31 00:51:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   9% 1/11 [00:00<00:01,  5.44it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  9.53it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  55% 6/11 [00:00<00:00, 10.06it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.93it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  82% 9/11 [00:00<00:00,  9.74it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-01-31 00:51:36 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.339 | nll_loss 3.821 | ppl 14.14 | wps 26739.7 | wpb 2406.2 | bsz 106 | num_updates 57080 | best_loss 5.339\n",
            "2022-01-31 00:51:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 57080 updates\n",
            "2022-01-31 00:51:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:51:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-01-31 00:51:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 57080 updates, score 5.339) (writing took 8.100869439000235 seconds)\n",
            "2022-01-31 00:51:44 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2022-01-31 00:51:44 | INFO | train | epoch 020 | loss 5.197 | nll_loss 3.712 | ppl 13.11 | wps 13237.6 | ups 3.27 | wpb 4047 | bsz 154.3 | num_updates 57080 | lr 7.94162e-06 | gnorm 1.821 | train_wall 856 | gb_free 11.7 | wall 17438\n",
            "2022-01-31 00:51:44 | INFO | fairseq_cli.train | done training in 17437.8 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELH0qKmE4HBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4708d9-afdc-4e49-d48d-d16ece6bed95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-27 12:45:09 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/言語処理100本ノック2020/ginza_spacy_epoch20.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-27 12:45:09 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-02-27 12:45:09 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-02-27 12:45:09 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/言語処理100本ノック2020/ginza_spacy_epoch20.pt\n",
            "2022-02-27 12:45:16 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-27 12:45:16 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-27 12:45:16 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/10 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-27 12:45:40 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-27 12:45:40 | INFO | fairseq_cli.generate | Translated 1,160 sentences (25,733 tokens) in 10.1s (115.26 sentences/s, 2556.95 tokens/s)\n",
            "S-112\tTransferred from the Kagoshima <unk> Depot .\n",
            "T-112\t鹿児島 機関 区 より <<unk>> 。\n",
            "H-112\t-1.0333943367004395\t鹿児島 車庫 から 鹿児島 車庫 に <unk> 。\n",
            "D-112\t-1.0333943367004395\t鹿児島 車庫 から 鹿児島 車庫 に <unk> 。\n",
            "P-112\t-0.0562 -1.4658 -1.5441 -0.1315 -1.3797 -1.3176 -2.8858 -0.3986 -0.1213\n",
            "S-189\tJapanese National Railways Passenger Car 50 Series\n",
            "T-189\t国鉄 50 系 客車\n",
            "H-189\t-1.2606467008590698\t日本 国有 鉄道 国鉄 50 系 電車\n",
            "D-189\t-1.2606467008590698\t日本 国有 鉄道 国鉄 50 系 電車\n",
            "P-189\t-0.7966 -1.8336 -0.4979 -4.0690 -0.5030 -1.1882 -1.0792 -0.1176\n",
            "S-508\tVolume 5 might have been lost .\n",
            "T-508\t5 巻 紛失 ?\n",
            "H-508\t-1.132690668106079\t巻 5 は 失わ れ た 。\n",
            "D-508\t-1.132690668106079\t巻 5 は 失わ れ た 。\n",
            "P-508\t-0.3453 -0.5691 -3.4802 -2.1530 -0.0941 -0.6634 -1.6034 -0.1530\n",
            "S-833\tJNR / JR Commuter Train Series 103\n",
            "T-833\t国鉄 103 系 電車\n",
            "H-833\t-0.3240832984447479\t国鉄 103 系 電車\n",
            "D-833\t-0.3240832984447479\t国鉄 103 系 電車\n",
            "P-833\t-0.1279 -1.3231 -0.0177 -0.0684 -0.0833\n",
            "仏教\n",
            "道元 （ <unk> ） は 、 鎌倉 時代 初期 の 禅僧 。\n",
            "曹洞 禅 の 祖 。\n",
            "その 後 も <unk> <unk> と 号し た 。\n",
            "宗派 に よっ て は <unk> の 称号 と 呼ば れる 。\n",
            "<unk> は <unk> 国師 、 <unk> 国師 と いう 。\n",
            "一般 に は 道元 禅師 と 呼ば れる 。\n",
            "日本 で は <unk> の <unk> 、 <unk> 、 <unk> 、 <unk> など が 普及 し た と さ れる 。\n",
            "また 、 日本 で 最初 の <unk> に <unk> を 入れ た と いう 話 も ある 。\n",
            "道元 の 出生 に つい て は 諸説 ある が 、 道元 の 子 で ある 右 大臣 源 通親 （ 通親 ・ 通親 ） や 通親 の 通親 （ 通親 ・ 通親 ） に 生まれる 。\n"
          ]
        }
      ],
      "source": [
        "# ===========\n",
        "# 92. 機械翻訳モデルの適用\n",
        "#============\n",
        "! mkdir -p /content/result90-94\n",
        "! fairseq-generate data-bin \\\n",
        "   --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/ginza_spacy_epoch20.pt\" \\\n",
        "   --batch-size 128 \\\n",
        "   --beam 5 > /content/result90-94/result.txt\n",
        "! head -20 /content/result90-94/result.txt\n",
        "\n",
        "# =============================\n",
        "# 出力ファイルから生成文を抽出\n",
        "# =============================\n",
        "! grep \"^H-\" /content/result90-94/result.txt | sort -V | cut -f3 > /content/result90-94/pred.txt\n",
        "! head /content/result90-94/pred.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciIcofh74HJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c1c1cf-e112-4159-d737-ef105a7ccb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "Namespace(ignore_case=False, order=4, ref='/content/data/test.ja', sacrebleu=False, sentence_bleu=False, sys='/content/result90-94/pred.txt')\n",
            "BLEU4 = 17.92, 50.5/24.4/14.0/8.8 (BP=0.908, ratio=0.912, syslen=24573, reflen=26951)\n"
          ]
        }
      ],
      "source": [
        "# ===========\n",
        "# 93. BLEUスコアの計測\n",
        "#============\n",
        "! fairseq-score --sys /content/result90-94/pred.txt --ref /content/data/test.ja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHxHBGOL4HQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85812b5f-dcb5-4805-87a8-56b557f0e205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-31 12:01:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:01:03 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-01-31 12:01:03 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-01-31 12:01:03 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-01-31 12:01:03 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt\n",
            "2022-01-31 12:01:06 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-01-31 12:01:06 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-01-31 12:01:06 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/54 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-01-31 12:01:41 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-01-31 12:01:41 | INFO | fairseq_cli.generate | Translated 1,160 sentences (25,991 tokens) in 26.8s (43.28 sentences/s, 969.81 tokens/s)\n",
            "2022-01-31 12:01:43 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:01:45 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 25, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-01-31 12:01:45 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-01-31 12:01:45 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-01-31 12:01:45 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt\n",
            "2022-01-31 12:01:48 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-01-31 12:01:48 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-01-31 12:01:48 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/54 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-01-31 12:02:55 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-01-31 12:02:55 | INFO | fairseq_cli.generate | Translated 1,160 sentences (25,784 tokens) in 59.5s (19.51 sentences/s, 433.70 tokens/s)\n",
            "2022-01-31 12:02:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:02:59 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 50, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-01-31 12:02:59 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-01-31 12:02:59 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-01-31 12:02:59 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt\n",
            "2022-01-31 12:03:02 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-01-31 12:03:02 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-01-31 12:03:02 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/54 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-01-31 12:05:01 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-01-31 12:05:01 | INFO | fairseq_cli.generate | Translated 1,160 sentences (25,981 tokens) in 111.7s (10.39 sentences/s, 232.68 tokens/s)\n",
            "2022-01-31 12:05:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:05:06 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 75, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-01-31 12:05:06 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-01-31 12:05:06 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-01-31 12:05:06 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt\n",
            "2022-01-31 12:05:08 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-01-31 12:05:08 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-01-31 12:05:08 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/54 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-01-31 12:08:00 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-01-31 12:08:00 | INFO | fairseq_cli.generate | Translated 1,160 sentences (26,044 tokens) in 163.7s (7.09 sentences/s, 159.12 tokens/s)\n",
            "2022-01-31 12:08:02 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:08:04 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-01-31 12:08:04 | INFO | fairseq.tasks.translation | [en] dictionary: 30000 types\n",
            "2022-01-31 12:08:04 | INFO | fairseq.tasks.translation | [ja] dictionary: 30000 types\n",
            "2022-01-31 12:08:04 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt\n",
            "2022-01-31 12:08:07 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-01-31 12:08:07 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-01-31 12:08:07 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/54 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-01-31 12:11:50 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-01-31 12:11:50 | INFO | fairseq_cli.generate | Translated 1,160 sentences (26,048 tokens) in 214.9s (5.40 sentences/s, 121.23 tokens/s)\n",
            "2022-01-31 12:11:52 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:11:55 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:11:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:11:59 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-01-31 12:12:02 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# ===========\n",
        "# 94. ビーム探索\n",
        "#============\n",
        "! mkdir -p /content/result90-94\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-generate data-bin --path /content/drive/MyDrive/ColabNotebooks/knock_100/ginza_spacy_epoch20.pt --batch-size 128 --max-tokens 1000 --beam $N > /content/result90-94/result.txt\n",
        "    ! grep \"^H-\" /content/result90-94/result.txt | sort -V | cut -f3 > /content/result90-94/pred.txt.$N\n",
        "\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-score --sys /content/result90-94/pred.txt.$N --ref /content/data/test.ja > /content/result90-94/BLEU.score.$N\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def read_score(filename):\n",
        "    with open(filename) as f:\n",
        "        line = f.readlines()[1]\n",
        "        line = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', line)\n",
        "        return float(line.group())\n",
        "\n",
        "x = [1, 25, 50, 75, 100]\n",
        "y = [read_score(f'/content/result90-94/BLEU.score.{num}') for num in x]\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1zKySel44HXD"
      },
      "outputs": [],
      "source": [
        "# ===========\n",
        "# 95. サブワード化\n",
        "#============\n",
        "! mkdir -p /content/data/spm\n",
        "# sentencepieceモデルの作成\n",
        "# ! spm_train --input=/content/kftt-data-1.0/data/orig/kyoto-train.en --model_prefix=/content/data/spm/spm_en --vocab_size=32000 --character_coverage=0.9995\n",
        "# ! spm_train --input=/content/kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=/content/data/spm/spm_ja --vocab_size=32000 --character_coverage=0.9995\n",
        "# /content/data/spm/spm_en と /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_en.model は同じもの \n",
        "# /content/data/spm/spm_ja と /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model は同じもの\n",
        "\n",
        "! mkdir -p /content/data/spm_data\n",
        "! spm_encode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_en.model\" --output_format=piece < /content/kftt-data-1.0/data/orig/kyoto-train.en > /content/data/spm_data/train.en\n",
        "! spm_encode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_en.model\" --output_format=piece < /content/kftt-data-1.0/data/orig/kyoto-dev.en > /content/data/spm_data/dev.en\n",
        "! spm_encode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_en.model\" --output_format=piece < /content/kftt-data-1.0/data/orig/kyoto-test.en > /content/data/spm_data/test.en\n",
        "! spm_encode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --output_format=piece < /content/kftt-data-1.0/data/orig/kyoto-train.ja > /content/data/spm_data/train.ja\n",
        "! spm_encode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --output_format=piece < /content/kftt-data-1.0/data/orig/kyoto-dev.ja > /content/data/spm_data/dev.ja\n",
        "! spm_encode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --output_format=piece < /content/kftt-data-1.0/data/orig/kyoto-test.ja > /content/data/spm_data/test.ja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========\n",
        "# 95. 91～94の繰り返し\n",
        "#============\n",
        "# 前処理\n",
        "! rm -r data-bin\n",
        "SRC = \"en\"\n",
        "TRG = \"ja\"\n",
        "DATA_DIR = \"/content/data/spm_data\"\n",
        "! fairseq-preprocess \\\n",
        "    --source-lang $SRC \\\n",
        "    --target-lang $TRG \\\n",
        "    --trainpref $DATA_DIR/train \\\n",
        "    --validpref $DATA_DIR/dev \\\n",
        "    --testpref $DATA_DIR/test \\\n",
        "    --srcdict \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/dict.en.txt\" \\\n",
        "    --tgtdict \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/dict.ja.txt\" \\"
      ],
      "metadata": {
        "id": "WD90VWLVqPln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3deffca-b8fb-4df0-a06e-1b985e8f334b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data-bin': No such file or directory\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:13:56 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict='/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/dict.en.txt', suppress_crashes=False, target_lang='ja', task='translation', tensorboard_logdir=None, testpref='/content/data/spm_data/test', tgtdict='/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/dict.ja.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/data/spm_data/train', use_plasma_view=False, user_dir=None, validpref='/content/data/spm_data/dev', wandb_project=None, workers=1)\n",
            "2022-03-01 07:13:56 | INFO | fairseq_cli.preprocess | [en] Dictionary: 32000 types\n",
            "2022-03-01 07:15:20 | INFO | fairseq_cli.preprocess | [en] /content/data/spm_data/train.en: 440288 sents, 13100758 tokens, 0.126% replaced (by <unk>)\n",
            "2022-03-01 07:15:20 | INFO | fairseq_cli.preprocess | [en] Dictionary: 32000 types\n",
            "2022-03-01 07:15:20 | INFO | fairseq_cli.preprocess | [en] /content/data/spm_data/dev.en: 1166 sents, 28578 tokens, 0.168% replaced (by <unk>)\n",
            "2022-03-01 07:15:20 | INFO | fairseq_cli.preprocess | [en] Dictionary: 32000 types\n",
            "2022-03-01 07:15:20 | INFO | fairseq_cli.preprocess | [en] /content/data/spm_data/test.en: 1160 sents, 30791 tokens, 0.11% replaced (by <unk>)\n",
            "2022-03-01 07:15:20 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 32000 types\n",
            "2022-03-01 07:16:50 | INFO | fairseq_cli.preprocess | [ja] /content/data/spm_data/train.ja: 440288 sents, 9429991 tokens, 0.075% replaced (by <unk>)\n",
            "2022-03-01 07:16:50 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 32000 types\n",
            "2022-03-01 07:16:50 | INFO | fairseq_cli.preprocess | [ja] /content/data/spm_data/dev.ja: 1166 sents, 21989 tokens, 0.0546% replaced (by <unk>)\n",
            "2022-03-01 07:16:50 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 32000 types\n",
            "2022-03-01 07:16:50 | INFO | fairseq_cli.preprocess | [ja] /content/data/spm_data/test.ja: 1160 sents, 23579 tokens, 0.0594% replaced (by <unk>)\n",
            "2022-03-01 07:16:50 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練\n",
        "! rm -r /content/checkpoints\n",
        "! fairseq-train data-bin --arch transformer \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --lr 3e-5 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --clip-norm 0.0 \\\n",
        "    --optimizer adam --max-tokens 5000 --max-epoch 120 \\\n",
        "    --patience 5 --no-epoch-checkpoints --tensorboard-logdir log  \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q2FL8qHKgdG",
        "outputId": "20dd8cf2-a111-46e6-add9-efbfe51c5a98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-28 16:08:07 | INFO | numexpr.utils | NumExpr defaulting to 4 threads.\n",
            "2022-02-28 16:08:07 | WARNING | apex.transformer.tensor_parallel | `fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 16:08:10 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': 'log', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 120, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch92.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='inverse_sqrt', max_epoch=120, max_tokens=5000, max_tokens_valid=5000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch92.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='log', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 16:08:10 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 16:08:10 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 16:08:12 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=32000, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-02-28 16:08:12 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-02-28 16:08:12 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-02-28 16:08:12 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-02-28 16:08:12 | INFO | fairseq_cli.train | num. shared model params: 93,290,496 (num. trained: 93,290,496)\n",
            "2022-02-28 16:08:12 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-02-28 16:08:12 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.en\n",
            "2022-02-28 16:08:12 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.ja\n",
            "2022-02-28 16:08:12 | INFO | fairseq.tasks.translation | data-bin valid en-ja 1166 examples\n",
            "2022-02-28 16:08:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-02-28 16:08:15 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2022-02-28 16:08:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-02-28 16:08:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-02-28 16:08:15 | INFO | fairseq_cli.train | max tokens per device = 5000 and max sentences per device = None\n",
            "2022-02-28 16:08:15 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch92.pt\n",
            "2022-02-28 16:08:17 | INFO | fairseq.optim.adam | using FusedAdam\n",
            "2022-02-28 16:08:18 | INFO | fairseq.trainer | Loaded checkpoint /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch92.pt (epoch 93 @ 262016 updates)\n",
            "2022-02-28 16:08:18 | INFO | fairseq.trainer | loading train data for epoch 93\n",
            "2022-02-28 16:08:18 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.en\n",
            "2022-02-28 16:08:18 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.ja\n",
            "2022-02-28 16:08:18 | INFO | fairseq.tasks.translation | data-bin train en-ja 440288 examples\n",
            "2022-02-28 16:08:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 093:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 16:08:18 | INFO | fairseq.trainer | begin training epoch 93\n",
            "2022-02-28 16:08:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 093: 100% 2847/2848 [13:40<00:00,  3.44it/s, loss=6.25, nll_loss=4.778, ppl=27.43, wps=10917.7, ups=3.55, wpb=3074.4, bsz=147.7, num_updates=264800, lr=3.68716e-06, gnorm=2.5, train_wall=28, gb_free=10.6, wall=806]2022-02-28 16:21:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 093 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.34it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.42it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.07it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.14it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 16:22:01 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 6.306 | nll_loss 4.725 | ppl 26.46 | wps 21220.3 | wpb 1999 | bsz 106 | num_updates 264864 | best_loss 6.306\n",
            "2022-02-28 16:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 264864 updates\n",
            "2022-02-28 16:22:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 16:22:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 16:22:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 93 @ 264864 updates, score 6.306) (writing took 7.997311781000462 seconds)\n",
            "2022-02-28 16:22:09 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
            "2022-02-28 16:22:09 | INFO | train | epoch 093 | loss 6.258 | nll_loss 4.786 | ppl 27.59 | wps 11355.7 | ups 3.43 | wpb 3311.1 | bsz 154.6 | num_updates 264864 | lr 3.68672e-06 | gnorm 2.4 | train_wall 807 | gb_free 11.8 | wall 833\n",
            "2022-02-28 16:22:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 094:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 16:22:09 | INFO | fairseq.trainer | begin training epoch 94\n",
            "2022-02-28 16:22:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 094: 100% 2847/2848 [13:35<00:00,  3.21it/s, loss=6.281, nll_loss=4.813, ppl=28.1, wps=11563.6, ups=3.5, wpb=3299.8, bsz=147, num_updates=267700, lr=3.66714e-06, gnorm=2.42, train_wall=28, gb_free=11.3, wall=1646]2022-02-28 16:35:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 094 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.67it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.63it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.10it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.16it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.23it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 16:35:46 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 6.302 | nll_loss 4.72 | ppl 26.36 | wps 21131.2 | wpb 1999 | bsz 106 | num_updates 267712 | best_loss 6.302\n",
            "2022-02-28 16:35:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 267712 updates\n",
            "2022-02-28 16:35:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 16:35:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 16:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 94 @ 267712 updates, score 6.302) (writing took 8.223234592998779 seconds)\n",
            "2022-02-28 16:35:54 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
            "2022-02-28 16:35:54 | INFO | train | epoch 094 | loss 6.246 | nll_loss 4.772 | ppl 27.33 | wps 11421.2 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 267712 | lr 3.66705e-06 | gnorm 2.402 | train_wall 807 | gb_free 11.2 | wall 1659\n",
            "2022-02-28 16:35:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 095:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 16:35:54 | INFO | fairseq.trainer | begin training epoch 95\n",
            "2022-02-28 16:35:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 095: 100% 2847/2848 [13:35<00:00,  3.40it/s, loss=6.259, nll_loss=4.787, ppl=27.6, wps=11959.9, ups=3.45, wpb=3466.5, bsz=179.1, num_updates=270500, lr=3.64811e-06, gnorm=2.423, train_wall=29, gb_free=11.7, wall=2458]2022-02-28 16:49:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 095 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.62it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.66it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.18it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.23it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 16:49:31 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 6.306 | nll_loss 4.724 | ppl 26.43 | wps 21126.1 | wpb 1999 | bsz 106 | num_updates 270560 | best_loss 6.302\n",
            "2022-02-28 16:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 270560 updates\n",
            "2022-02-28 16:49:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 16:49:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 16:49:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 95 @ 270560 updates, score 6.306) (writing took 3.9817301830007636 seconds)\n",
            "2022-02-28 16:49:35 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
            "2022-02-28 16:49:35 | INFO | train | epoch 095 | loss 6.235 | nll_loss 4.759 | ppl 27.09 | wps 11486.6 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 270560 | lr 3.6477e-06 | gnorm 2.407 | train_wall 806 | gb_free 11.6 | wall 2480\n",
            "2022-02-28 16:49:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 096:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 16:49:35 | INFO | fairseq.trainer | begin training epoch 96\n",
            "2022-02-28 16:49:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 096: 100% 2847/2848 [13:35<00:00,  3.61it/s, loss=6.284, nll_loss=4.814, ppl=28.14, wps=11166.4, ups=3.58, wpb=3121.1, bsz=149.5, num_updates=273400, lr=3.62871e-06, gnorm=2.459, train_wall=28, gb_free=12.3, wall=3293]2022-02-28 17:03:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 096 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.39it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.44it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.50it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.22it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 17:03:12 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 6.285 | nll_loss 4.697 | ppl 25.94 | wps 21083 | wpb 1999 | bsz 106 | num_updates 273408 | best_loss 6.285\n",
            "2022-02-28 17:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 273408 updates\n",
            "2022-02-28 17:03:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:03:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:03:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 96 @ 273408 updates, score 6.285) (writing took 8.092784481999843 seconds)\n",
            "2022-02-28 17:03:20 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
            "2022-02-28 17:03:20 | INFO | train | epoch 096 | loss 6.223 | nll_loss 4.746 | ppl 26.83 | wps 11431.5 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 273408 | lr 3.62865e-06 | gnorm 2.413 | train_wall 806 | gb_free 11.2 | wall 3305\n",
            "2022-02-28 17:03:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 097:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 17:03:20 | INFO | fairseq.trainer | begin training epoch 97\n",
            "2022-02-28 17:03:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 097: 100% 2847/2848 [13:35<00:00,  3.29it/s, loss=6.228, nll_loss=4.751, ppl=26.93, wps=11707.8, ups=3.5, wpb=3342.7, bsz=151.3, num_updates=276200, lr=3.61027e-06, gnorm=2.38, train_wall=28, gb_free=10.8, wall=4105]2022-02-28 17:16:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 097 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.66it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.61it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.18it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.23it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 17:16:57 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 6.266 | nll_loss 4.678 | ppl 25.6 | wps 21024.2 | wpb 1999 | bsz 106 | num_updates 276256 | best_loss 6.266\n",
            "2022-02-28 17:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 276256 updates\n",
            "2022-02-28 17:16:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:17:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 97 @ 276256 updates, score 6.266) (writing took 8.35209995599871 seconds)\n",
            "2022-02-28 17:17:06 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
            "2022-02-28 17:17:06 | INFO | train | epoch 097 | loss 6.211 | nll_loss 4.732 | ppl 26.58 | wps 11423.1 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 276256 | lr 3.6099e-06 | gnorm 2.415 | train_wall 807 | gb_free 10.5 | wall 4130\n",
            "2022-02-28 17:17:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 098:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 17:17:06 | INFO | fairseq.trainer | begin training epoch 98\n",
            "2022-02-28 17:17:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 098: 100% 2847/2848 [13:35<00:00,  3.78it/s, loss=6.103, nll_loss=4.611, ppl=24.43, wps=11249.6, ups=3.48, wpb=3231.2, bsz=167.2, num_updates=279100, lr=3.59146e-06, gnorm=2.439, train_wall=28, gb_free=10.7, wall=4945]2022-02-28 17:30:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 098 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.41it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.43it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.02it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.16it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 17:30:43 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 6.268 | nll_loss 4.677 | ppl 25.57 | wps 21057.3 | wpb 1999 | bsz 106 | num_updates 279104 | best_loss 6.266\n",
            "2022-02-28 17:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 279104 updates\n",
            "2022-02-28 17:30:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 17:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 17:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 98 @ 279104 updates, score 6.268) (writing took 3.9751220000034664 seconds)\n",
            "2022-02-28 17:30:47 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
            "2022-02-28 17:30:47 | INFO | train | epoch 098 | loss 6.201 | nll_loss 4.72 | ppl 26.35 | wps 11485.2 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 279104 | lr 3.59144e-06 | gnorm 2.416 | train_wall 807 | gb_free 11.2 | wall 4951\n",
            "2022-02-28 17:30:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 099:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 17:30:47 | INFO | fairseq.trainer | begin training epoch 99\n",
            "2022-02-28 17:30:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 099: 100% 2847/2848 [13:35<00:00,  3.67it/s, loss=6.217, nll_loss=4.737, ppl=26.67, wps=11477.8, ups=3.51, wpb=3266.6, bsz=156.3, num_updates=281900, lr=3.57358e-06, gnorm=2.513, train_wall=28, gb_free=11.9, wall=5753]2022-02-28 17:44:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 099 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.49it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.48it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.50it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.06it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 17:44:24 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 6.259 | nll_loss 4.669 | ppl 25.44 | wps 21099.5 | wpb 1999 | bsz 106 | num_updates 281952 | best_loss 6.259\n",
            "2022-02-28 17:44:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 281952 updates\n",
            "2022-02-28 17:44:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:44:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:44:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 99 @ 281952 updates, score 6.259) (writing took 8.789445253998565 seconds)\n",
            "2022-02-28 17:44:33 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
            "2022-02-28 17:44:33 | INFO | train | epoch 099 | loss 6.19 | nll_loss 4.707 | ppl 26.12 | wps 11412.8 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 281952 | lr 3.57325e-06 | gnorm 2.423 | train_wall 807 | gb_free 10.7 | wall 5778\n",
            "2022-02-28 17:44:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 100:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 17:44:33 | INFO | fairseq.trainer | begin training epoch 100\n",
            "2022-02-28 17:44:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 100: 100% 2847/2848 [13:37<00:00,  3.69it/s, loss=6.16, nll_loss=4.673, ppl=25.51, wps=11802.2, ups=3.47, wpb=3398.8, bsz=167.1, num_updates=284700, lr=3.55597e-06, gnorm=2.457, train_wall=28, gb_free=11.8, wall=6566]2022-02-28 17:58:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 100 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.60it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.69it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.65it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.18it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.24it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 17:58:12 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 6.248 | nll_loss 4.655 | ppl 25.2 | wps 21101.2 | wpb 1999 | bsz 106 | num_updates 284800 | best_loss 6.248\n",
            "2022-02-28 17:58:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 284800 updates\n",
            "2022-02-28 17:58:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:58:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 17:58:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 100 @ 284800 updates, score 6.248) (writing took 8.314253735999955 seconds)\n",
            "2022-02-28 17:58:20 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
            "2022-02-28 17:58:20 | INFO | train | epoch 100 | loss 6.177 | nll_loss 4.693 | ppl 25.87 | wps 11404 | ups 3.44 | wpb 3311.1 | bsz 154.6 | num_updates 284800 | lr 3.55534e-06 | gnorm 2.424 | train_wall 807 | gb_free 11 | wall 6605\n",
            "2022-02-28 17:58:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 101:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 17:58:20 | INFO | fairseq.trainer | begin training epoch 101\n",
            "2022-02-28 17:58:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 101: 100% 2847/2848 [13:35<00:00,  3.30it/s, loss=6.234, nll_loss=4.755, ppl=27.01, wps=11499.2, ups=3.43, wpb=3351.5, bsz=138.4, num_updates=287600, lr=3.53799e-06, gnorm=2.41, train_wall=29, gb_free=11.3, wall=7407]2022-02-28 18:11:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 101 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.09it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.19it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.01it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.17it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 18:11:57 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 6.249 | nll_loss 4.653 | ppl 25.16 | wps 21026.1 | wpb 1999 | bsz 106 | num_updates 287648 | best_loss 6.248\n",
            "2022-02-28 18:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 287648 updates\n",
            "2022-02-28 18:11:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 18:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 18:12:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 101 @ 287648 updates, score 6.249) (writing took 4.01023695999902 seconds)\n",
            "2022-02-28 18:12:01 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
            "2022-02-28 18:12:01 | INFO | train | epoch 101 | loss 6.167 | nll_loss 4.681 | ppl 25.66 | wps 11480.5 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 287648 | lr 3.5377e-06 | gnorm 2.428 | train_wall 807 | gb_free 11.7 | wall 7426\n",
            "2022-02-28 18:12:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 102:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 18:12:01 | INFO | fairseq.trainer | begin training epoch 102\n",
            "2022-02-28 18:12:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 102: 100% 2847/2848 [13:35<00:00,  3.82it/s, loss=6.149, nll_loss=4.66, ppl=25.28, wps=11483.6, ups=3.57, wpb=3213.5, bsz=150.6, num_updates=290400, lr=3.52089e-06, gnorm=2.502, train_wall=28, gb_free=11.8, wall=8215]2022-02-28 18:25:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 102 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.46it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.44it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 18:25:39 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 6.228 | nll_loss 4.63 | ppl 24.76 | wps 21045.6 | wpb 1999 | bsz 106 | num_updates 290496 | best_loss 6.228\n",
            "2022-02-28 18:25:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 290496 updates\n",
            "2022-02-28 18:25:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 18:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 18:25:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 102 @ 290496 updates, score 6.228) (writing took 8.407063212998764 seconds)\n",
            "2022-02-28 18:25:47 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
            "2022-02-28 18:25:47 | INFO | train | epoch 102 | loss 6.156 | nll_loss 4.669 | ppl 25.43 | wps 11418.1 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 290496 | lr 3.52031e-06 | gnorm 2.428 | train_wall 807 | gb_free 10.9 | wall 8252\n",
            "2022-02-28 18:25:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 103:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 18:25:47 | INFO | fairseq.trainer | begin training epoch 103\n",
            "2022-02-28 18:25:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 103: 100% 2847/2848 [13:35<00:00,  3.75it/s, loss=6.129, nll_loss=4.637, ppl=24.89, wps=11652.9, ups=3.47, wpb=3362.6, bsz=152.3, num_updates=293300, lr=3.50344e-06, gnorm=2.428, train_wall=29, gb_free=12.1, wall=9055]2022-02-28 18:39:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 103 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.31it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.36it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.42it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.66it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.19it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 18:39:24 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 6.225 | nll_loss 4.627 | ppl 24.71 | wps 21079.8 | wpb 1999 | bsz 106 | num_updates 293344 | best_loss 6.225\n",
            "2022-02-28 18:39:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 293344 updates\n",
            "2022-02-28 18:39:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 18:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 18:39:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 103 @ 293344 updates, score 6.225) (writing took 8.137735285999952 seconds)\n",
            "2022-02-28 18:39:32 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
            "2022-02-28 18:39:32 | INFO | train | epoch 103 | loss 6.146 | nll_loss 4.656 | ppl 25.22 | wps 11425.4 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 293344 | lr 3.50318e-06 | gnorm 2.431 | train_wall 807 | gb_free 12.5 | wall 9077\n",
            "2022-02-28 18:39:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 104:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 18:39:32 | INFO | fairseq.trainer | begin training epoch 104\n",
            "2022-02-28 18:39:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 104: 100% 2847/2848 [13:35<00:00,  3.15it/s, loss=6.168, nll_loss=4.681, ppl=25.65, wps=11855.3, ups=3.45, wpb=3434.8, bsz=164.6, num_updates=296100, lr=3.48684e-06, gnorm=2.431, train_wall=29, gb_free=11.8, wall=9867]2022-02-28 18:53:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 104 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.39it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.36it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.43it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.07it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 18:53:10 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 6.236 | nll_loss 4.638 | ppl 24.9 | wps 21056 | wpb 1999 | bsz 106 | num_updates 296192 | best_loss 6.225\n",
            "2022-02-28 18:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 296192 updates\n",
            "2022-02-28 18:53:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 18:53:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 18:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 104 @ 296192 updates, score 6.236) (writing took 3.9884285679981986 seconds)\n",
            "2022-02-28 18:53:14 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
            "2022-02-28 18:53:14 | INFO | train | epoch 104 | loss 6.136 | nll_loss 4.645 | ppl 25.02 | wps 11480.7 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 296192 | lr 3.4863e-06 | gnorm 2.441 | train_wall 807 | gb_free 10.6 | wall 9899\n",
            "2022-02-28 18:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 105:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 18:53:14 | INFO | fairseq.trainer | begin training epoch 105\n",
            "2022-02-28 18:53:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 105: 100% 2847/2848 [13:35<00:00,  3.48it/s, loss=6.173, nll_loss=4.686, ppl=25.75, wps=11379.1, ups=3.54, wpb=3217.7, bsz=138.4, num_updates=299000, lr=3.46989e-06, gnorm=2.438, train_wall=28, gb_free=11.5, wall=10704]2022-02-28 19:06:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 105 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.51it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.56it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.11it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.14it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 19:06:51 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 6.212 | nll_loss 4.611 | ppl 24.44 | wps 21015.2 | wpb 1999 | bsz 106 | num_updates 299040 | best_loss 6.212\n",
            "2022-02-28 19:06:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 299040 updates\n",
            "2022-02-28 19:06:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 19:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 19:07:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 105 @ 299040 updates, score 6.212) (writing took 8.496708528000454 seconds)\n",
            "2022-02-28 19:07:00 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
            "2022-02-28 19:07:00 | INFO | train | epoch 105 | loss 6.125 | nll_loss 4.632 | ppl 24.8 | wps 11417.4 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 299040 | lr 3.46966e-06 | gnorm 2.444 | train_wall 807 | gb_free 10.6 | wall 10725\n",
            "2022-02-28 19:07:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 106:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 19:07:00 | INFO | fairseq.trainer | begin training epoch 106\n",
            "2022-02-28 19:07:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 106: 100% 2847/2848 [13:35<00:00,  3.58it/s, loss=6.031, nll_loss=4.527, ppl=23.06, wps=11774, ups=3.51, wpb=3357.8, bsz=176.2, num_updates=301800, lr=3.45376e-06, gnorm=2.464, train_wall=28, gb_free=10.6, wall=11516]2022-02-28 19:20:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 106 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.19it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.28it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.66it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.02it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.11it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.17it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 19:20:37 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 6.213 | nll_loss 4.612 | ppl 24.46 | wps 21081.8 | wpb 1999 | bsz 106 | num_updates 301888 | best_loss 6.212\n",
            "2022-02-28 19:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 301888 updates\n",
            "2022-02-28 19:20:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 19:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 19:20:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 106 @ 301888 updates, score 6.213) (writing took 3.9905375380039914 seconds)\n",
            "2022-02-28 19:20:41 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
            "2022-02-28 19:20:41 | INFO | train | epoch 106 | loss 6.114 | nll_loss 4.62 | ppl 24.58 | wps 11483 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 301888 | lr 3.45325e-06 | gnorm 2.437 | train_wall 807 | gb_free 10.7 | wall 11546\n",
            "2022-02-28 19:20:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 107:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 19:20:41 | INFO | fairseq.trainer | begin training epoch 107\n",
            "2022-02-28 19:20:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 107: 100% 2847/2848 [13:36<00:00,  3.40it/s, loss=6.205, nll_loss=4.721, ppl=26.37, wps=11543.3, ups=3.45, wpb=3343.8, bsz=147.8, num_updates=304700, lr=3.43728e-06, gnorm=2.463, train_wall=29, gb_free=11.6, wall=12353]2022-02-28 19:34:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 107 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.50it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.52it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.07it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.14it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 19:34:19 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 6.204 | nll_loss 4.602 | ppl 24.28 | wps 20965.8 | wpb 1999 | bsz 106 | num_updates 304736 | best_loss 6.204\n",
            "2022-02-28 19:34:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 304736 updates\n",
            "2022-02-28 19:34:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 19:34:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 19:34:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 107 @ 304736 updates, score 6.204) (writing took 8.153154719999293 seconds)\n",
            "2022-02-28 19:34:27 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
            "2022-02-28 19:34:27 | INFO | train | epoch 107 | loss 6.105 | nll_loss 4.609 | ppl 24.4 | wps 11410.1 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 304736 | lr 3.43708e-06 | gnorm 2.447 | train_wall 808 | gb_free 10.9 | wall 12372\n",
            "2022-02-28 19:34:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 108:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 19:34:28 | INFO | fairseq.trainer | begin training epoch 108\n",
            "2022-02-28 19:34:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 108: 100% 2847/2848 [13:36<00:00,  3.59it/s, loss=6.001, nll_loss=4.492, ppl=22.5, wps=11688.8, ups=3.46, wpb=3380.7, bsz=167.3, num_updates=307500, lr=3.4216e-06, gnorm=2.471, train_wall=29, gb_free=11.2, wall=13165]2022-02-28 19:48:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 108 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.34it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.41it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.14it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 19:48:05 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 6.187 | nll_loss 4.581 | ppl 23.94 | wps 21078.5 | wpb 1999 | bsz 106 | num_updates 307584 | best_loss 6.187\n",
            "2022-02-28 19:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 307584 updates\n",
            "2022-02-28 19:48:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 19:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 19:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 108 @ 307584 updates, score 6.187) (writing took 8.311946046997036 seconds)\n",
            "2022-02-28 19:48:14 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
            "2022-02-28 19:48:14 | INFO | train | epoch 108 | loss 6.095 | nll_loss 4.597 | ppl 24.2 | wps 11414.3 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 307584 | lr 3.42113e-06 | gnorm 2.455 | train_wall 807 | gb_free 11.7 | wall 13198\n",
            "2022-02-28 19:48:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 109:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 19:48:14 | INFO | fairseq.trainer | begin training epoch 109\n",
            "2022-02-28 19:48:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 109: 100% 2847/2848 [13:36<00:00,  3.27it/s, loss=6.169, nll_loss=4.679, ppl=25.62, wps=12084.1, ups=3.42, wpb=3537.7, bsz=147, num_updates=310400, lr=3.40557e-06, gnorm=2.349, train_wall=29, gb_free=11.8, wall=14006]2022-02-28 20:01:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 109 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.31it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.29it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.39it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.01it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.10it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.19it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 20:01:51 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 6.184 | nll_loss 4.576 | ppl 23.85 | wps 20808.7 | wpb 1999 | bsz 106 | num_updates 310432 | best_loss 6.184\n",
            "2022-02-28 20:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 310432 updates\n",
            "2022-02-28 20:01:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 109 @ 310432 updates, score 6.184) (writing took 9.37851054299972 seconds)\n",
            "2022-02-28 20:02:01 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
            "2022-02-28 20:02:01 | INFO | train | epoch 109 | loss 6.085 | nll_loss 4.586 | ppl 24.01 | wps 11403.4 | ups 3.44 | wpb 3311.1 | bsz 154.6 | num_updates 310432 | lr 3.4054e-06 | gnorm 2.448 | train_wall 807 | gb_free 12.2 | wall 14025\n",
            "2022-02-28 20:02:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 110:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 20:02:01 | INFO | fairseq.trainer | begin training epoch 110\n",
            "2022-02-28 20:02:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 110: 100% 2847/2848 [13:35<00:00,  3.70it/s, loss=6.039, nll_loss=4.534, ppl=23.16, wps=11203.3, ups=3.58, wpb=3125.4, bsz=134.9, num_updates=313200, lr=3.39032e-06, gnorm=2.496, train_wall=28, gb_free=11.9, wall=14819]2022-02-28 20:15:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 110 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.43it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.44it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.48it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.07it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 20:15:38 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 6.184 | nll_loss 4.578 | ppl 23.88 | wps 20957.8 | wpb 1999 | bsz 106 | num_updates 313280 | best_loss 6.184\n",
            "2022-02-28 20:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 313280 updates\n",
            "2022-02-28 20:15:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:15:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:15:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 110 @ 313280 updates, score 6.184) (writing took 8.338908691002871 seconds)\n",
            "2022-02-28 20:15:46 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
            "2022-02-28 20:15:46 | INFO | train | epoch 110 | loss 6.076 | nll_loss 4.575 | ppl 23.84 | wps 11419.8 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 313280 | lr 3.38988e-06 | gnorm 2.46 | train_wall 807 | gb_free 11.7 | wall 14851\n",
            "2022-02-28 20:15:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 111:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 20:15:46 | INFO | fairseq.trainer | begin training epoch 111\n",
            "2022-02-28 20:15:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 111: 100% 2847/2848 [13:36<00:00,  3.53it/s, loss=6.028, nll_loss=4.52, ppl=22.94, wps=11780, ups=3.5, wpb=3367.2, bsz=173.8, num_updates=316100, lr=3.37473e-06, gnorm=2.453, train_wall=28, gb_free=10.8, wall=15660]2022-02-28 20:29:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 111 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.37it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.40it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.07it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.11it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.17it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 20:29:24 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 6.179 | nll_loss 4.568 | ppl 23.72 | wps 21007.4 | wpb 1999 | bsz 106 | num_updates 316128 | best_loss 6.179\n",
            "2022-02-28 20:29:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 316128 updates\n",
            "2022-02-28 20:29:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:29:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 111 @ 316128 updates, score 6.179) (writing took 8.229623038001591 seconds)\n",
            "2022-02-28 20:29:32 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
            "2022-02-28 20:29:32 | INFO | train | epoch 111 | loss 6.065 | nll_loss 4.563 | ppl 23.63 | wps 11417.2 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 316128 | lr 3.37458e-06 | gnorm 2.447 | train_wall 807 | gb_free 11.9 | wall 15677\n",
            "2022-02-28 20:29:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 112:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 20:29:32 | INFO | fairseq.trainer | begin training epoch 112\n",
            "2022-02-28 20:29:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 112: 100% 2847/2848 [13:35<00:00,  3.51it/s, loss=5.97, nll_loss=4.456, ppl=21.95, wps=11624.7, ups=3.52, wpb=3300.7, bsz=154.2, num_updates=318900, lr=3.35988e-06, gnorm=2.497, train_wall=28, gb_free=11.7, wall=16471]2022-02-28 20:43:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 112 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.41it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.43it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 20:43:10 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 6.16 | nll_loss 4.55 | ppl 23.42 | wps 20923.2 | wpb 1999 | bsz 106 | num_updates 318976 | best_loss 6.16\n",
            "2022-02-28 20:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 318976 updates\n",
            "2022-02-28 20:43:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:43:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 20:43:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 112 @ 318976 updates, score 6.16) (writing took 8.287657929002307 seconds)\n",
            "2022-02-28 20:43:18 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
            "2022-02-28 20:43:18 | INFO | train | epoch 112 | loss 6.056 | nll_loss 4.552 | ppl 23.46 | wps 11422.8 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 318976 | lr 3.35948e-06 | gnorm 2.462 | train_wall 807 | gb_free 11.5 | wall 16503\n",
            "2022-02-28 20:43:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 113:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 20:43:18 | INFO | fairseq.trainer | begin training epoch 113\n",
            "2022-02-28 20:43:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 113: 100% 2847/2848 [13:37<00:00,  3.21it/s, loss=6.081, nll_loss=4.579, ppl=23.91, wps=11376.7, ups=3.46, wpb=3289.7, bsz=151.3, num_updates=321800, lr=3.34471e-06, gnorm=2.451, train_wall=29, gb_free=11.4, wall=17313]2022-02-28 20:56:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 113 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.41it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.50it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.50it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 20:56:56 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 6.162 | nll_loss 4.55 | ppl 23.42 | wps 21038.6 | wpb 1999 | bsz 106 | num_updates 321824 | best_loss 6.16\n",
            "2022-02-28 20:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 321824 updates\n",
            "2022-02-28 20:56:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 20:57:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 20:57:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 113 @ 321824 updates, score 6.162) (writing took 4.0916794360018685 seconds)\n",
            "2022-02-28 20:57:00 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
            "2022-02-28 20:57:00 | INFO | train | epoch 113 | loss 6.046 | nll_loss 4.541 | ppl 23.28 | wps 11463.7 | ups 3.46 | wpb 3311.1 | bsz 154.6 | num_updates 321824 | lr 3.34458e-06 | gnorm 2.464 | train_wall 808 | gb_free 11.2 | wall 17325\n",
            "2022-02-28 20:57:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 114:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 20:57:01 | INFO | fairseq.trainer | begin training epoch 114\n",
            "2022-02-28 20:57:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 114: 100% 2847/2848 [13:36<00:00,  3.35it/s, loss=6.056, nll_loss=4.552, ppl=23.46, wps=11593.1, ups=3.45, wpb=3356.9, bsz=154.2, num_updates=324600, lr=3.33025e-06, gnorm=2.425, train_wall=29, gb_free=12.5, wall=18121]2022-02-28 21:10:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 114 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.30it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.35it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.42it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.06it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.19it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 21:10:39 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 6.149 | nll_loss 4.535 | ppl 23.18 | wps 21045.4 | wpb 1999 | bsz 106 | num_updates 324672 | best_loss 6.149\n",
            "2022-02-28 21:10:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 324672 updates\n",
            "2022-02-28 21:10:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 21:10:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 21:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 114 @ 324672 updates, score 6.149) (writing took 9.754373152005428 seconds)\n",
            "2022-02-28 21:10:49 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
            "2022-02-28 21:10:49 | INFO | train | epoch 114 | loss 6.037 | nll_loss 4.53 | ppl 23.1 | wps 11388.3 | ups 3.44 | wpb 3311.1 | bsz 154.6 | num_updates 324672 | lr 3.32988e-06 | gnorm 2.471 | train_wall 807 | gb_free 11 | wall 18153\n",
            "2022-02-28 21:10:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 115:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 21:10:49 | INFO | fairseq.trainer | begin training epoch 115\n",
            "2022-02-28 21:10:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 115: 100% 2847/2848 [13:36<00:00,  3.56it/s, loss=6.046, nll_loss=4.54, ppl=23.26, wps=11656.9, ups=3.44, wpb=3391.4, bsz=156.4, num_updates=327500, lr=3.31547e-06, gnorm=2.501, train_wall=29, gb_free=11.2, wall=18964]2022-02-28 21:24:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 115 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.29it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.37it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.16it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.22it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 21:24:27 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 6.14 | nll_loss 4.527 | ppl 23.05 | wps 21018.2 | wpb 1999 | bsz 106 | num_updates 327520 | best_loss 6.14\n",
            "2022-02-28 21:24:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 327520 updates\n",
            "2022-02-28 21:24:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 21:24:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 21:24:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 115 @ 327520 updates, score 6.14) (writing took 8.287070641003083 seconds)\n",
            "2022-02-28 21:24:35 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
            "2022-02-28 21:24:35 | INFO | train | epoch 115 | loss 6.027 | nll_loss 4.519 | ppl 22.93 | wps 11412.9 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 327520 | lr 3.31537e-06 | gnorm 2.467 | train_wall 807 | gb_free 11.9 | wall 18980\n",
            "2022-02-28 21:24:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 116:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 21:24:35 | INFO | fairseq.trainer | begin training epoch 116\n",
            "2022-02-28 21:24:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 116: 100% 2847/2848 [13:36<00:00,  3.61it/s, loss=6.042, nll_loss=4.534, ppl=23.17, wps=11695.8, ups=3.44, wpb=3398.1, bsz=149, num_updates=330300, lr=3.30139e-06, gnorm=2.418, train_wall=29, gb_free=10.5, wall=19777]2022-02-28 21:38:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 116 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.30it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.39it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.43it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.06it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 21:38:13 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 6.142 | nll_loss 4.525 | ppl 23.02 | wps 21055.8 | wpb 1999 | bsz 106 | num_updates 330368 | best_loss 6.14\n",
            "2022-02-28 21:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 330368 updates\n",
            "2022-02-28 21:38:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 21:38:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 21:38:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 116 @ 330368 updates, score 6.142) (writing took 3.9979263600034756 seconds)\n",
            "2022-02-28 21:38:17 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
            "2022-02-28 21:38:17 | INFO | train | epoch 116 | loss 6.018 | nll_loss 4.509 | ppl 22.76 | wps 11474.7 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 330368 | lr 3.30105e-06 | gnorm 2.475 | train_wall 807 | gb_free 11.1 | wall 19801\n",
            "2022-02-28 21:38:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 117:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 21:38:17 | INFO | fairseq.trainer | begin training epoch 117\n",
            "2022-02-28 21:38:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 117: 100% 2847/2848 [13:36<00:00,  3.42it/s, loss=6.1, nll_loss=4.6, ppl=24.25, wps=11674.5, ups=3.41, wpb=3422.7, bsz=153.9, num_updates=333200, lr=3.28699e-06, gnorm=2.546, train_wall=29, gb_free=10.3, wall=20614]2022-02-28 21:51:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 117 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.26it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.25it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.33it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.62it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.02it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.14it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 21:51:55 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 6.131 | nll_loss 4.513 | ppl 22.83 | wps 20922.3 | wpb 1999 | bsz 106 | num_updates 333216 | best_loss 6.131\n",
            "2022-02-28 21:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 333216 updates\n",
            "2022-02-28 21:51:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 21:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 21:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 117 @ 333216 updates, score 6.131) (writing took 8.582404132997908 seconds)\n",
            "2022-02-28 21:52:03 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
            "2022-02-28 21:52:03 | INFO | train | epoch 117 | loss 6.01 | nll_loss 4.499 | ppl 22.61 | wps 11408.3 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 333216 | lr 3.28691e-06 | gnorm 2.476 | train_wall 807 | gb_free 10.6 | wall 20628\n",
            "2022-02-28 21:52:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 118:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 21:52:03 | INFO | fairseq.trainer | begin training epoch 118\n",
            "2022-02-28 21:52:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 118: 100% 2847/2848 [13:36<00:00,  3.71it/s, loss=6.002, nll_loss=4.489, ppl=22.46, wps=11799.6, ups=3.42, wpb=3448.1, bsz=161.8, num_updates=336000, lr=3.27327e-06, gnorm=2.459, train_wall=29, gb_free=10.7, wall=21427]2022-02-28 22:05:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 118 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.34it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.41it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.48it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.16it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 22:05:41 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 6.14 | nll_loss 4.519 | ppl 22.92 | wps 21010.2 | wpb 1999 | bsz 106 | num_updates 336064 | best_loss 6.131\n",
            "2022-02-28 22:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 336064 updates\n",
            "2022-02-28 22:05:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 22:05:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-02-28 22:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 118 @ 336064 updates, score 6.14) (writing took 4.054368766999687 seconds)\n",
            "2022-02-28 22:05:45 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
            "2022-02-28 22:05:45 | INFO | train | epoch 118 | loss 6 | nll_loss 4.487 | ppl 22.43 | wps 11474.3 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 336064 | lr 3.27296e-06 | gnorm 2.477 | train_wall 807 | gb_free 11.8 | wall 21450\n",
            "2022-02-28 22:05:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 119:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 22:05:45 | INFO | fairseq.trainer | begin training epoch 119\n",
            "2022-02-28 22:05:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 119: 100% 2847/2848 [13:36<00:00,  3.50it/s, loss=6.054, nll_loss=4.548, ppl=23.4, wps=11472.1, ups=3.5, wpb=3274.2, bsz=143.5, num_updates=338900, lr=3.25923e-06, gnorm=2.557, train_wall=28, gb_free=10.2, wall=22264]2022-02-28 22:19:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 119 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.36it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.37it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.04it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.19it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 22:19:23 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 6.122 | nll_loss 4.503 | ppl 22.67 | wps 20915.5 | wpb 1999 | bsz 106 | num_updates 338912 | best_loss 6.122\n",
            "2022-02-28 22:19:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 338912 updates\n",
            "2022-02-28 22:19:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 22:19:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 22:19:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 119 @ 338912 updates, score 6.122) (writing took 8.11855311600084 seconds)\n",
            "2022-02-28 22:19:32 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
            "2022-02-28 22:19:32 | INFO | train | epoch 119 | loss 5.991 | nll_loss 4.478 | ppl 22.28 | wps 11410.5 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 338912 | lr 3.25918e-06 | gnorm 2.478 | train_wall 808 | gb_free 11.3 | wall 22276\n",
            "2022-02-28 22:19:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 120:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 22:19:32 | INFO | fairseq.trainer | begin training epoch 120\n",
            "2022-02-28 22:19:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 120: 100% 2847/2848 [13:36<00:00,  3.31it/s, loss=5.949, nll_loss=4.43, ppl=21.55, wps=11508.5, ups=3.5, wpb=3291.4, bsz=164.2, num_updates=341700, lr=3.24585e-06, gnorm=2.494, train_wall=28, gb_free=12.2, wall=23077]2022-02-28 22:33:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 120 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.28it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.30it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.40it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.19it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-02-28 22:33:10 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 6.108 | nll_loss 4.486 | ppl 22.42 | wps 20844.9 | wpb 1999 | bsz 106 | num_updates 341760 | best_loss 6.108\n",
            "2022-02-28 22:33:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 341760 updates\n",
            "2022-02-28 22:33:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 22:33:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-02-28 22:33:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 120 @ 341760 updates, score 6.108) (writing took 8.28262691400596 seconds)\n",
            "2022-02-28 22:33:18 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
            "2022-02-28 22:33:18 | INFO | train | epoch 120 | loss 5.983 | nll_loss 4.468 | ppl 22.13 | wps 11407.5 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 341760 | lr 3.24557e-06 | gnorm 2.479 | train_wall 808 | gb_free 12.7 | wall 23103\n",
            "2022-02-28 22:33:18 | INFO | fairseq_cli.train | done training in 23100.2 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 翻訳文の生成\n",
        "RESULT = \"/content/result95\"\n",
        "\n",
        "# 翻訳文生成\n",
        "! mkdir -p $RESULT\n",
        "! fairseq-generate data-bin \\\n",
        "   --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\" --batch-size 128 --beam 5 > $RESULT/result.txt\n",
        "\n",
        "# 翻訳文抽出\n",
        "! grep \"^H-\" $RESULT/result.txt | sort -V | cut -f3 > $RESULT/pred.ja\n",
        "\n",
        "# sentencepieceのdetokenize\n",
        "! cat $RESULT/pred.ja | spm_decode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --input_format=piece > $RESULT/pred.detok.ja\n",
        "\n",
        "# mecabによる単語分割\n",
        "! cat $RESULT/pred.detok.ja | mecab -Owakati > $RESULT/pred.detok_mecab.ja\n",
        "! cat /content/kftt-data-1.0/data/orig/kyoto-test.ja | mecab -Owakati > $RESULT/kyoto_test_mecab.ja\n",
        "\n",
        "# BLEUスコアの計算\n",
        "! fairseq-score --sys $RESULT/pred.detok_mecab.ja --ref $RESULT/kyoto_test_mecab.ja\n",
        "\n",
        "# ビーム探索\n",
        "RESULT_BEAM = \"/content/result95_beam\"\n",
        "! mkdir -p $RESULT_BEAM\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-generate data-bin --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\" --batch-size 128 --max-tokens 1000 --beam $N > $RESULT_BEAM/result.txt\n",
        "    ! grep \"^H-\" $RESULT_BEAM/result.txt | sort -V | cut -f3 > $RESULT_BEAM/pred.txt.$N\n",
        "    ! cat $RESULT_BEAM/pred.txt.$N | spm_decode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --input_format=piece > $RESULT_BEAM/pred.detok.txt.$N\n",
        "    ! cat $RESULT_BEAM/pred.detok.txt.$N | mecab -Owakati > $RESULT_BEAM/pred.detok_mecab.txt.$N\n",
        "\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-score --sys $RESULT_BEAM/pred.detok_mecab.txt.$N --ref $RESULT/kyoto_test_mecab.ja > $RESULT_BEAM/BLEU.score.$N\n",
        "\n",
        "# グラフ化\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def read_score(filename):\n",
        "    with open(filename) as f:\n",
        "        line = f.readlines()[1]\n",
        "        line = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', line)\n",
        "        return float(line.group())\n",
        "\n",
        "x = [1, 25, 50, 75, 100]\n",
        "y = [read_score(f'/content/result95_beam/BLEU.score.{num}') for num in x]\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NGDYZNKG3TCM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ea2cda2-c292-452e-f507-36b6f3780387"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:20:49 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:20:50 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:20:50 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:20:50 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:20:53 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-28 23:20:53 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-28 23:20:53 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/10 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-28 23:21:15 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-28 23:21:15 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,971 tokens) in 11.0s (105.81 sentences/s, 2095.38 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "Namespace(ignore_case=False, order=4, ref='/content/result95/kyoto_test_mecab.ja', sacrebleu=False, sentence_bleu=False, sys='/content/result95/pred.detok_mecab.ja')\n",
            "BLEU4 = 17.92, 50.9/25.0/13.4/7.4 (BP=0.951, ratio=0.952, syslen=25111, reflen=26370)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:21:24 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:21:25 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:21:25 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:21:25 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:21:28 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-28 23:21:28 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-28 23:21:28 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/48 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-28 23:22:18 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-28 23:22:18 | INFO | fairseq_cli.generate | Translated 1,160 sentences (24,960 tokens) in 38.7s (29.97 sentences/s, 644.89 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:22:24 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 25, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:22:24 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:22:24 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:22:24 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:22:28 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-28 23:22:28 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-28 23:22:28 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/48 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-28 23:23:52 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-28 23:23:52 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,800 tokens) in 72.3s (16.05 sentences/s, 315.53 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:23:58 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 50, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:23:58 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:23:58 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:23:58 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:24:01 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-28 23:24:01 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-28 23:24:01 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/48 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-28 23:26:11 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-28 23:26:11 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,675 tokens) in 118.5s (9.79 sentences/s, 191.34 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:26:17 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 75, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:26:17 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:26:17 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:26:17 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:26:21 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-28 23:26:21 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-28 23:26:21 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/48 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-28 23:29:20 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-28 23:29:20 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,692 tokens) in 168.0s (6.91 sentences/s, 135.09 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:29:26 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:29:26 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:29:26 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:29:26 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:29:29 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-02-28 23:29:29 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-02-28 23:29:29 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "  0% 0/48 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-28 23:33:21 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-28 23:33:21 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,615 tokens) in 220.5s (5.26 sentences/s, 102.57 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "wDKhweMI4Hdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c42fec-891d-4f09-9428-97e87c498c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-28 23:56:22 | INFO | numexpr.utils | NumExpr defaulting to 4 threads.\n",
            "2022-02-28 23:56:23 | WARNING | apex.transformer.tensor_parallel | `fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-02-28 23:56:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': 'log96', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 130, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='inverse_sqrt', max_epoch=130, max_tokens=5000, max_tokens_valid=5000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='log96', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-28 23:56:26 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-02-28 23:56:26 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-02-28 23:56:27 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=32000, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-02-28 23:56:27 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-02-28 23:56:27 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-02-28 23:56:27 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-02-28 23:56:27 | INFO | fairseq_cli.train | num. shared model params: 93,290,496 (num. trained: 93,290,496)\n",
            "2022-02-28 23:56:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-02-28 23:56:27 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.en\n",
            "2022-02-28 23:56:27 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.ja\n",
            "2022-02-28 23:56:27 | INFO | fairseq.tasks.translation | data-bin valid en-ja 1166 examples\n",
            "2022-02-28 23:56:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-02-28 23:56:31 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2022-02-28 23:56:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-02-28 23:56:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-02-28 23:56:31 | INFO | fairseq_cli.train | max tokens per device = 5000 and max sentences per device = None\n",
            "2022-02-28 23:56:31 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\n",
            "2022-02-28 23:56:32 | INFO | fairseq.optim.adam | using FusedAdam\n",
            "2022-02-28 23:56:33 | INFO | fairseq.trainer | Loaded checkpoint /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt (epoch 121 @ 341760 updates)\n",
            "2022-02-28 23:56:33 | INFO | fairseq.trainer | loading train data for epoch 121\n",
            "2022-02-28 23:56:33 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.en\n",
            "2022-02-28 23:56:33 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.ja\n",
            "2022-02-28 23:56:33 | INFO | fairseq.tasks.translation | data-bin train en-ja 440288 examples\n",
            "2022-02-28 23:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 121:   0% 0/2848 [00:00<?, ?it/s]2022-02-28 23:56:34 | INFO | fairseq.trainer | begin training epoch 121\n",
            "2022-02-28 23:56:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 121: 100% 2847/2848 [13:37<00:00,  3.32it/s, loss=5.888, nll_loss=4.359, ppl=20.53, wps=11514.1, ups=3.49, wpb=3303.5, bsz=160.8, num_updates=344600, lr=3.23217e-06, gnorm=2.398, train_wall=28, gb_free=11.2, wall=818]2022-03-01 00:10:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 121 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.69it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.66it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.17it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 00:10:12 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 6.104 | nll_loss 4.481 | ppl 22.33 | wps 20946.6 | wpb 1999 | bsz 106 | num_updates 344608 | best_loss 6.104\n",
            "2022-03-01 00:10:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 344608 updates\n",
            "2022-03-01 00:10:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 00:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 00:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 121 @ 344608 updates, score 6.104) (writing took 7.9846884199942 seconds)\n",
            "2022-03-01 00:10:20 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
            "2022-03-01 00:10:20 | INFO | train | epoch 121 | loss 5.974 | nll_loss 4.457 | ppl 21.96 | wps 11411.4 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 344608 | lr 3.23213e-06 | gnorm 2.482 | train_wall 805 | gb_free 11.4 | wall 829\n",
            "2022-03-01 00:10:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 122:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 00:10:20 | INFO | fairseq.trainer | begin training epoch 122\n",
            "2022-03-01 00:10:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 122: 100% 2847/2848 [13:34<00:00,  3.45it/s, loss=5.956, nll_loss=4.436, ppl=21.64, wps=11606, ups=3.46, wpb=3353.1, bsz=173.2, num_updates=347400, lr=3.21911e-06, gnorm=2.482, train_wall=29, gb_free=10.6, wall=1628]2022-03-01 00:23:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 122 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.58it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.59it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.11it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 00:23:56 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 6.113 | nll_loss 4.492 | ppl 22.5 | wps 21082.5 | wpb 1999 | bsz 106 | num_updates 347456 | best_loss 6.104\n",
            "2022-03-01 00:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 347456 updates\n",
            "2022-03-01 00:23:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 00:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 00:24:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 122 @ 347456 updates, score 6.113) (writing took 4.620201268997334 seconds)\n",
            "2022-03-01 00:24:01 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
            "2022-03-01 00:24:01 | INFO | train | epoch 122 | loss 5.966 | nll_loss 4.448 | ppl 21.82 | wps 11493.5 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 347456 | lr 3.21885e-06 | gnorm 2.486 | train_wall 806 | gb_free 11.1 | wall 1650\n",
            "2022-03-01 00:24:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 123:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 00:24:01 | INFO | fairseq.trainer | begin training epoch 123\n",
            "2022-03-01 00:24:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 123: 100% 2847/2848 [13:33<00:00,  3.91it/s, loss=5.972, nll_loss=4.455, ppl=21.94, wps=11471.6, ups=3.51, wpb=3264.5, bsz=149, num_updates=350300, lr=3.20576e-06, gnorm=2.553, train_wall=28, gb_free=11.5, wall=2463]2022-03-01 00:37:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 123 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.55it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.51it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.51it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 00:37:36 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 6.101 | nll_loss 4.475 | ppl 22.24 | wps 21046.8 | wpb 1999 | bsz 106 | num_updates 350304 | best_loss 6.101\n",
            "2022-03-01 00:37:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 350304 updates\n",
            "2022-03-01 00:37:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 00:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 00:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 123 @ 350304 updates, score 6.101) (writing took 8.240731659003359 seconds)\n",
            "2022-03-01 00:37:44 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
            "2022-03-01 00:37:44 | INFO | train | epoch 123 | loss 5.957 | nll_loss 4.438 | ppl 21.67 | wps 11452.4 | ups 3.46 | wpb 3311.1 | bsz 154.6 | num_updates 350304 | lr 3.20574e-06 | gnorm 2.487 | train_wall 805 | gb_free 10.6 | wall 2473\n",
            "2022-03-01 00:37:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 124:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 00:37:44 | INFO | fairseq.trainer | begin training epoch 124\n",
            "2022-03-01 00:37:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 124: 100% 2847/2848 [13:33<00:00,  3.43it/s, loss=5.983, nll_loss=4.467, ppl=22.12, wps=11929.2, ups=3.47, wpb=3434.3, bsz=160, num_updates=353100, lr=3.19303e-06, gnorm=2.435, train_wall=28, gb_free=12.3, wall=3273]2022-03-01 00:51:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 124 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.57it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.61it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 00:51:19 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 6.094 | nll_loss 4.468 | ppl 22.13 | wps 20941.4 | wpb 1999 | bsz 106 | num_updates 353152 | best_loss 6.094\n",
            "2022-03-01 00:51:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 353152 updates\n",
            "2022-03-01 00:51:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 00:51:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 00:51:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 124 @ 353152 updates, score 6.094) (writing took 8.121263560002262 seconds)\n",
            "2022-03-01 00:51:27 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
            "2022-03-01 00:51:27 | INFO | train | epoch 124 | loss 5.948 | nll_loss 4.428 | ppl 21.52 | wps 11452.3 | ups 3.46 | wpb 3311.1 | bsz 154.6 | num_updates 353152 | lr 3.19279e-06 | gnorm 2.49 | train_wall 805 | gb_free 11.9 | wall 3297\n",
            "2022-03-01 00:51:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 125:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 00:51:27 | INFO | fairseq.trainer | begin training epoch 125\n",
            "2022-03-01 00:51:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 125: 100% 2847/2848 [13:31<00:00,  3.55it/s, loss=5.923, nll_loss=4.4, ppl=21.11, wps=11800.1, ups=3.48, wpb=3394.2, bsz=168.4, num_updates=355900, lr=3.18044e-06, gnorm=2.51, train_wall=28, gb_free=11.6, wall=4080]2022-03-01 01:04:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 125 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.60it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.59it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.58it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.21it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 01:05:00 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 6.08 | nll_loss 4.451 | ppl 21.87 | wps 21007.9 | wpb 1999 | bsz 106 | num_updates 356000 | best_loss 6.08\n",
            "2022-03-01 01:05:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 356000 updates\n",
            "2022-03-01 01:05:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 01:05:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 01:05:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 125 @ 356000 updates, score 6.08) (writing took 8.282080911005323 seconds)\n",
            "2022-03-01 01:05:09 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
            "2022-03-01 01:05:09 | INFO | train | epoch 125 | loss 5.939 | nll_loss 4.417 | ppl 21.37 | wps 11483.1 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 356000 | lr 3.17999e-06 | gnorm 2.487 | train_wall 803 | gb_free 11.9 | wall 4118\n",
            "2022-03-01 01:05:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 126:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 01:05:09 | INFO | fairseq.trainer | begin training epoch 126\n",
            "2022-03-01 01:05:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 126: 100% 2847/2848 [13:32<00:00,  3.35it/s, loss=5.82, nll_loss=4.283, ppl=19.47, wps=11400, ups=3.5, wpb=3254, bsz=167.7, num_updates=358800, lr=3.16756e-06, gnorm=2.49, train_wall=28, gb_free=10.6, wall=4917]2022-03-01 01:18:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 126 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.66it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.63it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.58it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.14it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.20it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 01:18:43 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 6.075 | nll_loss 4.447 | ppl 21.81 | wps 20914.1 | wpb 1999 | bsz 106 | num_updates 358848 | best_loss 6.075\n",
            "2022-03-01 01:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 358848 updates\n",
            "2022-03-01 01:18:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 01:18:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 01:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 126 @ 358848 updates, score 6.075) (writing took 8.199131538000074 seconds)\n",
            "2022-03-01 01:18:51 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
            "2022-03-01 01:18:51 | INFO | train | epoch 126 | loss 5.931 | nll_loss 4.408 | ppl 21.23 | wps 11467.1 | ups 3.46 | wpb 3311.1 | bsz 154.6 | num_updates 358848 | lr 3.16735e-06 | gnorm 2.493 | train_wall 804 | gb_free 11.8 | wall 4940\n",
            "2022-03-01 01:18:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 127:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 01:18:51 | INFO | fairseq.trainer | begin training epoch 127\n",
            "2022-03-01 01:18:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 127: 100% 2847/2848 [13:33<00:00,  3.61it/s, loss=5.886, nll_loss=4.357, ppl=20.49, wps=11397.5, ups=3.63, wpb=3136.8, bsz=158.9, num_updates=361600, lr=3.15527e-06, gnorm=2.657, train_wall=27, gb_free=11, wall=5727]2022-03-01 01:32:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 127 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.63it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.64it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 127 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.16it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 01:32:26 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 6.079 | nll_loss 4.451 | ppl 21.88 | wps 21091.5 | wpb 1999 | bsz 106 | num_updates 361696 | best_loss 6.075\n",
            "2022-03-01 01:32:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 361696 updates\n",
            "2022-03-01 01:32:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 01:32:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 01:32:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 127 @ 361696 updates, score 6.079) (writing took 4.652347364004527 seconds)\n",
            "2022-03-01 01:32:30 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)\n",
            "2022-03-01 01:32:30 | INFO | train | epoch 127 | loss 5.923 | nll_loss 4.398 | ppl 21.09 | wps 11507.6 | ups 3.48 | wpb 3311.1 | bsz 154.6 | num_updates 361696 | lr 3.15485e-06 | gnorm 2.502 | train_wall 805 | gb_free 12.1 | wall 5760\n",
            "2022-03-01 01:32:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 128:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 01:32:30 | INFO | fairseq.trainer | begin training epoch 128\n",
            "2022-03-01 01:32:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 128: 100% 2847/2848 [13:33<00:00,  3.18it/s, loss=5.881, nll_loss=4.351, ppl=20.41, wps=11516.8, ups=3.46, wpb=3323.9, bsz=155.4, num_updates=364500, lr=3.1427e-06, gnorm=2.488, train_wall=29, gb_free=12.4, wall=6560]2022-03-01 01:46:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 128 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.71it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.76it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.19it/s]\u001b[A\n",
            "epoch 128 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.25it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 01:46:05 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 6.078 | nll_loss 4.448 | ppl 21.83 | wps 21152 | wpb 1999 | bsz 106 | num_updates 364544 | best_loss 6.075\n",
            "2022-03-01 01:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 364544 updates\n",
            "2022-03-01 01:46:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 01:46:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 01:46:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 128 @ 364544 updates, score 6.078) (writing took 4.837596552999457 seconds)\n",
            "2022-03-01 01:46:10 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)\n",
            "2022-03-01 01:46:10 | INFO | train | epoch 128 | loss 5.915 | nll_loss 4.389 | ppl 20.96 | wps 11506.9 | ups 3.48 | wpb 3311.1 | bsz 154.6 | num_updates 364544 | lr 3.14251e-06 | gnorm 2.5 | train_wall 805 | gb_free 11.9 | wall 6579\n",
            "2022-03-01 01:46:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 129:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 01:46:10 | INFO | fairseq.trainer | begin training epoch 129\n",
            "2022-03-01 01:46:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 129: 100% 2847/2848 [13:35<00:00,  3.52it/s, loss=5.762, nll_loss=4.217, ppl=18.6, wps=11508.6, ups=3.58, wpb=3210.6, bsz=167.9, num_updates=367300, lr=3.1307e-06, gnorm=2.534, train_wall=28, gb_free=12.4, wall=7369]2022-03-01 01:59:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 129 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.56it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.60it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.12it/s]\u001b[A\n",
            "epoch 129 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.19it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 01:59:47 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 6.066 | nll_loss 4.435 | ppl 21.64 | wps 21021.8 | wpb 1999 | bsz 106 | num_updates 367392 | best_loss 6.066\n",
            "2022-03-01 01:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 367392 updates\n",
            "2022-03-01 01:59:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 01:59:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 01:59:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 129 @ 367392 updates, score 6.066) (writing took 8.205566124997858 seconds)\n",
            "2022-03-01 01:59:55 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)\n",
            "2022-03-01 01:59:55 | INFO | train | epoch 129 | loss 5.907 | nll_loss 4.38 | ppl 20.82 | wps 11431.7 | ups 3.45 | wpb 3311.1 | bsz 154.6 | num_updates 367392 | lr 3.1303e-06 | gnorm 2.507 | train_wall 806 | gb_free 10.8 | wall 7404\n",
            "2022-03-01 01:59:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 2848\n",
            "epoch 130:   0% 0/2848 [00:00<?, ?it/s]2022-03-01 01:59:55 | INFO | fairseq.trainer | begin training epoch 130\n",
            "2022-03-01 01:59:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 130: 100% 2847/2848 [13:35<00:00,  3.64it/s, loss=5.906, nll_loss=4.378, ppl=20.79, wps=11249.6, ups=3.55, wpb=3168.5, bsz=145.6, num_updates=370200, lr=3.11841e-06, gnorm=2.527, train_wall=28, gb_free=11.2, wall=8208]2022-03-01 02:13:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 130 | valid on 'valid' subset:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:   9% 1/11 [00:00<00:02,  4.49it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  18% 2/11 [00:00<00:01,  6.54it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  36% 4/11 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  45% 5/11 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  55% 6/11 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  64% 7/11 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  73% 8/11 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "epoch 130 | valid on 'valid' subset:  82% 9/11 [00:01<00:00,  9.22it/s]\u001b[A\n",
            "                                                                       \u001b[A2022-03-01 02:13:31 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 6.069 | nll_loss 4.441 | ppl 21.71 | wps 21139.5 | wpb 1999 | bsz 106 | num_updates 370240 | best_loss 6.066\n",
            "2022-03-01 02:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 370240 updates\n",
            "2022-03-01 02:13:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 02:13:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_last.pt\n",
            "2022-03-01 02:13:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 130 @ 370240 updates, score 6.069) (writing took 4.48682224300137 seconds)\n",
            "2022-03-01 02:13:36 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)\n",
            "2022-03-01 02:13:36 | INFO | train | epoch 130 | loss 5.899 | nll_loss 4.371 | ppl 20.69 | wps 11483 | ups 3.47 | wpb 3311.1 | bsz 154.6 | num_updates 370240 | lr 3.11824e-06 | gnorm 2.493 | train_wall 806 | gb_free 11.3 | wall 8225\n",
            "2022-03-01 02:13:36 | INFO | fairseq_cli.train | done training in 8222.5 seconds\n"
          ]
        }
      ],
      "source": [
        "# ===========\n",
        "# 96. 学習過程の可視化\n",
        "#============\n",
        "! rm -r /content/checkpoints\n",
        "! fairseq-train data-bin --arch transformer  --restore-file \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch120.pt\"\\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --lr 3e-5 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --clip-norm 0.0 \\\n",
        "    --optimizer adam --max-tokens 5000 --max-epoch 130 \\\n",
        "    --patience 5 --no-epoch-checkpoints --tensorboard-logdir log96  \\"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! tensorboard --logdir \"/content/log96\" --host=localhost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kio_JpqrX1iw",
        "outputId": "ff052934-923a-4b60-c667-5ca2f20f6073"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpFWCCeb4Hkr"
      },
      "outputs": [],
      "source": [
        "# ===========\n",
        "# 97. ハイパーパラメータの調整\n",
        "#============\n",
        "! rm -r /content/checkpoints\n",
        "! fairseq-train data-bin --arch transformer  --restore-file \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\"\\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --lr 3e-5 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --clip-norm 0.0 \\\n",
        "    --optimizer adam --max-tokens 5000 --max-epoch 150 \\\n",
        "    --patience 5 --no-epoch-checkpoints --tensorboard-logdir log97  \\"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 翻訳文の生成\n",
        "RESULT = \"/content/result97\"\n",
        "\n",
        "# 翻訳文生成\n",
        "! mkdir -p $RESULT\n",
        "! fairseq-generate data-bin \\\n",
        "   --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\" --batch-size 128 --beam 5 > $RESULT/result.txt\n",
        "\n",
        "# 翻訳文抽出\n",
        "! grep \"^H-\" $RESULT/result.txt | sort -V | cut -f3 > $RESULT/pred.ja\n",
        "\n",
        "# sentencepieceのdetokenize\n",
        "! cat $RESULT/pred.ja | spm_decode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --input_format=piece > $RESULT/pred.detok.ja\n",
        "\n",
        "# mecabによる単語分割\n",
        "! cat $RESULT/pred.detok.ja | mecab -Owakati > $RESULT/pred.detok_mecab.ja\n",
        "! cat /content/kftt-data-1.0/data/orig/kyoto-test.ja | mecab -Owakati > $RESULT/kyoto_test_mecab.ja\n",
        "\n",
        "# BLEUスコアの計算\n",
        "! fairseq-score --sys $RESULT/pred.detok_mecab.ja --ref $RESULT/kyoto_test_mecab.ja\n",
        "\n",
        "# ビーム探索\n",
        "RESULT_BEAM = \"/content/result97_beam\"\n",
        "! mkdir -p $RESULT_BEAM\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-generate data-bin --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\" --batch-size 128 --max-tokens 1000 --beam $N > $RESULT_BEAM/result.txt\n",
        "    ! grep \"^H-\" $RESULT_BEAM/result.txt | sort -V | cut -f3 > $RESULT_BEAM/pred.txt.$N\n",
        "    ! cat $RESULT_BEAM/pred.txt.$N | spm_decode --model=\"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_ja.model\" --input_format=piece > $RESULT_BEAM/pred.detok.txt.$N\n",
        "    ! cat $RESULT_BEAM/pred.detok.txt.$N | mecab -Owakati > $RESULT_BEAM/pred.detok_mecab.txt.$N\n",
        "\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-score --sys $RESULT_BEAM/pred.detok_mecab.txt.$N --ref $RESULT/kyoto_test_mecab.ja > $RESULT_BEAM/BLEU.score.$N\n",
        "\n",
        "# グラフ化\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def read_score(filename):\n",
        "    with open(filename) as f:\n",
        "        line = f.readlines()[1]\n",
        "        line = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', line)\n",
        "        return float(line.group())\n",
        "\n",
        "x = [1, 25, 50, 75, 100]\n",
        "y = [read_score(f'/content/result97_beam/BLEU.score.{num}') for num in x]\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d517ba2a-5dc1-4ca6-c9c3-3f9f54468219",
        "id": "-VU8t_VXpTW7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:17:48 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 07:17:48 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-03-01 07:17:48 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-03-01 07:17:48 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\n",
            "2022-03-01 07:17:57 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 07:17:57 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 07:17:57 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 07:18:27 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 07:18:27 | INFO | fairseq_cli.generate | Translated 1,160 sentences (23,107 tokens) in 13.8s (84.21 sentences/s, 1677.47 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "Namespace(ignore_case=False, order=4, ref='/content/result97/kyoto_test_mecab.ja', sacrebleu=False, sentence_bleu=False, sys='/content/result97/pred.detok_mecab.ja')\n",
            "BLEU4 = 17.92, 50.9/24.8/13.3/7.3 (BP=0.957, ratio=0.958, syslen=25260, reflen=26370)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:18:35 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 07:18:35 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-03-01 07:18:35 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-03-01 07:18:35 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\n",
            "2022-03-01 07:18:38 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 07:18:38 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 07:18:38 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 07:19:16 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 07:19:16 | INFO | fairseq_cli.generate | Translated 1,160 sentences (24,307 tokens) in 27.4s (42.39 sentences/s, 888.20 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:19:21 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 25, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 07:19:21 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-03-01 07:19:21 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-03-01 07:19:21 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\n",
            "2022-03-01 07:19:24 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 07:19:24 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 07:19:24 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 07:20:38 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 07:20:38 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,578 tokens) in 63.8s (18.19 sentences/s, 354.06 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:20:43 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 50, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 07:20:44 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-03-01 07:20:44 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-03-01 07:20:44 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\n",
            "2022-03-01 07:20:46 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 07:20:46 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 07:20:46 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 07:22:53 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 07:22:53 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,553 tokens) in 115.8s (10.02 sentences/s, 194.80 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:22:58 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 75, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 07:22:58 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-03-01 07:22:58 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-03-01 07:22:58 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\n",
            "2022-03-01 07:23:01 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 07:23:01 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 07:23:01 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 07:26:03 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 07:26:03 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,439 tokens) in 172.3s (6.73 sentences/s, 130.21 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 07:26:09 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1000, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1000, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 07:26:09 | INFO | fairseq.tasks.translation | [en] dictionary: 32000 types\n",
            "2022-03-01 07:26:09 | INFO | fairseq.tasks.translation | [ja] dictionary: 32000 types\n",
            "2022-03-01 07:26:09 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch130.pt\n",
            "2022-03-01 07:26:12 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 07:26:12 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 07:26:12 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 07:30:12 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 07:30:12 | INFO | fairseq_cli.generate | Translated 1,160 sentences (22,446 tokens) in 230.4s (5.04 sentences/s, 97.44 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ISH5aLov4Hsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af20477-f7d1-4e35-ad2b-e8ed3a40d5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-01 13:08:34--  http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/1.0/pretrained_models/en-ja/base.tar.gz\n",
            "Resolving www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)... 163.137.218.58\n",
            "Connecting to www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)|163.137.218.58|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 851521284 (812M) [application/x-gzip]\n",
            "Saving to: ‘base.tar.gz’\n",
            "\n",
            "base.tar.gz         100%[===================>] 812.07M  1.36MB/s    in 10m 9s  \n",
            "\n",
            "2022-03-01 13:18:43 (1.33 MB/s) - ‘base.tar.gz’ saved [851521284/851521284]\n",
            "\n",
            "base/\n",
            "base/dict.en.txt\n",
            "base/LICENSE\n",
            "base/dict.ja.txt\n",
            "base/base.pretrain.pt\n",
            "--2022-03-01 13:18:53--  http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/1.0/spm_models/en-ja_spm.tar.gz\n",
            "Resolving www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)... 163.137.218.58\n",
            "Connecting to www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)|163.137.218.58|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1177051 (1.1M) [application/x-gzip]\n",
            "Saving to: ‘en-ja_spm.tar.gz’\n",
            "\n",
            "en-ja_spm.tar.gz    100%[===================>]   1.12M   755KB/s    in 1.5s    \n",
            "\n",
            "2022-03-01 13:18:55 (755 KB/s) - ‘en-ja_spm.tar.gz’ saved [1177051/1177051]\n",
            "\n",
            "enja_spm_models/\n",
            "enja_spm_models/spm.en.nopretok.model\n",
            "enja_spm_models/spm.en.nopretok.vocab\n",
            "enja_spm_models/spm.ja.nopretok.vocab\n",
            "enja_spm_models/spm.ja.nopretok.model\n"
          ]
        }
      ],
      "source": [
        "# ===========\n",
        "# 98. ドメイン適応（jparacrawlの事前学習済みモデルを利用する）\n",
        "#============\n",
        "\n",
        "# jparacrawlの事前学習済みモデルのダウンロード\n",
        "! wget http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/1.0/pretrained_models/en-ja/base.tar.gz\n",
        "! tar xzvf base.tar.gz\n",
        "! rm base.tar.gz\n",
        "! mv base ./pretrained_model_enja\n",
        "\n",
        "# sentencepieceモデルのダウンロード\n",
        "! mkdir -p /content/corpus\n",
        "! wget http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/1.0/spm_models/en-ja_spm.tar.gz\n",
        "! tar xzvf en-ja_spm.tar.gz\n",
        "! rm en-ja_spm.tar.gz\n",
        "! mv enja_spm_models /content/corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理\n",
        "\n",
        "# dataのsubword化\n",
        "! mkdir -p /content/corpus/spm_data\n",
        "SPM_EN = \"/content/corpus/enja_spm_models/spm.en.nopretok.model\"\n",
        "SPM_JA = \"/content/corpus/enja_spm_models/spm.ja.nopretok.model\"\n",
        "KFTT_DATA = \"/content/kftt-data-1.0/data/orig\"\n",
        "DATA_DIR = \"/content/corpus/spm_data\"\n",
        "! spm_encode --model=$SPM_EN --output_format=piece < $KFTT_DATA/kyoto-train.en > $DATA_DIR/train.en\n",
        "! spm_encode --model=$SPM_EN --output_format=piece < $KFTT_DATA/kyoto-dev.en > $DATA_DIR/valid.en\n",
        "! spm_encode --model=$SPM_EN --output_format=piece < $KFTT_DATA/kyoto-test.en > $DATA_DIR/test.en\n",
        "! spm_encode --model=$SPM_JA --output_format=piece < $KFTT_DATA/kyoto-train.ja > $DATA_DIR/train.ja\n",
        "! spm_encode --model=$SPM_JA --output_format=piece < $KFTT_DATA/kyoto-dev.ja > $DATA_DIR/valid.ja\n",
        "! spm_encode --model=$SPM_JA --output_format=piece < $KFTT_DATA/kyoto-test.ja > $DATA_DIR/test.ja"
      ],
      "metadata": {
        "id": "UtdkjAGTbyRr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理\n",
        "\n",
        "# fairseqの前処理\n",
        "! rm -r data-bin\n",
        "SRC = \"en\"\n",
        "TRG = \"ja\"\n",
        "DATA_DIR = \"/content/corpus/spm_data\"\n",
        "! fairseq-preprocess \\\n",
        "    --source-lang $SRC \\\n",
        "    --target-lang $TRG \\\n",
        "    --trainpref $DATA_DIR/train \\\n",
        "    --validpref $DATA_DIR/valid \\\n",
        "    --testpref $DATA_DIR/test \\\n",
        "    --srcdict /content/pretrained_model_enja/dict.en.txt \\\n",
        "    --tgtdict /content/pretrained_model_enja/dict.ja.txt \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTm3pg_FUhPu",
        "outputId": "a542883d-5c20-4a69-c0f1-4b0aa9ef5000"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data-bin': No such file or directory\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 13:19:33 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict='/content/pretrained_model_enja/dict.en.txt', suppress_crashes=False, target_lang='ja', task='translation', tensorboard_logdir=None, testpref='/content/corpus/spm_data/test', tgtdict='/content/pretrained_model_enja/dict.ja.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/corpus/spm_data/train', use_plasma_view=False, user_dir=None, validpref='/content/corpus/spm_data/valid', wandb_project=None, workers=1)\n",
            "2022-03-01 13:19:33 | INFO | fairseq_cli.preprocess | [en] Dictionary: 31968 types\n",
            "2022-03-01 13:20:51 | INFO | fairseq_cli.preprocess | [en] /content/corpus/spm_data/train.en: 440288 sents, 15072285 tokens, 0.00918% replaced (by <unk>)\n",
            "2022-03-01 13:20:51 | INFO | fairseq_cli.preprocess | [en] Dictionary: 31968 types\n",
            "2022-03-01 13:20:51 | INFO | fairseq_cli.preprocess | [en] /content/corpus/spm_data/valid.en: 1166 sents, 32765 tokens, 0.11% replaced (by <unk>)\n",
            "2022-03-01 13:20:51 | INFO | fairseq_cli.preprocess | [en] Dictionary: 31968 types\n",
            "2022-03-01 13:20:51 | INFO | fairseq_cli.preprocess | [en] /content/corpus/spm_data/test.en: 1160 sents, 35343 tokens, 0.00283% replaced (by <unk>)\n",
            "2022-03-01 13:20:51 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 31960 types\n",
            "2022-03-01 13:22:18 | INFO | fairseq_cli.preprocess | [ja] /content/corpus/spm_data/train.ja: 440288 sents, 11956368 tokens, 0.0657% replaced (by <unk>)\n",
            "2022-03-01 13:22:18 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 31960 types\n",
            "2022-03-01 13:22:19 | INFO | fairseq_cli.preprocess | [ja] /content/corpus/spm_data/valid.ja: 1166 sents, 27999 tokens, 0.189% replaced (by <unk>)\n",
            "2022-03-01 13:22:19 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 31960 types\n",
            "2022-03-01 13:22:19 | INFO | fairseq_cli.preprocess | [ja] /content/corpus/spm_data/test.ja: 1160 sents, 30126 tokens, 0.349% replaced (by <unk>)\n",
            "2022-03-01 13:22:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練\n",
        "\n",
        "! rm -r /content/checkpoints\n",
        "! fairseq-train data-bin --arch transformer --restore-file \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch9_98.pt\" \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --lr 3e-5 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --clip-norm 0.0 \\\n",
        "    --optimizer adam --max-tokens 5000 --max-epoch 10 \\\n",
        "    --patience 5 --no-epoch-checkpoints --tensorboard-logdir log98  \\\n",
        "    --reset-optimizer \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsDbWH82UzqB",
        "outputId": "6c90ddbd-29b9-4de0-f532-a4f7f682d5b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/checkpoints': No such file or directory\n",
            "2022-03-01 13:22:25 | INFO | numexpr.utils | NumExpr defaulting to 4 threads.\n",
            "2022-03-01 13:22:25 | WARNING | apex.transformer.tensor_parallel | `fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 13:22:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': 'log98', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch9_98.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=5000, max_tokens_valid=5000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=5, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch9_98.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='log98', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 13:22:28 | INFO | fairseq.tasks.translation | [en] dictionary: 31968 types\n",
            "2022-03-01 13:22:28 | INFO | fairseq.tasks.translation | [ja] dictionary: 31960 types\n",
            "2022-03-01 13:22:29 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(31968, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(31960, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=31960, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-03-01 13:22:29 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-03-01 13:22:29 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-03-01 13:22:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-03-01 13:22:29 | INFO | fairseq_cli.train | num. shared model params: 93,233,152 (num. trained: 93,233,152)\n",
            "2022-03-01 13:22:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-03-01 13:22:29 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.en\n",
            "2022-03-01 13:22:29 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.en-ja.ja\n",
            "2022-03-01 13:22:29 | INFO | fairseq.tasks.translation | data-bin valid en-ja 1166 examples\n",
            "2022-03-01 13:22:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-03-01 13:22:38 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2022-03-01 13:22:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-03-01 13:22:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-03-01 13:22:38 | INFO | fairseq_cli.train | max tokens per device = 5000 and max sentences per device = None\n",
            "2022-03-01 13:22:38 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch9_98.pt\n",
            "2022-03-01 13:22:46 | INFO | fairseq.optim.adam | using FusedAdam\n",
            "2022-03-01 13:22:46 | INFO | fairseq.trainer | Loaded checkpoint /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch9_98.pt (epoch 10 @ 0 updates)\n",
            "2022-03-01 13:22:46 | INFO | fairseq.trainer | loading train data for epoch 10\n",
            "2022-03-01 13:22:46 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.en\n",
            "2022-03-01 13:22:46 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-bin/train.en-ja.ja\n",
            "2022-03-01 13:22:46 | INFO | fairseq.tasks.translation | data-bin train en-ja 440288 examples\n",
            "2022-03-01 13:22:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3331\n",
            "epoch 010:   0% 0/3331 [00:00<?, ?it/s]2022-03-01 13:22:46 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-03-01 13:22:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 3330/3331 [16:15<00:00,  3.43it/s, loss=4.282, nll_loss=2.657, ppl=6.31, wps=12404.8, ups=3.41, wpb=3635.4, bsz=135.1, num_updates=3300, lr=2.475e-05, gnorm=1.513, train_wall=29, gb_free=10.5, wall=975]2022-03-01 13:39:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 1/13 [00:00<00:02,  5.69it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 3/13 [00:00<00:01,  9.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 5/13 [00:00<00:00, 10.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  54% 7/13 [00:00<00:00, 10.02it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  69% 9/13 [00:00<00:00,  9.98it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 11/13 [00:01<00:00,  9.76it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-03-01 13:39:03 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.066 | nll_loss 2.345 | ppl 5.08 | wps 24140.4 | wpb 2153.8 | bsz 89.7 | num_updates 3331\n",
            "2022-03-01 13:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3331 updates\n",
            "2022-03-01 13:39:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 13:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-01 13:39:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 3331 updates, score 4.066) (writing took 7.914230734999819 seconds)\n",
            "2022-03-01 13:39:11 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-03-01 13:39:11 | INFO | train | epoch 010 | loss 4.252 | nll_loss 2.622 | ppl 6.15 | wps 12141 | ups 3.38 | wpb 3589.4 | bsz 132.2 | num_updates 3331 | lr 2.49825e-05 | gnorm 1.472 | train_wall 963 | gb_free 11.6 | wall 994\n",
            "2022-03-01 13:39:11 | INFO | fairseq_cli.train | done training in 985.1 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/checkpoints/checkpoint_best.pt \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch10_98.pt\""
      ],
      "metadata": {
        "id": "NIvDTpu8V02G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPM_EN = \"/content/corpus/enja_spm_models/spm.en.nopretok.model\"\n",
        "SPM_JA = \"/content/corpus/enja_spm_models/spm.ja.nopretok.model\"\n",
        "\n",
        "# 翻訳文の生成\n",
        "RESULT = \"/content/result98\"\n",
        "\n",
        "# 翻訳文生成\n",
        "! mkdir -p $RESULT\n",
        "! fairseq-generate data-bin \\\n",
        "   --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch10_98.pt\" --batch-size 128 --beam 5 > $RESULT/result.txt\n",
        "\n",
        "# 翻訳文抽出\n",
        "! grep \"^H-\" $RESULT/result.txt | sort -V | cut -f3 > $RESULT/pred.ja\n",
        "\n",
        "# sentencepieceのdetokenize\n",
        "! cat $RESULT/pred.ja | spm_decode --model=$SPM_JA --input_format=piece > $RESULT/pred.detok.ja\n",
        "\n",
        "# mecabによる単語分割\n",
        "! cat $RESULT/pred.detok.ja | mecab -Owakati > $RESULT/pred.detok_mecab.ja\n",
        "! cat /content/kftt-data-1.0/data/orig/kyoto-test.ja | mecab -Owakati > $RESULT/kyoto_test_mecab.ja\n",
        "\n",
        "# BLEUスコアの計算\n",
        "! fairseq-score --sys $RESULT/pred.detok_mecab.ja --ref $RESULT/kyoto_test_mecab.ja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhV9PCa2VUM-",
        "outputId": "fe276185-80a4-49c1-b97d-27bbe6f56b9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "2022-03-01 13:55:45 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch10_98.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-01 13:55:45 | INFO | fairseq.tasks.translation | [en] dictionary: 31968 types\n",
            "2022-03-01 13:55:45 | INFO | fairseq.tasks.translation | [ja] dictionary: 31960 types\n",
            "2022-03-01 13:55:45 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch10_98.pt\n",
            "2022-03-01 13:55:48 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.en\n",
            "2022-03-01 13:55:48 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.en-ja.ja\n",
            "2022-03-01 13:55:48 | INFO | fairseq.tasks.translation | data-bin test en-ja 1160 examples\n",
            "2022-03-01 13:56:07 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-01 13:56:07 | INFO | fairseq_cli.generate | Translated 1,160 sentences (28,958 tokens) in 10.2s (114.17 sentences/s, 2850.17 tokens/s)\n",
            "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
            "Namespace(ignore_case=False, order=4, ref='/content/result98/kyoto_test_mecab.ja', sacrebleu=False, sentence_bleu=False, sys='/content/result98/pred.detok_mecab.ja')\n",
            "BLEU4 = 24.34, 59.4/32.5/19.2/11.6 (BP=0.951, ratio=0.953, syslen=25118, reflen=26370)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ビーム探索\n",
        "RESULT_BEAM = \"/content/result98_beam\"\n",
        "! mkdir -p $RESULT_BEAM\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-generate data-bin --path \"/content/drive/MyDrive/Colab Notebooks/nlp100/chapter10/spm_epoch10_98.pt\" --batch-size 128 --max-tokens 1000 --beam $N > $RESULT_BEAM/result.txt\n",
        "    ! grep \"^H-\" $RESULT_BEAM/result.txt | sort -V | cut -f3 > $RESULT_BEAM/pred.txt.$N\n",
        "    ! cat $RESULT_BEAM/pred.txt.$N | spm_decode --model=$SPM_JA --input_format=piece > $RESULT_BEAM/pred.detok.txt.$N\n",
        "    ! cat $RESULT_BEAM/pred.detok.txt.$N | mecab -Owakati > $RESULT_BEAM/pred.detok_mecab.txt.$N\n",
        "\n",
        "for N in [1, 25, 50, 75, 100]:\n",
        "    ! fairseq-score --sys $RESULT_BEAM/pred.detok_mecab.txt.$N --ref $RESULT/kyoto_test_mecab.ja > $RESULT_BEAM/BLEU.score.$N"
      ],
      "metadata": {
        "id": "w0oCrqipw2Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフ化\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def read_score(filename):\n",
        "    with open(filename) as f:\n",
        "        line = f.readlines()[1]\n",
        "        line = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', line)\n",
        "        return float(line.group())\n",
        "\n",
        "x = [1, 25, 50, 75, 100]\n",
        "y = [read_score(f'/content/result98_beam/BLEU.score.{num}') for num in x]\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "1lePtKvAw9AA",
        "outputId": "8af879a5-f549-4c5b-b53f-8df5c08f3a8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqZz1hZ64H0E"
      },
      "outputs": [],
      "source": [
        "# ===========\n",
        "# 99. 翻訳サーバの構築\n",
        "#============\n",
        "# 終了"
      ]
    }
  ]
}